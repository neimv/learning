<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <bookmarks list=""/>
  <node name="Ciencia de datos" unique_id="1" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617689129" ts_lastsave="1664320597">
    <node name="examenes de secciones" unique_id="115" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664320597" ts_lastsave="1664407685">
      <rich_text>¿Cuál es el orden correcto al solucionar una operación aritmética?
La estructura adecuada de argumentos dentro de la función condicional "SI" es:
La gráfica adecuada para mostrar participación por categoría es:
¿Qué expresión simbólica es correcta para la relación “El triángulo ABC es congruente con el triángulo XYZ”?

Un lienzo de  90 m se corta en 3 trozos: trozo A, trozo B y trozo C. Calcular cuánto  mide el trozo B; sabiendo que el trozo B y el trozo C miden el doble y  el triple que el trozo A, respectivamente., 30
Es un método de solución de ecuaciones lineales:
¿Cómo se relaciona data science con inteligencia artificial?
Las aplicaciones de movilidad facilitan la integración social sugiriendo recorridos hacia todas las zonas sin distinción. Esto es:

Es el resultado de elevar al cuadrado la expresión 5x²y
Son los sumandos que forman los miembros:
La utilidad principal de Excel es:
En Excel la herramienta Filtro es Ăºtil para todas estas cosas, excepto para:
</rich_text>
    </node>
    <node name="Mates" unique_id="89" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652638017" ts_lastsave="1664581715">
      <node name="simbolos matematicos" unique_id="90" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652638040" ts_lastsave="1654029130">
        <rich_text>simbolos
- son importantes por que estan a nuestro alrededor
- las mates se crearon o se inventaron:
   → se cree que ambas son reales en cierta medida
- nosotros entendemos el universo a travez de nuestra mente, somo parte del universo
- elementos basicos:
   → numeros
   → letras, realmente son alfabetos, el griego es importante
   → simbolos aritmeticos
   → elementos comunes, los tres puntos generalmente tiene una expresion muy distinto
- existen ciertas expresiones matematicas que se consideran bellas, por ejemplo la idedntidad de euler, se compone de elementos basicos dentro de las mates

Lenguaje matematico
- aritmetica //
   → es la base de todo en las matematicas
   → generalmente esto ya se da por sentado
   → existen otros metodos numericos, como el base 60, es muy favorable para las divisiones (2, 3, 4, 6, 10), nuestro tiempo se basa en este
- algebra
   → concepto abstracto de la variable
      ⇒ la variable puede tener varios valores
   → nace del persa: al-jabr wa-l
   → geometrizacion del algebra
   → griegos: latus - lado (incognita)
   → arabes: mal y jadhr - lado o base
   → europeos: jadhr - base o raiz de una planta
- se tiene 3 periodos:
   → retorica, se comunicaban de voz a voz, no existian los escritos
   → anotadas, se hacia escribiendo con palabras
   → simbolica, es donde stamos parados en este momento

Numero y varibles
- sistema binario
- sistema decimal
- sistema hexadecimalseparador decimal, coma o punto decimal, separar dentro de la parte entere de la fraccionaria
- numero naturales, son con los cuales contamos, existe una definicion con el 0, pero es raro
- enteros se tiene los negativos, 0 y naturales
- racionales, se pueden dividir en fracciones m/n donde n no puede ser 0
- irracionales, no se pueden expresa como una fraccion, como por ejemplo pi
- reales, incluyes todo lo anterior y todos los que no se pueden expresar como fraccion
- completos, numeros reales con el numero imaginario _i_
- alfabetos
   → el mas usado es el romano o latin
   → griego, se usa bastante para angulos y constantes
   → gotico, por ejemplo la d gotica para derivadas parciales
- por que x siempre es la incognita
   → Rene Descartes, fue quien empezo a usar las ultimas letras del alfabeto latino como incognitas y las primeras como parametros conocidos y se reforzo con los ejes cartecianos (x, y, z)

Notacion cientifica
- subindices:
   → enumeriones, x_1, x_2
   → diferenciadores, E_m = E_p + E_c
   → contadores de iteraciones, A = (a_i,j)_2x2
- se usan los puntos en diagonal cuando se expande hacia columnas y filas
- super indices
   → potencias, x^4
   → diferenciadores, por ejemplo segunda derivada de y
   → enumeraciones, se usa mas en textos: identidad de euler^1
- Notacion cientifica
   → se usa mucho para representar cantidades muy grande (positivo) y muy pequenio (negativos) mas facil
   → constante de avogrado, es un numero muy grande que se require expresar con 10 ^ 23
   → se usan prefijos de unidades en SI

Aritmeticos y de agrupacion
- se hace el uso de simbolos +: suma, -: resta, /: division y *: multipicacion, hay otras formas de hacer estos mismos, pero se complica en la computadora
- jerarquia (de izquierda a derecha cuando salen juntos)
   → signos de agrupacion
   → potencia y raiz
   → division y multiplicacion
   → suma y resta
- operadores de agrupacion, {}, [], (), en 1399, se usaba por primera vez, y fue el punto, para lectura, en 1470 se usaron parentesis
   → John Wallis, vinculum, paso a ser la raiz cuadrada
- simbolos combinados o alterados, como mas-menos y menos-mas

comparacion y desigualdad
- simbologia basica
   → menor que
   → mayor que
   → menor igual que
   → mayor igual que
- intervalos finitos
   → [a, b], a &lt;= x &lt;= b 
   → [a, b), a &lt;= x &lt; b
   → (a, b], a &lt; x &lt;= b
- intervalos infinitos
   → [-inf, a], -inf &lt; x &lt;= b 
   → [-inf, a), -inf &lt; x &lt; a
   → (-inf, inf], -inf &lt; x &lt; inf
- relacion
   → &lt;&lt; mucho menor que
   → &gt;&gt; mucho mayor que
   → ~ similar
   → ~= congruente
   → (tres puntos) ppor lo tanto
   → E -&gt; pertenece
- geometricos
   → paralelos ||
   → perpendicular, lineas perpendiculares
   → angulo, angulo de 45
   → angulo recto, angulo de 90
   → trinagulo
   → angulo XYZ, repsentado con un angulo de 45 con XYZ
   → triangulo ABC, un triangulo con letras ABC
   → existe los de:
      ⇒ segmentos de linea con AB
      ⇒ vector, linea con flecha
      ⇒ recta, flecha para los dos lados
      ⇒ arco, un “sombrero”

Operadores logicos
- </rich_text>
      </node>
      <node name="discretas" unique_id="116" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664581715" ts_lastsave="1664583318">
        <rich_text>- la logica es todo aquello con lo que tomamos decisiones
- aquello que tiene sentido para nosotros
- es coherente, estructurado y tiene sentido
- proposicion es dar un valor de verdadero o falso

Valor de verdad
- si se tiene p y q y alguna tiene un valor de verdadero, se pueden mezclar para obtener un valor resultado
- se usa 2^n para saber cuantos valores se tienen
- de aqui se puede tener la tabla de verdad
- </rich_text>
      </node>
    </node>
    <node name="Analisis de negocios" unique_id="2" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617689619" ts_lastsave="1618440726">
      <rich_text>¿Que es la ciencia de datos?
- big data: gran volumen de información
- solución matemática a un problema de negocio
Empresas con gran volumen de información:
- facebook
- bancos
- sintrafico

Tipos de datos
existen 5 tipos de datos principales
- personas, la generamos nosotros, preferencias, información de tiempo con personas
- transacciones, monetarias y no monetarias, flujo de dinero las primeras (que s epaga y en donde), las no financieras son como las compañias telefonicas (patrones de conducta)
- navegación web, cookies, registros de información desde el browser
- machine 2 machine, dispositivos de GPS, movilidad
- biométricos, es información unica y con debate etico
empresas
facebook: personas, información de gustos
bancos: transacciones monetarias (ver que se compra y con que frecuencia)
sintrafico: machien 2 machine

Cultura data-driven
1. crear cultura, hacer que todos tomen decisiones de acuerdo a los datos (hay que enseñar que son los datos)
2. recolectar (almacenaje y procesado)
3. medir todo (entender la data y por que)
4. datos precisos y relevantes (que es lo que realmente sirve de lo que tenemos, que sea precisa y estandar, tener datos que tenga los datos lo mas identicos posibles)
5. testear y crear hipotesis (saber que puede pasar y por que pasa, patrones especificos)
6. insights para tomas acciones (saber que vamos a hacer una vez con data recolectada)
7. automatizar &lt;3 . &lt;3

Machine learning e inteligencia artifical
la inteligencia articial es lo que se conoce como maquina inteligente
machine learning es el aprendizaje por la computadora y que se pueda mejorar
el machine learning nos sirve para
- detección de fraudes
- búsqueda web
- anuncios a tiempo real
- análisis de textos
- next best action
tres empresas:
- facebook
- amazon
- mercado libre

Deep learning
es el aprendizaje profundo, nos sirve generalmente para las saber que tipo de imagen es o que canción, son modelos con mucho entrenamiento

Roles en datos
- Ingeniero de datos, construye la informacion a almacenar, obtiene información y la guarda
- Analista BI, partiendo de la información guardada la extrae para información que le interese
- Data Scientist, predice por medio de modelos, explica las situaciones de la empresa (pasado, presente y futuro)

Herramientas
- SQL, analista e ingeniero de datos
- Python Y R, cientifico de datos analisis descriptivo y exploratorio

Conflictos de los datos
- hay información delicada de los usuario

Técnicas de storytelling
Estructura del problema: problema, solución, alcance (que se quiere explicar con este estudio)

Estructurar un caso de negocio:
que? - cual es el problema del negicio
por que? - cuales son los motivos o causas
como?:
	- análisis cuantitativo
	- análisis cualitativo
	- matriz cuantitativa-cualitativa
	- definir acciones
	- validación

Análisis cuantitativo
- identificar variables numericas
- cuales nos son utiles

Análisis cualitativo
- variables cualitativas
- clusterizar causas de contacto, agrupar problematicas
- clasificación
- profundizar

Toma de decisiones:
- </rich_text>
    </node>
    <node name="POO" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1618776094" ts_lastsave="1619095024">
      <rich_text># Programación básica en POO-Python
- es un modelado del mundo
- se puede decomponer

# Complejidad algoritmica
- es la comparacion de la eficiencia de dos diferentes algoritmos
- predice el tiempo en resolver un problema
- se puede definir como T(n)
- hay dos tipos, la temporal y espacial

Aproximaciones:
- tiempo de respuesta 1 vs 1, tiene el problema de depender del hardware o software, schedulers
- Contar pasos como medida abstracta, operaciones matematicas... puede ser mas eficiente, la solucion varia de programa a programa a nivel algoritmico
- Contar pasos asintoticamente, para el crecimiento

# Conteo abstracto
- aproximación del tipo matematico
- se cuenta que pasa adentro del programa
- se suma cada operacion que se hacen (los loops por cada una de las iteraciones)
- se puede representar las operaciones de loops en x, lo que puede darnos parabolas

# Notación asintótica
- conocido como “Big O notation”, esto se llama asi ya que se va acercando al infinito o se va acercando
- el input generalmente es que el que nos da esta salida
- existen otros tipos de notaciones
- este tipo de notación se puede saber sumando los ‘n’ pasos que vamos requiriendo en el algoritmos
- se toma el termino mas grande por ej. si queda ‘n’ vs ‘n**2’ se toma el ‘n**2’

# Clases de complejidad algoritmica
- O(1) este siempre sera constante, no importa cuando cresca el input
- O(n) Lineal, se crece de manera proporcional al imput
- O(log n) Logaritmica, crece mucho y de poco a poco se estabiliza, mergesort
- O(n log n) logaritmico lineal, crece de manera logaritmica pero constantemente
- O(n**2) Polinomial
- O(2**n) exponcial, este crece mas rapido que el polinomial, este es el menos efectivo ("tiralos a la basura"), son muy bueno a nivel teorico

# Busqueda lineal
- busqueda de manera secuencial
- el peor de los casos es que el elemento que se busca esta al final
- de tipo O(n)

# Busqueda binaria
- divide y conquistaras
- se parte en 2 en cada iteración
- asume que esta ordenada

# Ordenamiento de burbuja
- de tipo O(n**2)

</rich_text>
    </node>
    <node name="Probabilidad y Estadistica, marce" unique_id="4" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619978072" ts_lastsave="1623388351">
      <rich_text># Conceptos básico
- Probabilidad, que tan posible es que ocurra un evento
   → la probabilidad siempre va entre 0 y 1
   → se puede escribir en fraccion, decimal o porcentaje
   → la probabilidad nunca es negativa
   → se escribe P(A)
- Experimento: se busca un valor determinado, proceso que nos da los datos estadisticos a estudiar
   → numericos: numeros
   → no numericos: colores, nombres
- Espacio muestral: conjunto de valores que se obtienen en el experimento, se define por la letra Omega
- Suceso, son cada uno de los resultados que se obtienen en el experimento
   → posible: existe la probabilidad de que se obtenga lo que quiere
   → seguro: si todo el espacio muestral esta definido por lo que se necesita
   → imposible: que no se tenga el valor que se quiere

# Calculo de probabilidades
- Experimentos equiprobables: Cada suceso del espacio muestral tiene la misma probabilidad de ocurrencia
- Regla de Laplace: P(A) = Casos favorables de A / Casos posibles

# Probabilidad compuesta y diagramas de árbol
- es donde intervienen mas de un experimento aleatorio
- se hace la multiplicación de cada evento que tenemos
- los diagramas de arbol pueden ayudar a saber que probabilidad le toca a cada evento

# Union
- compatibles: encuentra los resultados necesarios en cada uno de los universos creados
   → P(AUB) = P(A) + P(B)
   → P(AUB) = P(A) + P(B) - P(A^B)
- incompatibles: no encuentra el resultado dentro de uno de los universos
- complementarios: cuando ambos universos se unen y crean todo el espacio muestral

# Intersección
- Suceso formado que cumplen a y b
   → P(A^B) = P(A) * P(B)
   → P(A^B) = P(A/B) * P(B) donde:
      ⇒ P(B/A) = P(A^B) / P(A). si P(A) != 0

# Variaciones, permutacion y combinaciones
- Combinatoria, estudia las agrupaciones partiendo de un conjunto de acuerdo al orden y al número de elementos
-  variaciones, subgrupos que ocurren cuando se agrupan cierto numero de elementos en una cantidad especifica 
   →  V(n,r) = n! / (n - r)!
- permutacones: Son variaciones de n elementos tomados en grupos de r, donde n = r
   → p(n) = n(n-1)(n-2)...(1) = n!
- combinaciones: Se obtienen al seleccionar de n elementos un subgrupo r, aqui si importa que no se repitan, se calcula a partir de:
   → C(n,r) = n! / r!(n - r)! = (n /n r) = C(n,r) = V(n, r) / P(r)

# Tabla de frecuencias
- la distribucion de datos, sirve para saber que metodo estadistico es el correcto a usar
- los datos se pueden presentar:
   → graficas
   → textual
   → en cuadros estadisticos
- la organizacion de datos es por medio de una tabla de frecuencias donde se muestra que tanto se repiten los datos
- la frecuencia absoluta es cada uno de los valores que tiene por valor
- la frecuencia absoluta acumulada es la suma la de la actual mas la anterior
- la frecuencia relativa es la division de la frecuencia absoluta entre la frecuencia total
- la frecuencia relativa acumulada es la suma de la frecuencia relativa mas su anterior

# Gráfica de dispersión
- Relaciona los datos de estudio, por medio de sus variables
- se representa por medio de un diagrama matematico
- Se le conoce como nube de puntos, son variables bidimensionales, sabiendo que tanto afectan o dependen de ellas

# Parametros estadisticos, centralización
- centralizacion: son valores recogidos, que representan de forma global a la muestra o poblacion
   → media: es la suma de todas las observaciones dividivo por el numero de observaciones
   → mediana: Es el valor de posición de datos ordenados, se toma el que esta en medio
      ⇒ par = X=X(n+1) / 2
      ⇒ impar= (X=X(N/2) + X(N/2+1)) / 2
   → moda: Es el valor que tiene mas repeticiones de datos

# Tipos de correlación o covarianza
- existen tres tipos de correlaciones
   → Directa: se da cuando una variable aumenta y la otra también o de caso inverso
   → Inversa: se presenta cuando una variable aumente la otra disminuye y en caso inverso
   → Nula: cuando no se encuentra ninguna relacion entre variables
- Covarianza: es la media aritmetica de los productos de las desviaciones de cada una de las variables respecto a sus medias respectivas

# Rango (Dispersion de distribuciones)
- son una serie de valores que indican que tan dispersos, juntos o separados estan los datos, esto de acuerdo a las medidas centrales
- rango o amplitud, es el recorrido de la distribución estadística, es la distancia que hay entre el mayor y menor
- para datos agrupados, el recorrido es la diferencia entre el límite real superior del ultimo intervalo y el primer intervalo
- mide la dispersion total de todos los elementos

# Desviacion media
- es la media aritmetica de los valores absolutos de todos los datos respecto a la media aritmetica

# Varianza y desviacion estandar
- desviacion estandar raiz cuadrada de la varianza
- varianza, que tan separados estan los datos

# Coeficiente de correlación
- es una valor cuantitativo
- relacion entre dos variables
- la proporcionalidad positiva esta dada por 1, negativa por -1, si no existe es igual a 0
- es a covarianza de x, y entre las desviaciones tipicas de x y de y

# Cuartiles, deciles y percentiles
- Cuartiles son valores que dividen a la población en 4 partes iguales, representan al 25%, 50% y 75% de los datos, el 2 representa la mediana
- Deciles divide a los valores en 9 partes, el 5 representa la mediana
- Percentiles divide el conujunto de datos en 100 partes iguales el percentil 50 coincide con la mediana

# Que es y para que sirve la regresion logistica
- regresion simple, a partir de datos como la correlación y los datos tabulados, se puede encontrar un valor futuro que se puede predecir
- la formula es x(i) = a + bt
</rich_text>
    </node>
    <node name="regresion-python" unique_id="5" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1620426905" ts_lastsave="1620654556">
      <rich_text># Regresion líneal y machine learning
- machine learner, usando modelos de regresion lineal
- prediccion de datos por medio una linea, los datos que esten mas cerca de ella
- por medio de variables ‘x’ y ‘y’
- se puede sobreajustar o underfit puede dar predicciones reales
- regresion lineal para regresiones
- regresion logistica es para clasificaciones
- naive bayes clasificacion
- KNN regresion y clasificacion
- arboles regresion y clasificacion

# Explicacion matemática de la regresión líneal
- termino estadistico, modelo matematico, relacion entre una relacion dependiente e independiente
- y = bo + biX
   → y: dependiente
   → X: independiente
   → bo: constante
   → bi: pendiente, inclinacion del sistema

# Metodo de mínimos cuadrados
- sumatoria de x menos promedio de x multiplicado por y menos y promedio y se divide entre la sumatoria de los cuadrados de x menos promedio de x
   → sum ((x - avg(x)(y - avg(y)) / sum (x - avg(x))^2
   → sirve para encontrar la inclinación
- con el pomedio de puntos se puede encontrar bo, siendo y = avg(y) y la x = avg(x), y b1 es la inclinación</rich_text>
    </node>
    <node name="Calculo basico" unique_id="6" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621260505" ts_lastsave="1622078620">
      <rich_text># Aprendamos calculo
- las matemáticas son un lenguaje, sirven para modelar y entender fenomenos de nuestra realidad
- Descenso del gradiente es lo final

# Qué es el cálculo?
- realizar operaciones para llegar a un resultado
- calculo infinitesimal, cuando las cantidades tienden a ser cercanas a 0
- calculo diferencial, estudia la tasa de cambio de las funciones, los cambios pequeños se conocen como delta x
- calculo integral, es el proceso inverso del diferencial

# Que es una funcion
- es una regla donde cada elemento de A, se le asigna un elemento del conjunto B
- una funcion es como una maquina, y = f(x)
- una funcion se puede representar:
   → Verbalmente
      ⇒ de forma verbal, diciendo el problema
   → Numéricamente
      ⇒ se puede representar a lo largo de una tabla, donde tenemos x y y en cada columna
   → Visualmente
      ⇒ se muestra por medio de una gráfica
   → Algebraicamente
      ⇒ este se representa de y = f(x) = x^2

# Dominio y rango de una función
- el dominio, se define como los valores que toma x y estan definidos en f(x)
- el rango, son todos los resultados que nos da x una vez resuelta

# Como se compone una neurona
- dentro de una neurona se tiene una funcion, donde recibe estimulos (entradas)
- hace sumas ponderadas de los valores de entrada
- se les agrega un peso determinado (W)
   → y = f(x) = W1x1 + W2x2 + b
   → y = f(x) = W1x1 + W2x2 + ... + Wnxn + b
   → bias, es constante es un rango para ajustar el valor

# Funciones activadoras de neuronas
- las funciones de activacion nos da una linea curva, se ajusta al comportamiento de los datos
- tipos:
   → paso escalonado, heaviside, su rango esta dado con [0, 1], solo toma o valor 0 o 1
   → funcion sigmoide, es una curva que parte en 0.5, sus valores van de 0 a 1, pero no los toca, f(x) = 1 / (1 + e^-x), su rango va de (0, 1), puede tomar cualquier valor de 0 a 1
   → funcion tangente hiperbolica, su rango va de (-1, 1) y toma cualquier valor, tanh(x) = (e^x - e^-x) / (e^x + e^-x)
   → funcion ReLU, funcion rectificada lineal f(x) = max(0, x), 0 para x menores o iguales a 0 y x para valores mayores a x

# Función de coste
- se calculan que tan alejados estan los datos reales de la predicción
- (y_pred - y_real)^2 = error
- error_total = sumatoria de i = 1 hasta n de error dado por la prediccion - datos reales al cuadrado
- ECM = 1 / n (sum_i=1_n (y_prom - y)^2)

# Que es un limite
- se puede decir que es a que valor tiende una funcion en un punto dado

# De donde surge la derivada
- surge para saber la tangente a una curva
- linea que toca la curva en un solo punto
- el punto se evalua en x + delta_x, esto s una razon de cambio, donde hay un movimento
- el incremento sirve para saber con mayor exactitud la pendiente, esto cuando ese incremento esta muy cerca uno del otro

# Máximos y mínimos
- teorema de la primera derivada
   → Si f’(x)&gt;0 hacia la izquierda de un punto a y si f’(x)&lt;0 hacia la derecha del punto a, entonces f tiene un máximo relativo en (a, f(a))
   → Si f’(x)&lt;0 hacia la izquierda de un punto a y si f’(x)&gt;0 hacia la derecha del punto a, entonces f tiene un mínino relativo en (a, f(a))
   → Si f’(x) es menos o mayor de ambos lados, no es ni un máximo ni un mínimo
- Teorema de la segunda derivada
   → Si f’’(x)&lt;0 entonces f tiene un máximo relativo en (x, f(x))
   → Si f’’(x)&gt;0 entonces f tiene un mínimo relativo en (x, f(x))
   → Si f’’(x)=0 no se puede determinar si es un máximo o un mínimo o ninguno de los dos. Se debe utilizar el teorema de la primera derivada para poder determinarlo

# Parciales
- con la parcial de X, se encuentra la tangente que esta sobre x, mientras que la de y nos ayuda a encontrar la que esta con y
- con ambas se puede encontrar un plano que pase sobre la curva

# Gradiente
- Vector que nos dice donde asciende de manera mas rapida na superficie
- se representa con nabla y es la derivada parcial de x + la derivada parcial de y

# Descenso del gradiente
- para optimizacion de funciones
- es un proceso iterativo para mejorar la funcion, se puede representar como:
   → w : w - {alpha}{grad}F
- la limitacion es que si tienen muchas curvas puede caer en un minimo muy pequeño o puede variar mucho</rich_text>
    </node>
    <node name="Probablidad" unique_id="7" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622420216" ts_lastsave="1653857321">
      <rich_text># Que es la probablidad?
- se usa en situaciones donde hay incertidumbre
- El azar no es mas que la medida de nuestra ignorancia. los fenomenos fortuitos son aquellos cuyas leyes o causas simplemente ignoramos
- la toma de decisiones con informacion incompleta
- lenguaje y conjunto de herramientas, para cuantificar la incertidumbre
- axioma: P = N. de sucesos exitosos / N. de sucesos totales
- escuelas:
   → frecuentista, son numero que solo se alcanzan en infinitas ocasiones, tiene tendencia a 1/2 por ejemplo la moneda
      ⇒ espacio muestral, espacio donde estan todos los eventos elementales
   → bayesiana
- sucesos:
   → elemental, que va a pasar sin restriccion
   → suceso: resultado con alguna restriccion, es algo mas general
- Espacio muestral: donde estan toda las ocurrencias que van a pasar
- Axiomas:
   → P = numero de sucesos exitosos / numero de sucesos totales
- propiedades:
   → va del 0 al 100
   → si es 100 es certeza
   → si es 0 es imposibilidad
   → disjuntos: la probabilidad es la suma de cada evento

# Probabilidad en machine learning
- fuentes de incertidumbre:
   → datos, este es un proceso imperfecto, debido a los instrumentos que se usan
   → atributos del modelo, las variables preditores son un subconjunto reducido, lo que hace que halla mas incertidumbre
   → arquitectura del modelo, representación de la realidad
- modelo de clasificacion
   → entrenamiento
      ⇒ extractor de atributos
      ⇒ atributos
      ⇒ algoritmo de ML
   → prediccion
      ⇒ extractor de atributos
      ⇒ atributos
      ⇒ modelo de clasificacion
      ⇒ etiqueta
- etapas del modelo (no todos los modelos son probabilisticos):
   → arquitectura, que modelo se usa, si usa o no probabilidad
   → parametros, entrenamiendo, aprendiendo por la distribucion de probabilidad, MLE
   → calibracion, ajuste del modelo por medio de hiper parametros, esto pasa mucho en las redes neuronales donde uno modifica los pesos para que sea ams exacto
   → interpretacion de la prediccioón
   → resultado

# Tipos de probabilidad
- regla del producto: P(A,B) = P(A|B)P(B)
- marginal: P(A) = SUM_B(P(A,B))
- conjunta
   → cuando se calcula la probabilidad de dos o mas sucesos
   → se calcula con un conteo al espacio muestral
- marginal
   → solo la probabilidad de un suceso, sin importar otro suceso
- condicional
   → depende de que ya se tenga un condicion
   → esto nos reduce el espacio muestral
   → calculando el valor de la probabilidad condicional y multiplicandolo por la probabilidad nos dara la probabilidad conjunta, regla del producto

# Que es una distribución o una densidad
- es una funcion en el sentido del calculo que toma una variable aleatoria y a cada uno de sus posibles estados le asigna una probabilidad
- lineal = discreta
- P(X=x) -&gt; densidad de probabilidad
   → X -&gt; variable aleatoria
   → x -&gt; posibles valores
   → El domino son todos los valores posibles que puede tener la variables aleatoria
   → se dividen en dos:
      ⇒ funciones discretas, 1, 2, 3, 4, lanzar dado o una moneda, se pueden contar facilmente
      ⇒ funcion continuas, 0 a infinito, la temperatura, dentro de los numero reales
   → area bajo la curva, funcion de probabilidad acumulada
   → en discreatas esta dada mas por un diagrama de barras, histograma

# Distribuciones discretas
- Distribución de Bernoulli P(X=1) = p, P(X=0) = 1 - p
   → la suma de probabilidades debe de dar el 100%
   → ocurrencias binarias
   → se acompleja teniendo varias secuencias de eventos binarios -&gt; distribución binomial
   → combinatorio: (n k) = n! / k!(n-k)!
   → formula: p(k, n) = (n k) (p^k) * (1-p)^(n-k)
- otras distribuciones, Poisson, geométrica, hipergeométrica, binomial negativa
- multinomial es una distribucion donde se tiene mas de dos valores por ejemplo el caso de los dados

revisar el metodo binom (parece ser una clase)
algoritmos deterministicos aleatorios

# Distribuciones continuas
- data set seattlecentral
- distribucion nomal o gaussiana, e muy comun en los datos
- 
- otras:
   → exponencial
   → pareto

Estimar una distribucion
- ajustar una distribucion a un conjunto de datos, para poder hacer una prediccion
- se crea una hipotesis para poder saber que tipo de distribucion es
- no parametrica: no se forzan parametros de una funcion unica, son varias distribuciones, generalmente se combinan varias funciones, medoto de suavizado y funcion base
- funciones en sklearn: kernels (kernel density estimation)

# Que es MLE
- Estimación de máxima verosimilitud
- estimar densidades de probabilidad dentro de un esquema de trabajo muy general
- framework para estimacion de densidades de un cojuntos de datos:
   → escoger la distribucion, teniendo solo una muestra de los datos
   → escoger los parametros de la distribucion, que ajustan mejor la distribucion
- generalmente se tienen una distribucion mayor, pero en nuestro caso siempre tenemos una muestra de los datos
- es un problema de optimización
   → se tienen muchos datos que pueden variar la salida
   → se toma el valor mas probable que pueda pasar calcular los valores

# MLE en machine learning
- ajustar densidades a datos
- se ajustan densidades datos en ml
- aqui es donde se busca el modelo que nos va a ayudar para los datos
- regresion lineal con MLE:
   → se encuentra el modelo lineal
- los minimos cuadrados son un problema de MLE

# Regresion logistica
- problema de clasificacion binaria
- los resultados son de dos tipos en clasificacion binaria
- sigmoide: y = 1 / (1 + exp(-x))
- se pueden dar P con valores entre 0 y 1, usando los errores se arregla que nos de las categorias con las probabilidades mas altas
- distribucion bernoulli:
   → p = p x 1 + (1 - p) x 0
   → L = ŷ * y + (1 - ŷ) x (1 - y)
- cross entropy: CE = -(sum)y_i * log ŷ + (1 -y_i) * log (1 - ŷ)

# Teoria de Bayes
- refleja una filosofia diferente de los sucesos aleatorios
- P(A|B) -&gt; posteriori, cual es la probabilidad de que pase A dando los elementos de B
- P(B|A) -&gt; Verosimil, prueba de la evidencia de que pase A de acuerdo a B
- P(A) -&gt; priori, creencia inicial que se tiene, puede no ser la realidad
- P(B) -&gt; evidencia, esta es la realidad, modifica las probabilidades
- por lo tanto P(A|B) = (P(B|A)P(A)) / P(B)
- MAP sirve como MLE para bayesianos

# TEMAS A ESTUDIAR
La función de error que se usa en regresión logística se conoce como:
¿Qué son las funciones en programación?
Una distribuciÃ³n de probabilidad es:

</rich_text>
    </node>
    <node name="algebra lineal" unique_id="8" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622838312" ts_lastsave="1623196812">
      <rich_text># Las bases
- el tipo de datos se diferencia en los grados de libertad
- escalar es un numero, variables normales en python
- vector, es un lugar donde se colocan mucho numeros
- la matriz tiene dos grados de libertad, es la union de varios vectores
- el tensor tiene uno o mas grados de libertad que la matriz, se puede decir que son multiples matrices

# Propiedades de las matrices
- asociativo: Ax(BxC) = (AxB)xC
- distributivo: Ax(B+C) = (AxB)+(AxC)
- conmutatio: BxC = CxB, no lo es en matrices, en el caso de vectores si lo es
- (AB)^t = B^tA^t

# Que es combinacion lineal
- es multiplicar un vector por un escalar, otro vector por otro escalar y sumar el resultado para obtener un nuevo vector

# La norma
- La norma sirve para medir el tamaño de un vector, no puede ser negativo, queremos conocer el error al hacer las aproximaciones
- norma &gt;= 0
- la norma(v) = 0 ⇔ v = 0
- la deisgualdad triangular, es la suma de dos vectores, se calculan las nomas se puede decir que norm(v3) &lt;= norm(v1) + norm(v2)
- norm(a*v) = abs(a)*norm(v)
- la unica forma en que la norma de las sumas sea igual a la suma de las normas de cada vector es que ambos sean parte de si mismos

# Normas:
- L0: nos devuelve la cantidad de elementos distintos de cero
- L1: sum_i abs(vi)
- L2: es la distancia euclidiana entre dos puntos
- en ML se usa mucho el (L2)^2
- L_inf = max_i abs(v_i)

# Producto interno de dos vectores
- el producto interno de dos vectores es la norma de cada vector por el angulo que forman entre ellos

# matriz identidad
- es el elemento neutro del producto interno
- al multiplica A*A^1 = Id.
- la singular es aquella que no tiene matriz inversa
- al multiplicar la matriz lineal por un vector no hace una combinacion lineal de las distintas coordenadas, se dice que hace una ponderacion
- una matriz es simetrica, cuando su traspuesta es igual a la matriz A = A^t

# Vectores ortogonales
- para ser ortogonal es en referencia a otro vector
- el angulo que forman los dos vectores forman 90°
- ortonormal, cuando la norma de los vectores es 1
- se pueden volver ortonormales si se les divide por su norma

# matriz ortogonal
- es cuando todas sus filas y columnas con ortonormales
- los vectores que se forman en la matriz deben de ser ortogonales
- A^t*A = A*A^t = id -&gt; A^t = A^-1

# la traza y el determinante
- la traza nos devuelve el mismo numero independientemente de que sistema de referencia se utilice para representar la matriz
- traza(ABC) = traza(ACB) = traza(CBA)
- el determinante nos da el espejo del espacio</rich_text>
    </node>
    <node name="estadistica descriptiva" unique_id="10" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624149121" ts_lastsave="1653320979">
      <rich_text>- Curso para descifrar lo que es cierto y lo que no

# Que es estadistica descriptiva
- descriptiva: “resumir un historial deportivo”
   → como es un desempeño despues de varios partidos
   → se crean metricas resumidas... en este tipo de estadistica
- inferncial: predecir desempenio futuro del jugador
- basicamente es resumir la informacion
- como se define lo que es correcto o no, definición de metricas unicas
- maked statistics, Charles Wheelan
- se aprende por:
   → resumir grandes cantidades de informacion
   → tomar decisiones
   → responder preguntas con relevancia social
   → reconocer patrones en los datos
   → descubir neofitos

es mas comun que las primeras fases de la ciencia de datos se requiera mas de la estadistica descriptiva:
- tipos de datos, pipeline de procesamiento, analisis exploratorio, estadistica descriptiva, correlacion, reducciones de datos

Un analisis exploratorio es la base de la estadistica descriptiva, identificar correlacion, si se puede reducir el conjunto de datos

# Tipos de datos
- Categoricos (genero, categoria, metodos de pago), como tal no son números separan los datos en categorias
   → ordinal, existe una relacion de orden entre categorias
   → nominal, no existe una relacion
- Numericos (edad, altura, temperatura), son explicitamente numeros
   → discretos, generalmente la edad
   → continuos, la altura ya que es un poco mas flotante
- categoricos: object y bool, numericos: int64, float64
- medidas de tendencia central y de dispersion... TIPOS DE ESTADISTICOS DESCRIPTIVOS

# Medidas de tendencia central
- sirve para resumir información
- Media (promedio), dice una ubicacion del conjunto de datos
   → es susceptible a valores atípicos
- Mediana (dato central), ordenados del mayor al menor, es que este en medio es la Mediana, valor balanceado
- Moda (dato que mas se repite)
   → no aplica para datos numéricos continuos
- Se usa generalmente la tabla de frecuencias
- una tabla de frecuencias es para saber cada cuantas veces aparece un dato, que tan comun es
- la media es muy susceptible a valores atipicos
- la moda no aplica pada datos numericos continuos
- con longitud impar se puede obtener el valor de enmedio, pero cuando es par se calcula la mediana sacando el promedio de los dos valores de enmedio

# Medidas de dispersion
- son un complemento para las medidas de tendencia central
- Rango
   → valor minimo y maximo de un conjunto de datos
- Rango intercuartil
   → se basa en los cuartiles
   → se divide el conjunto de datos en 4 subdivisiones
   → Q2 es la mediana
   → Q1 esta entre la mediana y el minimo
   → Q3 esta entre la mediana y el maximo
   → la distancia entre el Q1 y el Q3 es el rango intercuartil
   → diagrama de caja sirve para visualizar los cuartiles
- Desviacion estandar
   → 
   → (punto - promedio)^2 -&gt; la suma de cada elemento entre su numero de elementos -&gt; es la varianza
   → la desviacion estandar es la raiz de la varianza
   → en el caso de que sea por medio de la muestra no se divide entre el numero de elementos sino de (numero de elementos - 1)
- Distribución normal
   → tiene forma de campana
   → el promedio mas/menos 3 veces la desviacion estandar es donde se encuentran la totalidad de todos los datos
   → identificacion de datos anomalos:
      ⇒ Q1 - 1.5 * IQR
      ⇒ Q3 + 1.5 * IQR
   → coinciden lo de la desviacion estandar con los cuartiles

# estadística en la ingesta de datos
- pipelines de procesamiento de datos numericos:
   → es muy necesario normalizarlos, por que los optimizadores son optimos mientras que todos los atributos que estan tienen las mismas magnitudes
   → escalamiento lineal o normalizar
      ⇒ se deben de usar entre el rango de -1, 1 (generalmente son mas eficientes los ML
      ⇒ los escalamientos se hacen cuando estan uniformemente distribuidos o tiene una distribucion simetrica
      ⇒ existen diferentes tipos (se usan dependiendo de lo que se esta tratando de hacer con el modelo)
         • max-min, se transforma a un valor normalizado donde se usa una transformacion para ir de un valor x (es el valor a transformar que se toma) a x_s, se puede defirnir como x_s = (2x - min - max) / (max - min)
         • clipping, se toma la distribucion y se corta entre los valores limite inferior y superior, descarta valores por eso no es muy usable, los que rebasan los valores superior e inferior se forzan a ser esos valores
         • z-score, es mas comun, esta basado en definicion de promedio y desviacion estandar, x_s = (x - prom) / desv. estandar
         • winsorizing, es como el clipping pero usando los quartiles
      ⇒ como usarlos:
         • data simetrica o uniformemente distribuida
- transformacion no lineal (se usa cuando no estan con distribuciones simetricas)
   → datos fuertemente sesgados, no simetricos
   → existen diferentes:
      ⇒ logaritmos
      ⇒ sigmoides
      ⇒ polinomiales
   → se usan antes de escalar linealmente
   → generalmente los datos dicen que tipo d etransformacion se requiere
   → generalmente los valores uniformemente distribuidos se usa max min y con la distribucion tipo Gauss se usa z-score
- Pipelines de procesamiento de datos categoricos
   → Dummy
      ⇒ representacion compacta
      ⇒ Mejor para inputs linealmente independientes
      ⇒ una correlacion no tan fuerte
      ⇒ cuando las categorias son independientes entre si
   → One-hot
      ⇒ para algo mas grande
      ⇒ permite describir categorias no incluidas inicialmente
      ⇒ no ordinales
      ⇒ son representaciones categoricas
   → ambos se deben de mapear manualmente las categorias de los valores
   → categorias no ordinales, no tienen un orden entre ellas
   → a veces se trata como Dummy y One Hot como lo mismo, aunque Dummy realemente no existe
- se pueden tratar variables numericas como categorias: si, depende del caso del uso
- Correlaciones:
   → cuando dos variables tienen un comportamiento identico se dice que estan correlacionadas
   → si tienen una correlacion muy alta es posible no incluirlas ya que pueden estar aportando la misma informacion
   → se puede reducir el número de variables
   → mide las desviaciones de una variable x con relacion a otra variable
- el coeficiente de correlacion
   → p = (cov) / (std(x) * std(y))
   → es la medida especifica que cuantifica la intensidad de la relacion lineal entre dos varibales en un analisis
   → es el coeficiente P
- la correlacion mide algo que puede ser casualidad que varie, al revisar el problema puede no ser causa-relación
- causasion no esta asociado con correlacion, correlacion no implica causasion
- matriz de covarianza:
   → cuando se obtienen todas las variables de covarianza entre ellas
   → muhas veces se tienen que normalizar los datos

# PCA
- anáisis de componentes principales
   → proyección(sombra) de un vector sobre otro
      ⇒ es que se proyecta sobre la superficie del segundo vector (como una sobra), se desea calcular solo la longitud que tiene sobre el vector
         • vec(a_p) = a_p vec(b) = ((vec(a) * vec(b)) / |vec(b)|) uni(b)
   → cada vector propio es una de las direcciones principales de la cual capturamos varianza de los datos originales
   → hay que revisar todo lo que tenga que ver con los eigen values y eigen vectores para poder hacer el PCA
   → al llegar a este punto generalmente es por que se piensa entrenar un modelo
   → generalmente esto se hace para crear nuevas variables</rich_text>
    </node>
    <node name="Estadistica inferencial" unique_id="98" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655388312" ts_lastsave="1655742204">
      <rich_text>Descriptiva, nos ayuda a decribir y entender los datos, como se estan comportando en el presente
	- como se comportan los datos, tendencia, variabilidad y distribucion
Inferencial, inducciones se predice, para validar teorias
	- la abstraccion, a partir de una poblacion se trae una muestra
	- Muestreo
	- intervalos de confianza, sacar conclusiones y deducciones
	- validacion de hipotesis, teorias que nos cuestionaremos y se deben demostrar
	- tecnicas para evitar sesgos
	- sacar conclusiones a partir de los parametros que se tienen de la poblacion
	- estudio del grado de fiabilidad
	- se sintetiza, para entender una distribucion de la informacion, creacion y validacion de hipotesis, creacion de experimentos y modelos predictivos, para llegar a conclusiones y ver si son precisos
	
Estadisticos principales
- experimento, procedimiento que puede pasar una, multiple o infinitas
   → se puede entontrar diferentes resultados
   → si es un resultado cambiante es aleatorio
   → si no se modifican en el tiempo, son deterministas
- poblacion y muestra
   → poblacion es el denominador, es algo grande
   → muestra, extraccion de los datos de poblacion
      ⇒ este debe de ser una muestra representativa, suficiente grande para sacar una conclusion, no estadisticamente significativo
      ⇒ elegir una poblacion sesgada, debe de adapatarse dentro de lo que se va a estudiar
- Evento, resultados finales, cada uno de los resultaos posibles
- variables, caracteristicas o atributos que se tienen de la poblacion o muestra
   → cualitativos, categoricos
   → cuantitativas
      ⇒ discretos, numero enteros
      ⇒ continuas, como le peso o altura
- Probabilidad, que tan probable es obtener un evento determinado
- probabilidad condicionada, teniendo un evento a y b, que tan probable es que pase algo tomando en cuenta el otro evento

Poblaciones normales
- se de las mas habituales
- es simetrica en x y y
- sigue una distribucion en campana
- casi siempre esta la mayor concentracion en la media
- se tiene la mayotia de la informacion en un punto centrico
- moda = media = mediana

Teorema Central del Limite
- muestras
   → son la extraccion de una poblacion
      ⇒ deben de ser lo suficientemente grande para sacar una conclusion
      ⇒ no debe de estar sesgada, debe de tener diferentes atributos
   → tipos
      ⇒ aleatorio simple, siempre tiene la misma posibilidad de ser elegidos
      ⇒ sistematico, sigue un intervalo, regla o condicion
      ⇒ estratificado, se hace un muestreo basado en una de las categorias o variables
- teorema del limite central
   → la mayoria de los eventos en el mundo se explican sobre una distribucion normal

La media muestral
- media, valor cual es el promedio, entre todas las observaciones, se puede usar en multiples contextos
- moda, es el dato que mas se repite, el mas frecuente
- mediana, es el valor que nos ayuda a dividir una distribucion entre dos
- que es la media muestral, es lo mismo que la media aritmetica, la media entre todas las observaciones, es basado en la muestra tomada de la poblacion

Varianza y desviacion estandar
- nos ayuda a calcular que tan dispersa esta la poblacion o muestra, con respecto a la media
- la desviacion estandar es la raiz cuadrada de la varianza

Intervalos de confianza
- son un par o varios pares de numeros donde se estima que estara cierto valor desconocido respecto de un parametro poblacional con un determinado nivel de confianza
- nos muestran cuales son las poblacion o concentracion de numeros y con que probabilidad lo tenemos
- un indice de confianza del 99% es muy estricto, o 68% que es tambien muy usado, tambien se usa el 95%, se comportan de una manera simtrica
-  nivel de significacion
   → donde se rechaza y donde no e rechaza la hipotesis nula
   → cual es el alpha con el cual se puede encontrar el limite donde un resultado es o no es estadisticamente significativo
   → cuando el valor es menor al indice de significacion, se acepta que es estadisticamente significativo
   → se explora que no halla una anomalia de diferencia
   → en los picos, se dice si si rechazar los dos parametros que son diferentes

Calculo de intervalo de confianza
- existen dos maneras
   → conociendo los datos de la distribucion
   → no conociendolos
- calculo con un 95% de certidumbre, la concentracion lo tiene, el 5% se distribuye entre izquierda y derecha
- tabla Z para indices de confianza, se busca el 97.5%
- se sabe que que tenemos una distribucion de media de 28 y su desviacion de 4, con un indice del 80% de confianza
   → formula de la z; Z = (x - mu) / desv, mu es el promedio de la poblacion
   → se toma el valor de z usando la tabla y buscando el valor mas cercano al porcentaje de confianza que queremos, en este caso es 90 ya que se toma 10% del inicio y el 80% de confianza

Prueba de Hipotesis
- prueba de significacion, si existe una diferencia significativa entre el tamanio de la muestra y el parametro general
- pasos a seguir:
   → Se establece una hipotesis nula (H0), y una hipotesis alternativa (H1)
   → seleccionar el nivel de significancia
   → seleccionar el estadistico de prueba
   → formular la relga de decision
   → interpretar los resultados y tomar una decision

Tipos de Pruebas de hipotesis
- Distribucion de t de Student, se usa para estimar una media poblacional normalmente distribuida a partir de una muestra pequenia que sigue una distribucion normal y de la de que desconocemos la desviacion estandar, se usa cuando la muestra es pequenia
- Coeficiente de pEarson, se usa para medir la dependencia lineal entre dos variables aleatorias cuantitativas
- Analisis de la varianza (ANOVA), se usa para comprobar las varianzas entre las medias o promedios de diferentes grupos

Errores tipicos
- Contexto
   → rechazar H0 si es verdadera, error de tipo 1, evasion (alpha)
   → no rechazar H0 cuando es false, error de tipo 2, betha

Bootstrapping
- metodo de remuestro sobre una poblacion pequenia, o en distribuciones muy sesgadas
- se obtienen varios remuestreos

Validacion cruzada
- se usa al final del analisis
- para demostrar que los datos de prueba son independientes de los datos de entrenamiento
- procedimiento
   → se dividen los datos de forma aleatoria en k grupos de tamanio similar
   → se usan k-1 grupos para entrenar el modelo y uno de ellos para validarlo
   → el proceso se repite k veces usando un grupo distinto como validacion en cada iteracion

¿Qué indica el valor de Z al calcular intervalos de confianza?
</rich_text>
    </node>
    <node name="estadistica computacional" unique_id="11" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626023332" ts_lastsave="1626023332"/>
    <node name="estructura de datos" unique_id="65" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643578407" ts_lastsave="1643578415">
      <node name="lineales" unique_id="66" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643578415" ts_lastsave="1643830110">
        <rich_text># Arrays
- representacion interna de una coleccion de informacion
- caracteristicas
   → elemento, valor almacenado
   → indice, referencia a la posicion de diche elemento
- se usan debido a que guardan informacion de manera consecutiva, las listas de python crecen de manera dinamica
- generalmente tiene una capacidad
- existen de 1, 2 y 3 dimensiones, en python se recomienda no usar mas de 2 dimensiones
- los arrays son un tipo de lista
- no se pueden agregar o remover posiciones, tamaño fijo
- uso:
   → generalmente se usan en sprites de videojuegos
   → opciones en un menu
- python si cuenta con un modulo array

# Nodos y singly linked list
- consisten en nodos conectados unos a otros
- sencillas o dobles
- no se accede por indice sino por recorrido
- conceptos:
   → data, valor que se alverga
   → next referencia al siguiente nodo
   → previuos referencia al nodo anterior
   → head, primer nodo en la lista
   → tail, ultimo nodo
- los nodos se reparten en la memoria
- se usan los nodos para conectarse a otro nodo
- para creacion de estructuras mas complejas
- se usan para la optimizacion
- en las linked list no se tiene indices, se tienen que emular

# stacks
- conocidos como pilas
- basados en arrays o en link lists
- son LIFOS
- push, pop, top y bottom son sus metodos
- un stack y una lista son similares pero no lo mismo, las listas se ven afectadas por sus metodos

# queues
- FIFOs
- Rear ultimo elemento
- Front primer elemento
- Priority queues, se basa en FIFOs con elementos de menor o mayor prioridad
- pop
- add


errores
Son las dos principales categorÃ­as de estructuras de datos:
¿Qué métodos debe tener un array al crearse?
¿Qué escenarios debemos considerar en los métodos para añadir y/o eliminar nodos en una linked list?
En general, ¿qué se necesita para realizar operaciones como insertar o eliminar nodos de una linked list?
</rich_text>
      </node>
    </node>
    <node name="por que aprender CDD" unique_id="70" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1647208030" ts_lastsave="1647813119">
      <rich_text>que es data science:
- es un proceso de descubrir infromacion valiosa de los datos
- para tomar decisiones y crear estrategias
- Crear productos basadps en inteligencia artificial
- es un conjkuntos de pasos
   → obtener datos
   → transformar y limpiar datos
   → explorar, analizar y visualizar datos
   → usar modelos de mchine learning
   → integrar datos e IA a productos de SF
- es una interseccion de conocimientos
   → mates
   → ciencias computacionales
   → conocimiento del dominio

Que es inteligencia artificil?
- va mas alla del analisis y manipulacion de datos
- algoritmos para emular nuestra inteligencia natural
- reconocer patrones en grandes cantidades de datos
- generalmente solo tienen una funcion
- se usa generalmente m,achine learning
- se debn separar en datos de entrenamiento y de entrada, no pueden ser iguales
   → aprende con los datos ded entrenamiento
   → ya una vez hecho el algoritmo se le dan los datos de entrada
   → se crea una prediccion
- es una herramienta para el data science

Que es big data?
- grnades volumenes de datos, variados y veloces
- resulta complicado procesarlos con metodos tradicionales
- caracteristicas
   → volumen, muchos datos
   → velocidad, se procesan en tiempo real
   → variedad, es de todo tipo de datos, estructurados y no estructurados
   → veracidad, calidad y confibles
   → valor, deben dar un valor a la empresa
- se procesa al dividirla en partes pequenias
- tecnologias como spacrk o hadoop
- es un tipo de materia prima para data science

que no es?
- magia
- inteligencia artificial
- tener solamente metricas de algun dato sin hallazgos de valor
- tampoco son puras matematicas
- trabajar solo con big data
- por que no aprender
   → no quieres aprender constantemente
   → no disfrutas el trabajo sucio
   → no te gusta comunicar y negociar
   → no te sientes motivado por ayudar a otras personas
   → no te gusta hacer que las cosas pasen

areas de aplicacion
- machine learning
- deep learning
- RPA, automatizacion de procesos
- vision artificial
- procesamiento de lenguaje natural
- robotica
- areas
   → salud
   → procesos productivos
   → procesos comerciales
   → redes sociales

Roles
- data scientist
- data analyst
- data engineer
- machine learning engineer

Data Analyst
- utiliza los datos para obtener insights, informacion de valor que permite y ayuda para la decision de toma de valores
- extrae, analiza y reporta algo amigable para la gente de negocio
- tiene mas contacto con las demas areas de la empresa
- uso de sql y python
   → limpiar y organizar datos
   → analizar datos para identificar patrones y tendencias
   → se deben comunicar a las personas de negocios, por medio de visualizaciones
- bussiness analyst
   → tiene un conocimiento mas profundo del negocio y tiene ma comunicacion con la otras areas
- data visualization specialist
   → son especialistas en como mostras las metricas en tableros
- Herramientas y tecnologias
   → consultas a bases de datos, SQL
   → software de visualizacion de datos, power bi y tableu
   → excel y google sheets
   → usar R o Python
   → usar jupyter notebooks
   → pandas, numpy y matplotlib
   → uso de matematicas, probabilidad y estadistica descriptiva
   → saber como las organizacion usan los datos
   → consultas en SQL
   → herramientas de analisis y visualizacion
   → estadistica aplicada a analisis de datos

Data scientists
- toma datos de diversas fuentes, y los usa para crear modelos de ML para encontrar informacion valiosa
- se tomen decisiones basadas en datos
- incorpora datos al software
- obtiene limpia y procesa datos
- dise;a y utiliza modelos de ML para generar predicciones de los datos
- monitea la precision de los modelos y de los datos
- automatiza los procesos de recoleccion y limpieza de datos
- crear reportes de informacion
- incorpora datos a los productos
- programacion avanzada con POO
- machine learning y estadistica avanzada
- herramientas y tecnologias
   → programacion con python o R
   → jupyter notebooks
   → pandas numpy y matplotlib
   → algoritmos y librerias de ml como scikit-learn y tensorflow
   → bases de SQL y NoSQL
   → mates
      ⇒ algebra
      ⇒ estadistica descriptiva e inferencial
      ⇒ probabilidad
      ⇒ algebra lineal
      ⇒ calculo
- como empezar
   → como se usan los datos en las organizaciones
   → programacion con python
   → jupyters
   → estadistica y probabilidad aplicada a data science

Data engineer
- toma datos crudos y crea datos limpios para analisis
- trabaja para que el equipo tenga datos para analisis
- crea pipelines ETL
- data pipelines de ETL y bases ded datos
- extrae datos de diferentes fuentes
- transforma datos y los almacena en bases de datos para analisis
   → bases OLTP
- se usan bases de datos OLAP, especializadas
- crear las automatizaciones
- variaciones
   → data architect
   → big data architect
- herramientas y tecnologias
   → programacion con python y bases de ingenieria de software, scala
   → linux
   → automatizacion y scripting
   → jupyter notebook
   → manejo avanzado de bases de datos SQL y NoSQL
   → pandas, dask y apache spark
   → airflow
   → tecnologias cloud
   → contenedores docker
   → orquestadores kubernetes
   → mates
      ⇒ estadistica descriptiva

Machine learning engineer
- recibe el modelo que viene desde el DS y lo pone en produccion
- crear productos basados en IA
- escala los modelos
- generar una evaluacion extensiva de metricas de los ML
- contruye, escala y robustece sistemas de ML que funcionen en prod
- colaborar con los DS y otros areas de SF
- monitoreo del desempenio y funcionalidad de los sistemas de ML
- herramientas y tecnologias
   → programacion avanzada con python, java, C++
   → bases solidas de ingeniera de software, conocimiento de devops y backend
   → pandas, numpy, matplotlib
   → frameworks y librerias de ML, tensorflow, keras, SKlearn
   → despliegue de modelos
      ⇒ flask, fastapi
      ⇒ tecnologias cloud
      ⇒ contenedores
      ⇒ kubernetes
   → mates
      ⇒ estadistica descriptiva e inferencial
      ⇒ probabilidad
      ⇒ algebra lineal
      ⇒ calculo

Soft skills
- ingles
- comunicacion y storytelling
- desarrollo de pensamiento critico, solicitudes sin impacto en el negocio
- creatividad
- hacer que las cosas pasen y tomar la responsabilidad
- trabajo en equipo
- curiosidad</rich_text>
    </node>
    <node name="excel" unique_id="75" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650833576" ts_lastsave="1651240815">
      <rich_text>Consulta de informacion
- BuscarV
   → informacion incompleta
   → contrastacion de hojas y libros diferentes
   → tener todo en una “base de datos”
   → evitar busqueda manual
   → evita buscar manualmente e inserta a la base central
   → formula =buscarv("celda o valor";"[file]hoja'celdas'";"columna donde esta la informacion";0(coincidencia exacta))
   → en ingles:  VLOOKUP
- valores absolutos y relativos en formulas
   → F2 para mostrar la formula
   → se mueve en la forma de una celda a la otra, por que los mueve en secuencia
   → se cambia en la columna donde esta el data
   → se pueden fijar en excel las columnas
   → se va a la formula y se fija con $ para fijar solo las filas
   → si se pone $A$2 no se movera el valor para ninguno de los datos
   → se aniade una fila en la parte superior, esto como tip, se pone el numero de la columna a buscar, se cambia el numero hardcode por el numero que debe tomar en la celda de arriba, se tiene que fijar la columna de los numeros con E$1 (respectivamente)
   → $let$num fija columnda y fila
   → $letnum fija la columna
   → let$num fija la fila
- checar orders_has_products, order, product, product_sold_vendor
- BuscarH
   → nombre_provedor, vendor.txt
   → se modifica con T
   → =buscarh("celda o valor";"[file]hoja'celdas'";"columna donde esta la informacion";0(coincidencia exacta))
- copiar sin formulas (revisar en calc)
- cambiar las fechas de formato
- resta de fechas
- resta de fecha de hoy a cuando llego, en dias
- reemplazo de texto

- listas desplegables
   → psara hacerlas se tiene que crear una lista en otro hoja y desde datos traerla
   → se pueden tener unos estados con dependencia de otra
   → checar como se hacen listas en calc y listas dependientes

- Separar texto y juntarlo
   → MES() -&gt; MONTH() para obtener datos de fecha (mes)
   → en datos, texto en columnas para separar el texto, se tiene que poner cual es el separador
   → para concatenar, formula contatenar
   → para obtener un slice, DERECHA(col, #Chars) o IZQUIERDA

- Formulas Basicas
   → sumar -&gt; suma
   → promedio -&gt; promedio
   → contar -&gt; contar

- mas formulas
   → de manera relativa a los datos
   → aleatorio para crear un numero aleatorio
   → SUMAR.SI, (rango del criterio, “criterio”, rango de suma)
   → PROMEDIO.SI, trabaja igual
   → CONTAR.SI, same, el rango a contar es el del criterio

Nombrar rangos y ahcer operaciones
- se pueden nombrar rango para simplificar formulas, se seniala y se cambia el nombre en el cuadro de nombres

Promedios ponderados
- valor por el peso y sumar
- sumaproducto

las formulas condicionales trabajan como un if ternario
se pueden agregar formato de colores a las columnas con condicionales

- tablas dinamicas
   → se pueden incluir campos desde la tabla source
   → en las tablas dinamicas en medio se ponen valores con los que se pueden operar, ventas, ganancias
   → cada campo se puede modificar la operacion, por ejemplo por default se suma, pero se puede cambiar a saber cuantos fueron
   → se pueden hacer calculos con los datos que estan dentro de la tabla, esto para tablas dinamicas
- graficas
   → se usan para tener un mensaje mas claro
   → datos procesados y agrupado, tiene que ser lo mas especifico y claro posible
- minigraficas
   → minis en 3.6.5

- se peuden bloquear y proteger las hojas, se hacen por medio de password
- tambien se puede proteger el libro
- se puede mostrar informacion de manera interactiva tambien


La utilidad principal de Excel es:
Dos de las malas pr├ícticas al trabajar con bases de datos son:
Una técnica efectiva y rápida para verificar la efectividad de las funciones relativas es:
El uso principal de los semáforos en Excel es dar diseño y armonía a la base de datos.
</rich_text>
    </node>
    <node name="notebooks" unique_id="76" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650904726" ts_lastsave="1650928253">
      <rich_text>se pueded programar:
- en local
- en web broser
- en servidor
- en celular

Colab:
- tiempos de ejecucion y escalabilidad es uno de los temas mas importantes por los cuales usar nube
- servicio en la nube
- basado en jupyter notebooks
- no requiere configuracion
- trabajo a nivel de archivo
- uso gratuito de GPU y TPU

Deepnote
- servicio en la nube
- basado en JN
- no requiere conf
- trabajo a nivel de proyecto
- colaboracion en tiempo real
- integracion con multiple apps
- acceso a una terminar o linea de comando
- almacenamiento de variables de entorno
- publicar proyectos

#</rich_text>
      <rich_text family="monospace">Crear ambiente
conda create --name py39 python=3.9 pandas=1.2
#Ir al ambiente
conda activate py39
# Instalar boltons
conda install -c conda-forge boltons
#Devolver a un punto anterio y asi no tener que remover
conda list -r
#Devolverme a larevision 0
conda install --revision 0
#Revisar si esta instalado
conda list boltons
#Exportar tu ambiente
conda env export
#Exportar tu ambiente pero sin las versiones
conda env export --no-builds
#Exportar tu ambiente LA MEJOR
conda env export --from-history
#Exportar tu ambiente a un archivo
conda env export --from-history --file environment.yam
#Remover ambiente 
conda env remove --name py39
#Importar el ambiente
conda env create --file environment.yaml
#Ir al ambiente 
conda activate py39</rich_text>
      <rich_text>


VSCode a trav├ęs de extensiones te permite tener un ambiente de notebooks integrado. En ellos puedes:
¿Cuál de los siguientes servicios/tipos de Jupyter Notebooks te permite crear gráficas sin necesidad de código?
</rich_text>
    </node>
    <node name="bussiness intelligence" unique_id="83" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651261296" ts_lastsave="1651514026">
      <rich_text>Introduccion
- concepto: inteligencia del negocio, utilizar informacion dentro de la organizacion para tomar decisiones
- pariente cercano de la ciencia de datos, analisis descriptivo, en pasado y presente
- generacion de reporte y descubrimientos de la informacion
- ETL, extract, transform y load
- exploracion, que se puede encontrar que explica lo que esta pasando
- descubrimientos, patrones
- reporting, se concentra la informacion para la audiencia
- como??
   → visualizacion de datos
   → storytelling
   → recomendaciones
- importancia
   → ayuda para mejorar productos y procesos dentro ded la empresa

Stakeholders
- personas o grupos que rodean a la empresa, personas de interes
- accionistas, clientes, empleados, proveedores, sociedad, gobierno
- esto nos ayuda a tener mas claro a quien va dirigidos los intereses
- se tienen que entender los intereses
- se deben de priorizar
   → empresa tradicional
      ⇒ accionistas
      ⇒ clientes
      ⇒ empleados
   → startup
      ⇒ clientes
      ⇒ empleados
      ⇒ accionistas
- el BI tiene que salvaguardar los intereses de los stakeholders

Tipos de empresas
- se hace el analisis por que es muy distinto una de otra
- se tiene que entender la afectacion de cada una de las actividades cotidianas
- utilidad bruta = ingresos - costos de venta
- utilidad operativa = utilidad bruta - gastos de administracion (todo aquello que no esta relacionado con la produccion)
- utilidad neta = utilidad operativa - otros gastos e inresos - impuestos
- venta de productos
   → costos de venta, son las ventas - costos de ventas
- venta de servicios
   → generalmente los costos de venta no afectan tanto, el software es un ejemplo, se gasta mas en nomina y gastos administrativos

Income statement simplificado
- si no hay utilidad bruta, se debe detener y arreglar o cerrar la empresa
- si la operativa esta tan baja con la bruta, los gastos administrativos estan muy caros, se debe reducir la operativa con la bruta
- como afecta cada actividad y saber que es lo que se quiere mejorar
- afectacion distinta
   → costo en materia prima, si aumenta utilidad bruta disminuye
   → incremento en nomina, incremento en gastos operativos utilidad operativa disminuye
   → cambios en oficina, si aumenta, disminuye la utilidad operativa
   → aumento presupuesto de marketing, costos de operacion, utilidad operativa disminuye, se ve como un aumento de ventas
   → cambio en el precio del producto, utilidades brutas, pueden aumentar o disminuir las ventas
- se debe de pensar de lo particular a lo general

- ingreso, es todo aquello que no se le ha sustraido los gastos o costos de los bienes
- utilidad, es cuando al ingreso se le extraen los costos o gastos, mayort ingreso no significa mayor utilidad
- costos
   → costo fijo, costos que son constantes, por ejemplo la renta
   → costo variable, se pueden identificar a una actividad, a mayor actividad mayor costo
   → semivariable, puede cambiar o mantenerse
   → mayor costo no siempre significa menos utilidad

Margen de contribucion:
- elemento con que podemos entender las cosas como un pequenio resumen
- la diferencia entre el precio de los productos menos el costo de los productos
- nos ayuda a encontrar un punto de equilibrio, no hay ganancias ni perdidas, cuanto se tiene que vendar para no perder ni ganar
- PE = costos fijos / (precio venta - costo variable por unidad)

Razones matematicas en los negocios
- definicion, relacion entre dos magnitudes o valores
- se debe pensar en porcentajes
- utilidad / ingresos
- margen de contribucion / precio
- ventas de producto a / ventas totales

Extraccion de datos
- se tiene que tener muy en cuenta los ETL
   → se debe de ser cuidadoso con los extract, ya que no siempre hay informacion cierta
   → se puede encontar la informacion de silos, no siempre lo que se encuentra en un departamente esta en otro departamento
   → hay que tener cudadido con el uso de los software, SAP, Excel, Google Studio
- tipos de archivos para extraccion
   → excel, texto de csv

Limpieza de datos
- se debe procurar que todo lo que extrajimos es entendible
- garbage in, garbage out
- este es el primer acercamiento, donde se puede entender lo que tenemos de informacion

Exploracion de datos
- se hace para saber que nos dicen los datos
- se hacen preguntas que no sabiamos que existian
- que es lo que hace que sucede en ciertos patrones
- pueden llegar a cambiar las preguntas e hipotesis sociales
- software mas comunes
   → se peude usar python o R
   → programas como excel, tableu, power bi

Descubrimientos, highlights,
- los high nos ayudan a tomar las decisiones
- siempre empezar con lo mas buscado y lo menos buscado (bottom and top data)
- encontrar patrones o temporalidad
- esfuerzate en enteder la data

Reporting
- pensar en la audiencia
- pensar en el mensaje, como se va a transmitir
- saber si el mensaje va a ser estatico o dinamico, impreso es un reporte estatico, usar power bi y tableu para reportes dinamicos
- tener buenas practicas visuales
- contexto, dashboards y storytelling, la audiencia debe de saber de que se habla
- Descripcion, descubrimientos y sugerencias

Alertas, automatizacion y live reporting
- el li e reporting es el reporte en vivo, un problema es la falta de tiempo real al hacerse este analisis
- se automatiza por medio de software
- lertas, KPI, metrivas y colores (tomarle cierta atencion al reporte en el momento)

Eficiencia, max profit y min cost
- la eficiencia nos ayuda a mejorar un proceso, y los recursos dedicados a este proceso
- siempre tenemos que buscar la utilidad
- siempre se debe de intentar minimizar los costos
- siempre pensar en la jerarquia ded los stakeholders


La nĂ³mina es un ejemplo de costo...
Elige la f√≥rmula para encontrar el punto de equilibrio, utilizando el margen de contribuci√≥n
Podemos saber mucho sobre la empresa simplemente leyendo el estado de resultados. Esto es: 
Si tu proveedor de materia prima decide aumentar el precio de los materiales con los que fabricas tus productos, ¿qué utilidad se vería afectada? Considera que todo lo demás permanece igual.


¿Qué significa ETL?
¿Cuáles son las fases del proceso de Business Intelligence?
Si tu proveedor de materia prima decide aumentar el precio de los materiales con los que fabricas tus productos, ¿qué utilidad se vería afectada? Considera que todo lo demás permanece igual.
</rich_text>
    </node>
    <node name="numpy-pandas" unique_id="82" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651261091" ts_lastsave="1651528707">
      <rich_text>Numpy
- es muy veloz, 50 veces que usar listas en python o en C
- optimiza almacenamiento en memoria
- maneja distintos tipos de datos
Pandas
- herada ventajas de numpy
- maneja multiples archivos, formatos
- alineacion inteligente
- se puede hacer analitica, etls y mas

Array
- estructura central de NP
- representa datos de manera estructurada
- indexado
- acceso a uno o muchos elementos

Tipos de datos
- ints

Dimensiones
- scalar dim 0
- vector dim 1
- matriz dim 2
- tensor dim &gt;= 3

Se tienen series y dataframes
las series es un arreglo unidimensional, busqueda por indice y slicing, operacion aritmeticas y manejo de varios tipos de datos
dataframe, es una estructura matricial, estructura principal, de dos dimensiones, indices en filas y columnas</rich_text>
    </node>
    <node name="visualizacion de datos" unique_id="9" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623292658" ts_lastsave="1655388356">
      <rich_text># Que es la visualización
- el input: es la parte donde se inicializa la visualizacion de datos, se pone de forma estructurada o no estructurada
- el output: es el producto final que sirve para reconocer patrones de forma visual
- para que se usa: reconocer patrones, historias...
- es importante por que estamos en la era de la informacion

### Florence Nightingale

# Importancia
- la dificultad para retener informacion del humano
- carga cognitiva: es el esfuerzo para retener la informacion
- nos ayuda a entender nuestra información
- ben Shneiderman, la visualizacion da respuestas a preguntas que no sabias que tenias
- sirve para comunicar mejor

# Buenas practicas
- define la audiencia y motivo
- utilizar la percepción visual
- estandariza, no usar tendencia engañosa, usar mismas medidas con compartivas, no cortar axis, alinear siempre
- simplificar pero no recortar
- disminuye el sesgo... no tener las preferencias personales
- no al cherry-picking, retomar o tomar datos que solo muestran nuestro punto
- principios gestalt, ley de proximidad, ley de similitud, ley de la continuidad

# Conflictos de ética
- la audiencia cree y escucha lo que mostramos
- credibilidad y mensaje, si se pierde la confianza es dificil volver a obtenerla, los datos deben contar sus propias historias
- segmentación

# Graficas
- son importantes para enviar mensajes, en especial para gente que no tiene tiempo de hacer interpretacion de los datos
- dataviz, termino de visualizacion
- de barras:
   → es una representacion de barras
   → hace comparacion rapida
   → son datos por categorias, que se agrupan por frecuencias
   → existen de diferentes formas, verticales, horizontales y de stack(aqui se une el 100%)
   → se debe usar un color distinto para cada categoria
   → representar de mayor a menos a menos que sea escala de tiempo
- de pie (pastel):
   → es un circulo, donde las categorias tienen una representacion por área
   → debe de ser muy sencilla
   → se puede simplificar en una grafica de dona
   → se peuden poner anotaciones para ver el valor preciso
   → no usar graficas en 3D afectan la percepsion visual
   → no usar mas de 6 categorias
- dispersion:
   → es posicionar en un plano dos variables, es la mas comun en la ciencia de datos
   → colores son importantes
   → se debe entender la dispersion de los datos en lo plano
      ⇒ correlacion positivo
      ⇒ correlacion negativa
      ⇒ sin correlacion
   → no poner muchas anotaciones ya que ocupa muchos puntos
- de burbujas:
   → es una variacion de la scatter plot
   → muestra el tamaño de la populacion
   → no debe de tener necesariamente un grafico con correlacion de grafica
   → uso de colores para definir categorias
   → no usar graficos 3D
- de mapas:
   → datos ubicados geograficamente
   → simplificar anotaciones
- heatmap:
   → permite sobreponer sobre otra visualizacion una paleta de colores, que nos ayuda a saber la frecuencia que tenemos
   → nos muestra los lugares en donde mas se repiten sucesos
   → se pueden usar dentro de graficas de tabla
   → se usa mucho sobre paginas web, para saber por donde pasan mas los cursores
   → calibrar la paleta de colores
- tablas:
   → representacion de manera ordenada
   → se utiliza cuando se quiere entregar un mensaje acompañado de otro datavis
   → solo si son expertos en la materia, no utilizar datos extensos

# Como afecta en el bussiness
- direccion y gerencia, estan corto de tiempo, entender en el menor tiempo posible, bajando la carga cognitiva
- nos sirve para comunicarnos con el propio equipo
- eficiencia y mejora

# Explora, descubre y pregunta
- los datos son numero o letras
- ofrecer informaciòn relevante
- explorar en las bases de datos o informacion para conocer que se tiene ahi
- trabajar en equipo es fundamental
- tomar decisiones por medio de lo anterior

# Business intellingence
- referido a informaciòn del negocio
- data visualization, lo usa para interpretar desiciones para las altas gerencias
- generalmente es gente que entienden el negocio
- ellos identifican los movimientos principales y conocen cada medida para mejorar el negocio

# Recoleccion de datos
- los datos son muy diversos
- se tienen bases publicas y privadas
- la informacion puede venir de informacion estructurada y no estructurada
- distintos tipos de archivos y fuentes

# Limpieza de datos
- estandarizar formato
- GIGO / RIRO, entra basura, sale basura, se tiene que limpiar lo mejor que se puedan los datos
- preparacion -&gt; visualizacion

# Exploracion de datos
- Descubrir, preguntar, reformular y analizar
- es importante contar historias
- evitar errores, bias, cherry picking

# creacion de graficas
- se usa para entregar mensaje a los altos mando
- que quiero comunicar
- que se adapta mejor a mi mensaje
- quien es mi audiencia
- no olvidar las buenas practicas

# Generacion de reportes
- son aglomerado de las data viz
- son para la audiencia para que se tomen decisiones
- concentrar los resultados, se debe enfocar en un solo mensaje
- a mayor retencion menor esfuerzo, carga cognitiva baja

# Definir KPI
- sirven para saber si se puede mejorar o no
- dependen del area donde se este, estos son especiales para cada area donde uno se encuentre
- SMART:
   → </rich_text>
      <rich_text weight="heavy">S</rich_text>
      <rich_text>pecific
   → </rich_text>
      <rich_text weight="heavy">M</rich_text>
      <rich_text>easurable
   → </rich_text>
      <rich_text weight="heavy">A</rich_text>
      <rich_text>chievable
   → </rich_text>
      <rich_text weight="heavy">R</rich_text>
      <rich_text>elevant
   → </rich_text>
      <rich_text weight="heavy">T</rich_text>
      <rich_text>ime-bound</rich_text>
    </node>
    <node name="Visualizacion de datos tableau" unique_id="84" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651530656" ts_lastsave="1652027094">
      <rich_text>Por que es importante aprender
- fundamental,representacion graficade informacion
   → input: informacion (data)
   → output: revelaciones (insights) e historias
- se tiene muchos datos, la visualizacion nos ayuda a encontrar las historias dentro de esos datos

Tableau:
- excel es de las primeras herramientas donde se pueden visualizar datos
- power BI, transicion natural del excel al BI
- google analytics, para mercadotecnia, generalmente para campanias y anuncios
- tableau, permite hacer dashboards, visualizaciones y narracion de datos
- se maneja bastante arrastre y suelte
- la comunidad es una de las mas importante en la visualizacion de datos
   → cda lunes se tiene un makeoverMonday
   → Viz o the day es la mas bonita de todos
   → ironviz todos los nominados de makeoverMonday pasan a esta parte
- errores mas comunes
   → se deen de usar buenos colores, dependiendo del contexto
   → los valores comparativos deben de ser identicos y estandarizados
   → en un reporte se debe de hacer eficiente que se quiere demostrar
   → rerevisar tipo de presentacion, dinamica o estatica
   → se debe tener un color diferente para cada categoria
   → grafica de arbol??
- defiir proposito de las visualizacion
   → cual es el mensaje
   → se debe definir el tipo de perfil, se le debe de poner un nombre, predecir preguntas que podriamo tener
   → como lo hare
   → preguntas claves que debe responder la presentacion

Storrytelling
- story board en tableau
- se presentan los descubrimientos en forma de historia
- se debe definir la estructura de la historia
   → contexto
   → resumen
   → puntos de interes
   → conclusiones
   → recomendaciones
</rich_text>
    </node>
    <node name="visualizacion de datos python" unique_id="86" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652031812" ts_lastsave="1655388334">
      <rich_text>la visualizacion de los datos es importante por que da respuesta  de preguntas que no sabiamos que teniamos, se pueden tener datos distintos con los mismo estadisticos
ayuda a entontrar hallazgos en los datos de forma mas facil
mucho uso en DS e IA para sabe que tan bueno es el modelo
matplotlib
	- 2003
	- basada en numpy
	- emula matlab
	- escrita en python
	- es bastante simple
	- es rapida
	- es personalizable
seaborn
	- esta montada sobre matplotlib
	
Metodo orientado a objetos:
- pyplot
   → rapido
   → facil
   → una sola figura
- OO:
   → mayor personalizacion
   → mas amigable a multiples diagramas
   → mas codigo

Seaborn
- construido sobre matplotylib
- integrada para estructuras de pandas
- velocidad
- poco codigo
- customizable

Set
- sirve para definir temas, colores, paletas que se van a usar

Sentencia de Matplotlib para crear un lineplot de x, y con puntos azules:
¿En la sentencia de Matplotlib "fig, axes = plt.subplots(nrows=1, ncols=2)" axes de qué tipo se le asigna un objeto?
</rich_text>
    </node>
    <node name="funciones matematicas para DS e IA" unique_id="85" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652028696" ts_lastsave="1652291243">
      <rich_text>Las matematicas son un lenguaje
las funciones nor permiten modelar nuestra realidad

que es una funcion?
- funciones de una sola variable, es por que depende de una sola cosa para que varie
- una funcion es copmo una maquina, donde entra un elemento x para que salgo una y
- es una regla donde cada elemento de un conjunto A se le asina un conjunto de B
- se puede representar:
   → verbalemente, habladito che
   → numericamente, generalmente una tabla de valores
   → visualmente, por medio de grafica, donde represente los valores
   → algebraicamente, la mas guapa, y = f(x) = x
- tipos de variables
   → variables cualitativas
      ⇒ nominales, se les asigna una cualidad, por ejemplo los colores
      ⇒ ordinales, representan un orden, como se puede acomodar un objeto
      ⇒ binarias, solo tomand dos valores, unos y ceros
   → variables cuantitativas
      ⇒ discretas, representan un numero de valores finitos, son puramente enteras, se separan por pasos
      ⇒ continuas sus valores pueden ser infinitos, toman valores dentro de los numero reales
- dominio y rango
   → que valores pueden tener las funciones?
      ⇒ dominio, son los valores que toma x-independiente
      ⇒ rango son los resultados que nos da una funcion
- simbolos: </rich_text>
      <rich_text link="webs https://laboratoriomatematicas.uniandes.edu.co/semarquitec/simbolosmat.htm">https://laboratoriomatematicas.uniandes.edu.co/semarquitec/simbolosmat.htm</rich_text>
      <rich_text>
- conjuntos:
   → de forma grafica y circular
   → el universo se representa con omega
   → union, todos los elementos de los conjuntos se unen, se representan con U
   → interseccion, son solo los elementos que se comparte: se representa con una U volteada
   → perteneces es como una E: a E A, la E es curveada
   → se tiene el conjunto vacio que se representa con una circulo con una linea enmedio o con {}
   → conjuntos de numeros
      ⇒ naturales, son todos los enteros sin incluir el cero, discreto
      ⇒ enteros, son todos los enteros negativos y positivos, tambien es discreto
      ⇒ racionales, incluyen decimales y fracciones, a/b pertenecen a los enteros, donde b es diferente de 0
      ⇒ Irracionales, todos aquellos numero que tienen una expansion de decimal que no sse pueden escribir as: a/b, por ejemplo pi, euler
      ⇒ naturales es la union de todos los demas conjuntos, hay naturales positivos y naturales negativos
- Funciones y mas funciones (algebraicas y trascendentales)
   → el notebook esta en visualizacion_matplot
   → funcion lineal, tiene la forma f(x) = mx + b, solo depende de uno que es completamente lineal en este caso X
      ⇒ el dominio y rango van de -inf a inf
      ⇒ las funciones que grafican las computadoras son discretas
      ⇒ b = bias, donde corta x
   → polinomicas, es un polinomio de X, de x a la n hasta x a la cero
      ⇒ el grado es el numero mas grande que tiene nuestra funcion
      ⇒ generalmente los pares hacen curvas de concavas, los impares no
   → funciones trascendentes, no se pueden hacer por medio de sumas de polinomios, el caso de funciones trigonometricas, exponenciales, logaritmicas
   → funciones seccionadas, nciones que se comportan diferente dependiendo de los trozos, por ejemplo la funcion de Heaviside
   → funciones compuestas, es la combinacion de varias funciones, es lo llamado (f o g ), pasa por una funcion primero y luego pasa por la otra, en f o g, siempre pasa primero la evaluacion de g luego la de f
   → Funciones reales:
      ⇒ sus dominio y rango estan contenidos en el conjunto de los reales
- el perceptron
   → es la forma de como se define una neurona artificial
   → diferentes seniales de entrada, varios pesos, union sumadora, una funcion activadora y una salida
   → pesos sinapticos, numeros encargados de ponderar lo que entra
   → la funcion sumadora nos permite sumar el valor de entrada por su peso ponderado
   → carlos alarcon redes neuronales
- funciones de activacion
   → sirven para dar funciones de salida diferentes al perceptron
   → funcion lineal, lo que pasa lo regresa igual, sirve para mantener valores a travez de un proceso
   → funcion Heaviside, sirve para hacer clasificaciones categoricas, un booleano
   → sigmoide, sirve para un proceso de regresion logistica, sirve mucho pra probabilidades, los negativos siempre se acercaran al cero, al tocar el cero en x, se va la mitad mientras mas grande sea el valor tenderan al 1
   → tangente hiperbolica, funcion de escalamiento, va de -1 a 1, tiene que ver muhco con back propagation, si se acerca a 0 va a valer cero, si son muy grandes tienden a ser 1 y si son negativos a -1
   → ReLU, si el valor es 0 o menor que cero lo pone como cero, si es mayor que cero lo pondra como x, si exite lo tomo en cuenta si no no lo tomo en cuenta, existe una variacion llamada liquid ReLu
- regresion lineal simple, 
   → punto de corte es bias
   → inclinacion es la pendiente
- calculo del error:
   → el error es que tan lejos estoy de valor original al de prediccion
   → e = y_pred - y_real
   → |e| = |y_pred - y_real|
   → e**2 = (y_pred - y_real)**2
   → E = SUM_i=i-N (y_pred - y_real)**2
   → ECM = 1 / N (SUM_i=i-N (y_pred - y_real)**2)
</rich_text>
    </node>
    <node name="postgresql" unique_id="92" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654044994" ts_lastsave="1655318818">
      <rich_text>postgres es un motor de base de datos,
lenguaje, PL/PSQL
servidor, es donde se instala el motor, fisico
motor, es el que permite estrucutrar todo dentro del server

ACID:
A - atomocity, se peuden separar las funcion en pequenias tareas y unidas en un todo
C - consistency, todo lo que se relaciono al objeto relacional, los datos tiene congruencia entre si
I - isolation, se pueden tener varias tareas ejecutandose al mismo tiempo
D - durability, se tiene la seguridad de que no se pierde la informacion

se debe de crear la base: transporte_masivo, tabla viajero (id_viajero, nombre, fecha_registro)
usar version 11.5

consola, acceso por psql:
- \l , lista todas las bases de datos
- \dt, tablas que tiene la base de datos
- \c {nombre base de datos}, para cambiar la base de datos
- \d {tabla}, sirve para describir la tabla
- \h, para obtener todas las funciones
- \h {funcion}, nos muestra la documentacion de la funcion
- \g, permite volver a ejecutar la consulta anterior
- \timing, esto ayuda a saber cuanto se tardo en demorar los queries

archivos de configuracion
- tres archivos:
   → postgresql.cong
   → pg_hba.conf, aqui se modifica como sera el acceso para la base de datos
   → pg_ident.conf, permite mapear usuarios del usuario con el usuario de la base de datos
- en pgadmin:
   → SHOW config_file;
- el archivo esta generalment en la carpeta de configuracion
- </rich_text>
      <rich_text link="webs https://pgtune.leopard.in.ua/">https://pgtune.leopard.in.ua/</rich_text>
      <rich_text>

tipos de datos:
- numericos
- monetarios
- texto
- binarios
- fecha/hora
- boolean
- geometricos
- direccion de red
- tipo bit
- XML, JSON
- arreglos

Disenio de la base de datos:
- estation
   → id
   → name
   → direction
- train
   → id
   → model
   → capacity
- pasajero
   → id
   → name
   → address
   → date
- trayecto
   → id
   → name
   → id_train
   → id_station
- viaje
   → id
   → id_pasajero
   → id_trayecto
   → inicio
   → fin

tablas
- tiene la estructura e informacion de la base de datos
   → CREATE, crear tabla
   → ALTER, alterar la tabla
   → DROP, borra la tabla

particiones
- sirven para cuando se tiene mucha informacion en una tabla
   → separacion fisica de datos, se guardan varias partes de la tabla en el mismo o diferentes discos
   → estructura logica, se puede usar el mismo select, pero se pueden separar por fechas por ejemplo

Roles
- que puede hacer:
   → crear y eliminar
   → asignar atributos
   → agrupar con otros roles
   → roles predeterminados
- se debe de crear un usuario con los permisos necesarios para no borrar algo que no se deba de borrar

LLaves foraneas
- aqui deben de ser congruentes si existe en una tabla debe de existir en la otra
- tabla de origen
- tabla destino 
- acciones

SQL Join:
- teoria de conjuntos implementada en sql
- JOIN == INNER JOIN

Funciones especiales:
- On CONFLICT DO {action}, soluciona problemas para insertar datos o modificar, que no se puedan hacer y que despues podemos arreglar
- RETURNING {campo}, devuelve todos los cambios que se hicieron
- LIKE/ILIKE, busqueda estilo expresiones regulares
- IS/IS NOT, permite comparar dos tipos de datos no estandar
- COALESCE, compara dos valores, regresa el que no es nul
- NULLIF, compara dos valores y si son iguales ergresa nulo
- GREATEST, Compara un arreglo de valores, regresa el mayor
- LEAST, Compara un arreglo de valores, regresa el menor
- BLOQUES ANONIMOS permite agregar condicionales dentro de una consulta de datos

VISTAS:
- se usan mucho cuando la consulta se repite muchas veces
- volatiles, valores mas constante de consultar, tiende a cambiar si cambian los datos
- materializadas: persistente, sirve para datos de un dia atras
- sirve para ejecutar algo mas simple
- cuando la vista materializada no tiene datos, falla, se usa REFRESH para traer los datos

PL/SQL
- procedimientos almacenados
- se desarrolla codigo dentro de la base de datos
- para crealas se tiene que hacer
   → DO $$ {codigo} $$
   → RAISE NOTICE, lanza un error en consola
   → DECLARE se usa para variables, {rec} {record|type} := {value}, con record se trae toda la fila
   → FOR rec {SELECT} LOOP

TRIGGER:
- dispara acciones que se activan cuando pasa un insert, update o delete en una tabla
- para trigger se debe de cambiar el retorno, se regresa un TRIGGER
- si ponemos OLD no se hace el cambio
- si se pone NEW si se permite el cambio

Datos externos:
- DBLink, conexion a datos remotos con una consulta

Backups y restauracion
- pg_dump, custom (formato unico), tar (comprimido), plain(sql plano), directory (estructura sin comprimir), se puede seleccionar todo lo que tiene la base o solo datos, tablas...
- pg_restore

Mantenimiento:
- se hace sin nuestro concentimiento, trabajan en segundo plano
- Vacuum, vaciado, quita lo que no esta funcionando
   → liviano, lo hace constantemente
   → full, bloquea las tablas para hacer la limpieza, en tablas muy grande
- se puede hacer a nivel de base de datos o de tablas
- vacuum, la mas importante
   → full, quedara limpia en su total
   → freeze, durante el tiempo se congela
   → analyze, el mas suave, aplica revision y no aplica cambios
- analyze, no hace cambios en la tabla, hace la revision y dice como esta la tabla
- reindex, aplica para tablas que tienen indices entre ellos, pasa si los indices son mas grandes que las tablas 
- cluster, reorganiza la informacion en el disco

Replicas
- mecanismos que permiten evitar probelmas de entrada y salida en los OS
- piensa siempre en modo replica
- IOPS, limitante a nivel de OS, 

Crear dos servidores
- master
   → en configuracion
      ⇒ wal_level = hot_standby
      ⇒ max_wal_senders = 2
      ⇒ archive_mode = on
      ⇒ archive_command = ‘cp %p /tmp/%f’
   → en pg_hba.conf
      ⇒ host   replication   all   {ip}/32    trust
   → se debe reiniciar
- replica
   → se debe de parar
   → se borra rm -rf /var/lib/psql/data/*
   → pg_basebackup -U {user} -R -D /var/lib/pgsql/data/ -- host={ip_master} --port={port}, se copia la base
   → se modifica en configuracion
      ⇒ hot_standby = on
   → se reinicia


POSTGRES para ciencia de datos

historia de las bases de datos
- se usaban de manera texto, manera plana un txt, obtener estos datos era complicado y mas procesarlos
- es un esfuerzo matematico para hacer algo mas complejo

Puntos fuertes:
- se cree que estan un poco obsoletos, cosa no cierta
- multiproposito
- ampliamente utilizadas
- informacion consistente
- flexible
- retrocomplatible
- complemente programable, permite usar lenguajes para hacer algo ams complejo que un CRUD

Conceptos:
- entidades/tablas: cosas del mundo real
- atributos: son los atributos que tienen las entidades
- relaciones: conexiones entre entidades
- trigger, son funciones a ligar los store procedur con un evento
- store procedure, funciones o procedimientos que se guardan en el manejador de base de datos una y otra vez, sin repetir codigo

Principales sentencias SQL
- se usan las mismas que postgresql

Que es la ciencia de datos:
- el cientifico de datos comunmente lidera el equipo
- se requiere saber cual es lo que el negocio require y las necesidades de datos, para accionar con los demas equipos
- se debe entender el negocio, ya que lo ams grande que hace es el impacto que tiene sobre la empresa
- generalmente en las startup se es one man army, que es el que hace todo con data
- aplicacion:
   → la parte mas fundamental es en la toma de decisiones
   → data driven: se toman las decisiones basadas en datos, se debe de saber que representan
   → informacion significativa, tiene que ver con la historia a contar de los datos, sabiendo que se puede aprovechar
   → formato de acuerdo para el publico adecuado
   → neutralidad de datos, tener cuidado con la discriminacion mas con los datos que se toman
   → se tiene que contar una historia
- equipos orientados a datos:
   → DBA, el administrador de base de datos, es el primer rol creado
   → Data warehouse, almacen de datos, cuando ya no es posible en 1 o 2 servidores, para guardar millones de datos
   → ETL/Data pipelines, obtener datos, transformarlos y guardarlos en otro lado
   → BI, inteligencia del negocio, precursor del data science, traer los datos y entender los patrones que nos estan dando
   → ML, tecnica que ayuda para clasificacion de gran cantidad de datos o predecir basandose en data historica
   → data science, entender a la organizacion e impactar de forma positiva, las necesidades de la organizacion y planear un camino
- ds vs ml
   → machine learning, es un conjunto de ciencias, estrategias y algoritmos para usar en la compu, con esto se pueden resolver varios problemas, existen 2 tipos:
      ⇒ clasificacion, toma datos y encuentra en los datos grupos, patrones por medio de ciertas caracteristicas
      ⇒ predicciones, toma datos historicos, con fecha y temporalidad, por medio de patrones busca la tendencia que hay en ellos
   → ciencia de datos, tiene que ver mas con conocer a la organizacion o empresa, puede ser por medio de un equipo o uno solo
      ⇒ debe de impactar de forma positiva a la organizacion
   → ml es una herramienta fundamental para el data science
   → la herramienta adecuada para el trabajo adecuado

Diferencias entre manejadores
- filosofia de codigo abierto y orientado a la comunidad
- sumamente adaptable, manejo de documentos json, funciones orientadas a estdistica, lenguaje PSQL
- se dedica a manejo de objetos OOB
- se tienen particiones
- common table expresion, tablas virtuales, se crean en momento
- window function, encontrar relacion entre un row y el resto de registros

PLPGSQL
- programacion mas ordenada y procedural
- para creacion de triggers
- se ejecutan los pasos del procedimiento pero no se extrae ningun valor
- existen stored procedure (son estandar y no regresan valor) y function (no son estandar, mas avanzadas y regresan un valor)

en postgresql se tienen tipos de datos especiales, que son los datos que crea el usuario (aparte de los JSON)
tipo de dato lista, sirve como validador

Pensando en la presentacion de los datos
- el reto es mas grande es saber que datos debemos extraer y como mostrarlos
- planeacion
   → es hacia donde queremos hacer todo lo demas
   → saber que queremos probar con nuestros datos
   → como obtener los datos de acuerdo al proposito que tenemos
- la forma mas comun para este tipo de informacion son los dashboard
   → tienen diferentes significados, dependera de l uso o lo que queremos saber
   → la presentacion siempre es muy importante

Trabajando con objetos (json)
- generalmente se guarda un cadena, donde generalmente se vuelven mas lentas y complicadas
- PG tiene dos implementacion JSON y JSONB (es mas veloz JSONB)
- como extraer la data json

Agregando objetos
- sumarizacion de datos, obtener cosas como maximos, minimos, promedio y demas

Common table expression
- se usan para ahorrar memoria
- lenguaje pensado para consulat de datos estructurado
- no tiene estructuras de control ni iterativas, para eso se usan las common table expresion
- manejo de WITH RECURSIVE

Window Functions
- relacionar un row con otros row dentro de la tabla
- generalmente se usan para hacer ranking
- dense rank, diferencia cuando se tiene un rango igual

Particiones
- son de manera explicita
- casi todas lo hacen, pero lo hacen internamente para mejorar los datos
- cada seccion contiene un rango
- en las particiones no existen como tal las llaves primarias
- aqui el indice y clave es por donde se particiono la tabla</rich_text>
    </node>
    <node name="Machine learning" unique_id="93" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654702111" ts_lastsave="1663360401">
      <node name="ML y DS en python3" unique_id="94" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654702126" ts_lastsave="1654702126"/>
      <node name="MLOps" unique_id="91" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="11" is_bold="0" foreground="" ts_creation="1652713315" ts_lastsave="1656961210">
        <rich_text>- first week
   → putting ml in production
      ⇒ good use of tools and practice
      ⇒ goal: predict the duration of a taxi trip
      ⇒ desing -&gt; train -&gt; operate, using an API to calculate aprox time of trip
   → env. preparing
      ⇒ use AWS platform
      ⇒ creating an EC2 instance
         • Ubuntu
         • x86-64
         • using t2.xlarge (4vcpu - 16 GiB ram - 30Gb rom)
      ⇒ adding configuration to ssh/config to connect
      ⇒ use anaconda with python, to avoid licence in anaconda push ‘q'
      ⇒ another tolls is docker and docker compose
      ⇒ export PATH="${HOME}/soft:${PATH}" to use docker-compose ?_?
   → experience with programming (python)
   → being comfortable with command line
   → exposure to ML is helpful
   → dataset used is NYC taxi
   → only use of year 2011 and nexts
   → D
   → #66DaysOfMLOps
- is necesary the order in the notbook, for example:
   → read_data
   → train and validate data
   → multiple ml
   → save of model in bin
   → the model contains experiment tracker and model registry
- load an prepare data
- vectorize the values -&gt; dct values
- train in different values -&gt; model
- machine learning services, to check time of trip
- the model is deploy and before is necesary the monitoring, to check that model is effective, exclude de human using the pipeline to create model v2

Experimenting Tracking
- concepts:
   → ML experiment: the process of building an ML model, A/B testing
   → Experiment run: each trial in an ML experiment, start playin with data and hiperparameter
   → Run artifact: any file that is associted with an ML run
   → Experiment metadata
- Experiment tracking:
   → (using differents version of data, multiple experiments):
   → process of keeping track of all relevant infromation
      ⇒ Source code
      ⇒ environment
      ⇒ data
      ⇒ model
      ⇒ hyperparameters
      ⇒ metrics
   → why is important?:
      ⇒ main, to get a history and know what changes have the model
      ⇒ reproducibility
      ⇒ organization
      ⇒ optimization
   → in spreadcheets:
      ⇒ why is not enough?
         • Error prone
         • No standart format
         • Visibility &amp; collaboration
         • Copy and paste the values and results (human error)
         • if a new DS read this, probably is very difficult to he
         • the collaboration is fundamental
   → MLFlow
      ⇒ Open source platform for ML Lifecycle
         • tracking
         • models
         • model registry
         • projects
      ⇒ allows you to organize experiments and track of:
         • parameters
         • metrics
         • metadata, for example tags to search
         • artifacts
         • models
      ⇒ extradata:
         • source cide
         • version of code (git commit)
         • start and end time
         • author
- Getting started with MLFLOW
   → is necesary requirements
   → reqs is a good practice
   → using conda to create env.
   → pip install -r requirements.txt
   → pip list to show packages and version
   → to run mlflow:
      ⇒ mlflow ui --backend-store-uri {str_conn}
   → if the experiment does not exists is created
- Experiment tracking
   → add parameter tuning tieh notebook
   → show how it looks
   → select the best
   → autolog
   → more logging in this mlflow
   → is possible see different visulization of runs
- machine learning lifecycle:
   → data sourcing
   → data labeling
   → data version
   → Model Management:
      ⇒ experiment tracking
         • model training
         • model evaluation
         • model architecture
      ⇒ model versioning
      ⇒ model deployment
      ⇒ **scaling hardware
   → prediction monitoring
   → folder system
      ⇒ error prone
      ⇒ no versioning
      ⇒ no model lineage
- model registry
   → the new implementing model, and is necesary deploy the model
   → is very difficult if only gets one email with the model, for example if fails in prod is not posible the rollback
   → model registry, models with parameters, tracks
   → with the registry is possible know what model is used in that stage
   → manage different stages, staging, production and archive
   → by default if one model is send to stage and exists another model in this state the existing is send to archived, but is possible send to stage, unmark when send another to archived
   → model managment provides:
      ⇒ model lineage
      ⇒ model versioning,
      ⇒ stage transitions
      ⇒ annotations

HomeWork 2
- version: 1.26.0
- num file in output folder: 4
- parameters: 19
- --default-artifact-root
- best rmse: 6.628
- best rmse: 6.55

MLFLOW in practice
- different scenaries:
   → a single data scientist, competition model
   → a cross-functional team with one data scientist, one model
   → multiple data scientist on multiple models
- differnet configurations
   → backend store
      ⇒ local
      ⇒ SQLAlchemy compatible
   → artifacts store
      ⇒ local
      ⇒ remote (s3...)
   → trackign server
      ⇒ no tracking server
      ⇒ localhost
      ⇒ remote
- in the folder 02/examples, exists files with the 3 scenaries
   → in first scenario is created by default a folder with name mlruns, with folder 0 and using yml meta
   → in case of new experiment is create folder with number 1 and save all data in artifacts
   → if run mlflow ui with the folder created by default this load the files
   → in scenario 2, this is using local database, and tracking server, is used if I need test with multiple hyper params in the model
   → in this escenary is necesary use the mlflow server and add the artifacts-save
   → created one folder artifacts_local, in this is saved the artifacts of model with metadata
   → scenario 3, this multiple DS working in multiple models, the remote server is in EC2, s3 buckets and using postgres db
   → in this scenario is used the url of server and the aws profile
   → a good practice of devops is create a role for the instance that connect to s3, is not a good idea use a user IAM
   → other good practice is: the db without external connection only for the ec2
   → in this scenario the artifacts is saved in s3
   → remote benefits:
      ⇒ shared experiment
      ⇒ collaborate
      ⇒ give more visibility of the data
   → issues
      ⇒ security, restrict access to the server
      ⇒ scalability
         • check deploy mlflow on aws farget
         • check mlflow at company scale
      ⇒ isolation
         • define standard for naming experiments, models and a ser of default tags
         • restrict access to artifacts
   → limitations
      ⇒ Auth and User
      ⇒ data versioning
      ⇒ model/data monitoring &amp; alerting, this is outside of the scope of MLFlow
   → alternatives:
      ⇒ neptune
      ⇒ comet
      ⇒ weights &amp; biases

- Negative engineering and workflow (orchestration)
   → explain what is a orchestation
   → set of tool to scheduled and monitoring, that work that you want yo accomplish, for example a ml pipeline
   → 90% time spent, another 10% is for work in modeling:
      ⇒ retries when api go down
      ⇒ malformed data
      ⇒ notification
      ⇒ observability into failure
      ⇒ conditional failure logic
      ⇒ timeouts
   → prefect is a solution to this negative

Prefect
- open source workflow orchesttration
- python base
- modern data stack
- native dask integration
- use dag-free workflow
- extraordinary developer experience
- transparent and observable orchestration rules
- a good practice: if __name__ == ‘__main__’:
- to start prefect: prefect orion start

Prefect on VM machine
- is his lection is used EC2, in my case I treid to create a docker
- is important that port 22 is open in SG rule, http/https and port tcp/upd in port 4200 to all
- in machine is necesary run conda and install orion prefect
- command: prefect orion start --host 0.0.0.0
- configure in local:
   → prefect config view
   → prefect config set PREFECT_API_URL="http://{ip}:{port}/api"
   → python {script}

Deployment prefect flow
- storage
   → prefect storage ls
   → prefect storage create
- prefect deployment create {file.py}
- </rich_text>
        <rich_text style="italic" foreground="#fd5e53">SubprocessFlowRunner</rich_text>
        <rich_text> is used to deploy in local storage
- prefect work-queue preview {queue id}
- prefect agent start {queue id}

Work Queues and agents
- is possible separate by tags the deployments, is possible get more control

Trhee ways of deploting a model
- experiment tracking:
   → desing
   → train, training pipeline
   → operate, deployment
- deployment:
   → batch, offline
      ⇒ run regularly
      ⇒ good when execute every time (hourly, daily, monthly)
      ⇒ get data from database
      ⇒ predicts, generally with last time used (hourly, daily or monthly)
      ⇒ save the result in another site
      ⇒ generate reports
   → online
      ⇒ running all the time
      ⇒ variances:
         • web service, http access
            ◇ this is one to one client-server
         • stream service, events listening
            ◇ using producers and cosumers
            ◇ this is possible many to many
            ◇ multiple services multiple consumers in the same time
            ◇ one example is a content moderation for example Youtube
               ▪ check copyright violations
               ▪ Porn explicity
               ▪ violence
- marketing:
   → the user send a petition and get the result

Web services, using flask and docker
- using the normal duration-prediction
- to test install libraries: `</rich_text>
        <rich_text foreground="#c0c0c0">pipenv install scikit-learn flask --python=3.9</rich_text>
        <rich_text>` this use pipenv
- to run: pipenv shell
- to test dependencies: pipenv install --dev package

Web Service from MLFlow
- Is necesary start mlflow server with artifacts in s3
- get id of model-run
- set_tracking_uri to mlflow
- check the forms of get the models

for the lambda role is necesary create PutRecord and PutRecords
the steps are:
- creating the role, this use the aws policy to lambdas, LambdaKinesisExecutionRole
   → this is a serverless function
   → in lambda handler, the event gets the data
- creating the lambda function, this is created with the same aws console
- creating the kinesis, with test is important the partition-key, with the command get the shard-id

Batch deploy
- turn the notebook for training a model into a notebook for applying the model
   → only get the model
   → the dataframe to predict
   → prepare the features
   → to big dataframe and with output in another location
- turn the notebook into a script
   → jupyter nbconvert --to script {name_notebook}
- clean it and parametrize
   → with main
   → using sys
   → I prefer use click

Batch: scheduling batch
- using prefect to deploy the batch
- is necesary a depploy to execute one this
- is possible send paramaters in deploy of prefect
- is possible call another flow using another flow

Monitoring of ML models
- check the degrade and need monitoring of model
- how to prepare:
   → service health
      ⇒ only uptime, mememory and latency
      ⇒ in production services
   → model performance
      ⇒ check the prediction of model
      ⇒ depending the type of problem
      ⇒ how it performs
      ⇒ did anything break
   → data quality and integrity
      ⇒ where it breaks
      ⇒ where to dig further
   → data and concept drift
      ⇒ is the model still relevant
   → performance by segment
   → model bias/fairness
   → outliers
   → explainability
- batch vs online service model
   → how to:
      ⇒ add ML metrics to service health monitoring
      ⇒ build an ML-focused dashboard
- monitoring:
   → send data to mongo(results)
   → send too monitoring service, metrics to prometheus and grafana
   → form: mlservice + usage simulation + logging + online monitoring
   → another: ML service + usage simulation + logging + online monitoring + batch monitoring, this create a HTML report
   → comunication by containers

Setting environment
- is necesary to this module, docker and docker-compose</rich_text>
      </node>
      <node name="ML Engineer" unique_id="110" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663360401" ts_lastsave="1663361604">
        <rich_text>- Introduction to ML
   → </rich_text>
      </node>
    </node>
    <node name="estadistica inferencial" unique_id="95" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="133" is_bold="0" foreground="" ts_creation="1654816797" ts_lastsave="1654816797"/>
    <node name="etica en el manejo de datos" unique_id="96" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="10" is_bold="1" foreground="" ts_creation="1654889158" ts_lastsave="1655056410">
      <rich_text>Que son los datos personales:
- datos que permiten identificar a una persona de manera directa o indirecta
- se pueden clasificar por jerarquia de importancia o riesgo, hay datos que son unicos y otros que permiten identificar a multiples personas
   → datos ordinarios, nombre, direcciones, finanzas
   → datos sensibles, origen etnico, opiniones politicas y religiosas, orientacion sexual
   → datos especiales, genericos, biometricos, de salud
- cuales no lo son
   → informacion anonimizada
   → datos de personas juridicas, aquellas que son de una institucion, solo los de individuos
   → informacion que no permita la identificacion
- en que diferencias los datos biometricos
   → son un tipo de datos personales
      ⇒ por naturaleza
         • universal
         • unico
         • permanente
      ⇒ segun sus caracteristicas
         • rasgos fisicos y fisiologicos
         • rasgos de comportamiento y personalidad

Escandalos historicos
- 1940, Holanda, por medio del censo se pudo saber datos de personas de origen judio, muerte del 75% de poblacion judia
- 2016 Mexico, problema del INE en el S3 de amazon con datos de personas

Intereses por la informacion
- cada vez se pueden procesar datos mas facilmente
- se puede analizar
- se usa para predecir y saber quien es el usuario
- el incremento de informacion se ha tenido que regular
- tipo y volumen de informacion que puede recolectar la informacion, deben de tener motivos de por que lo hacen
- de manera ilicita se compra y vende datos

Bias y GIGO en datos
- bias-&gt; sesgo, se toma una muestra que no es estadisticamente sifnificativa, extracto de nuestro universo
   → debe ser suficientemente grande para tener una buena muestra
   → un sesgo comun es cuando se trabaja con reconociemiento facial, se tiene mas fotos de personas blancas
- Garbage in, garbage out
   → la calidad del resultado depende de la calidad de entrada
   → la fuente debe de ser veraz

Advertencias de uso en marketing
- se puede crear publicidad personalizada
   → los usuario tiene el poder de decir y calificar los productos
   → sesgos discriminatorios
   → precios por informacion privilegiada
   → minupulacion de campanias politicas
   → formanto de conductas adictivas y ataques emocionales

Advertencias de uso en campanias politicas
- el gobierno se interesa en los datos
- Cambridge analitica ofrecia como manipular el comportamiento de la audiencia
   → datos propios a partir de cneutrnas
   → datos de redes como facebook
   → ciencia del comportamiento y modelos predictivos
- la empresa identificaba quien era vulnerable o quient enia una conciencia politica determinada, se manipula o se distorciona la realidad para no votar en la otra persona

Reglamentos
- penalizacion por mal uso de datos
- regulacion de datos
- ley federal de proteccion de datos personales en posesion de particulares, en mexico y en LatAm, se debe de justificar el por que se hace uso y recoleccion de los datos
   → todos los datos deben de ser de uso y seguros
- sansiona la compra de bases de datos personales
- GDPR, reglamente general de proteccion de datos
   → ley de la union europea para proteccion de datos tanto estando o no estando en el pais
   → permite tener un control de los usuario y de su control
   → se buisca la estandarizacion
   → aplica para cualquier empresa del mundo

Etica y deep learning
- vehiculos autonomos, reconocimiento de imagenes
   → aun sigue en pruebas
   → dilemas eticos hacen que no podemos ver o usar
   → peligros de ataques ciberneticos sobre el codigo
- reconocimiento facial
   → se basa en datos biometricos
   → del rostro completo o parcial
   → se basa en reconocimiento de vectores
   → se desarrolla para acceder a espacios fisicos con mayor facilidad
   → como usuario se puede decidir si otorgarlo o no
   → se debe de pensar tambien en el usuario y si es etico como proteger la informacion
   → en Mexico ya existe uso de este tipo de accesos para el papeleo
- etica en la pandemia
   → la pandemia ha sido un gran reto
   → uso de controles de acceso
- relaciones interpersonales
   → redes sociales, trabajo, uso de apps
   → las interacciones en el trabajo han evolucionado
   → ahora se conoce gente de manera no fisica
   → como devs de tecnologia debemos de garantizar un espacio seguro
   → funciones de audio, nosotros otorgamos acceso al microfono, se puede bloquear el acceso al microfono para que no nos muestre anuncios
   → redes sociales despues de la vida, cuando uno muere deja toda su huella electronica, una persona debe de elegir que hacer con las redes sociales
- etica en el NLP
   → investigacion de la interaccion entre PC y lenguaje humano, por medio de lenguas naturales
   → casi siempre es en ingles
   → un modelo desarrollado es micrsoft-alibaba vs stanford
      ⇒ el algoritmo gano la compresion lectora
   → gpt-3
      ⇒ lenguaje autoregresivo, aprendizaje profundo 
      ⇒ tiene como reto identificar que es real y que es falso

Data for good
- Politicas publicas
   → atencion del estado a una necesidad social
   → se pueden implmentar a partir de normas, instituciones, prestaciones y servicios
   → ayudan para alcanzar principios eticos e igualdad en sociedades
   → tipos de clasificacion
      ⇒ autoridad institucional, municipal, mundial, nacional
      ⇒ sector, medico, medio ambiente, edcicacion
      ⇒ destinatarios, a quien van dirigidas
      ⇒ elaboracion, participativa o autoritaria
      ⇒ planificacion, anticipativa o reactiva
- Datos y prevencion de crimenes
   → a partir de dos tipos de recoleccion
      ⇒ en camaras, de tipo reactivo
      ⇒ por medio de reportes
      ⇒ preventiva a partir de donde pude suceder algun crimen
- datos y salud
   → por medio de prediccion se puede tener una oportunidad
   → el uso de apple watch para ver como se esta comportando nuestro sistema
- movilidad
   → no solo tiene que ver con vehiculos motorizados
   → se facilita la movilidad y recolectan informacion gracias a un radar
   → lo datos que se recolectan se pueden analizar, para saber como es el flujo de la ciudad, esto ayuda a crear politicas publicas
   → ciudades inteligentes a partir de los datos, emplean tecnologia para anticiparse
   → la otra cara de la moneda
      ⇒ dilema etico, muchas zonas pueden ser discriminadas, nos diriguen siempre por vias principales ya que son mas seguras, eso hace un flujo mayor
- educacion
   → las escuelas online permiten trackear los resultados
   → nos ayuda a recomendarnos algunos cursos
   → algoritmos para mejorar la experiencia
- medio ambiente:
   → reactivo, que se puede hacer despues de que algo ya sucedio
   → por medio de predicciones podemos saber si se puede realizar alguna actividad de acuerdo a como esta el ambiente
   → se puede evaluar para saber si algo esta en optimas condiciones


¿Qué podría generar un anuncio sin planificación ética?
La falta de estandarizaci√≥n de un protocolo ante la pandemia es un ejemplo de dilema √©tico y de debate. Esto es:
</rich_text>
    </node>
    <node name="curso EDA" unique_id="99" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655834724" ts_lastsave="1656024455">
      <rich_text>Desarrollo de tecnicas para entender y explorar que dice un dataset, como plantear un modelo de machine learning, hacer un analisis de datos exploratorio completo

Que es el analisis exploratorio de datos (EDA)
- KDD knowledge discovery in databases
- SEMMA, sample, explore, modify, model and assess
- CRISP-DM, Cross industry standar process for data mining
- esto nace por la mineria de datos, se busca conocimiento de todos los datos que tenemos
- metodologia para poner un orden
   → empiezan los requerimientos, necesidad a contestar
   → que informacion se tiene para llegar al objetivo
   → como se esta procesando, que tipos de datos, la data es correcta o no
   → procesada y recolectada podemos entenderla
   → tengo suficiente informacion o no
- etapas:
   → definicion del problema
   → preparacion de los datos
   → analisis de datos
   → desarrollo y presentacion de los resultados
   → * si se puede hacer con toda la base de datos es recomendable

EDA vs Clasico vs Bayesiano
- clasico, resultados directo a la comunicacion, lo saca y lo muestra, se hace el analisis y no se sabe que mas se puede o se va a hacer con el, no se da contexto del problema a resolver
- bayesiano, Prior probability, que paso anteriormente para predecir el futuro, que puede pasar con este modelo y que se puede enterder
- EDA, este se busca que sea totalmente dinamico, se lanza un modelo de machine learning o IA o automatizacion o un insight al negocio, este es el primer paso a la ciencia de datos

Tools
- python jupyter
- AWS sagemaker, se puede llevar a prod por medio de este, se paga por lo que se usa
- Spark usando EMR, esto es para grandes volumenes, si es para tiempo real se debe de pensar que se usara y que necesidades tengo
- google tiene sus jupyter y jupyter-lab
- azure notebooks, este solo se hace experimentacion, los modelos se deben de pasar directamente a plataforma
- R - Rstudio, la mejor opcion si se necesitan librerias que esten con documentacion cientifica
- KNIME, si sabes a nivel negocio que se requiere, pero no hay tecnicos

Visualizaciones de EDA
- se usa el dataset zoo.data
- uso de altair para visualizar datos, se tiene que saber cual es la X y y la Y a usar, la ventaja es que se pueden exportar a PNG o a SVG, esta es visual
   → mark_line para graficas con lineas
   → mark_bar para tener una grafica de barras
- uso de plotly, es de paga
   → esta hace algunos calculos por si misma
   → se usan pie para ver de manera general que estamos haciendo
   → se le puede poner el title en la misma grafica

Transformacion de datos
- uso de chicago data, se usa map_plot
- generalmente se hacen el cambio de fechas (en python con csv) o los cambios de tipos de datos float -&gt; int

Estadistica descriptiva
- se busca contar la historia de a que hora fueron los accidentes y donde
- se trata de describir las variables para tener una solucion al problema dado
- crosstab de pandas para saber que el dataset esta parejo para las variables que se usaran

Distribucion de los datos
- se debe de entender la naturaleza de los datos que se estan graficando o viendo
- var continua, puede tomar cualquier valor dentro de un intervalo
- var discreta, no puede tomar ningun valor entre dos consecutivos
- que tipos de distribuciones pueden existir
- en que nos puede ayudar en saber la distribucion, se puede coneocer por medio de la graficacion
- obtencion de ajuste de curva

Medidas de tendencia central
- como se comporta una poblacion
- no se puede predecir el comportamiento individual, pero sie el comportamiento promedio
- ley de los grandes numero, bajo ciertas condiciones generales la media de n variables aleatorias se aproxima a la media de las n medias, se puede hacer una aproximacion de la distribucion
- teorema del limite central
   → cuando el tamanio de la muestra es lo suficientemente grande, la distribucion de las medias sigue a una distribucion normal
- otras medidas
   → media, moda, mediana, min, max, producto de valores, suma acumulada

Medidas de dispersion
- desviacion estandar, repasa la cantidad de dispersion de los datos de una poblacion entera
   → que tan separados estan los datos
- Varianza, encontrar que tan diferentes estan los datos
- asimetria estadistica (skewness), ayuda a saber que forma hay de los datos sin necesidad de graficarlos, esta se puede saber de acuerdo a la moda, mediana y media
   → moda &lt; mediana &lt; media, positivo skew
   → moda = mediana = media, simetrico skew
   → moda &gt; mediana &gt; media, negativa skew
- kurtosis
   → es una caracteristica para saber que tan acumulados estan los datos
   → nos ayuda a saber que tan probable es que se tenga una variable

tablas pivote y cross tabulations
- puede sacar la varibilidad sobre todo el dataset, de acerudo a una agrupacion de campos
- cross tab, se da un rango donde estan las condiciones dadas

Correlacion
- es la relacion que guarda entre una variable y otra
- es un porcentaje donde se dice que corresponde un dato con otro dato

Analisis de series de tiempo
- como se comporta una variable a lo largo del tiempo
- uso de quandl
   → dataset FRED/GDP
   → dataset CHRIS/CME_GC1
- dataframe open power system data

Desarrollo y evaluacion de modelo
- se deben de tener pruebas de hipotesis (este depdnde de la ditribucion y tipos de datos)
   → Anova, comprabacion muy especifica entre ciertos tipos de datos
   → Z-test
   → T-test
   → chi squared test, si no se sabe la distribucion
- p value, es el valor que esta entre el intervalo del 5%, si no cae aqui quiere decir que se tiene una probabilidad de exito, si es menor a esto se acepta o rechaza de acuerdo a que sea cierta, esto solo se hace dentro del analisis de datos de la muestra
- siempre se deben de dividir los datos en dos, entrenamiento y testing
- se entrena el modelo
- se evalua
   → presicion= VP / (VP + FP)
   → exactitud= (VP + VN) / (VP + FP + FN + VN)
      ⇒ sig: vp = verdadero positivo, fp = falso positivo, vn = verdadero negativo, fn = falso negativo
- el modelo puede ser preciso pero no exacto

Regresion y evaluacion de hipotesis
- tipos de regresores
   → NNR
   → Decision Tree
   → LASSO
   → Ridge
   → ElasticNet
- uso de classification_report, esete da precision, recall (sensibilidad), f1-score, supponrt, se le envia la original contra la predicha
- uso de confusion_matrix, y original contra y predicha, dice cuando falsos positivos y negativos se tienen

Analisis exploratorio completo
- uso de describe para saber que set tiene
- se analiza la density
- se debe de tener un objetivo a analizar, en el caso de los vinos la calidad
- la correlacion nos ayuda para saber que variables usar
- mucho tiene que ver el experto para saber tambien que variables se deben de analizar, no es siempre a como tenemos la idea
- para hacer un ajuste de curva se debe de saber la distribucion


¿Cuáles son algunas propiedades de una serie de tiempo?
¿Qué es un análisis multivariable?
¿Qué comandos usamos para agrupar datos?
</rich_text>
    </node>
    <node name="Analisis exploratorio nuevo" unique_id="111" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663875000" ts_lastsave="1664407703">
      <rich_text>
Introduccion
- proceso de conocer en detalle y darle sentido a los datos
- saber que tipos de datos son
- aqui se sabe como tratar a los datos
- se convierten en informacion util
- que razones se tiene para hcer este analisis
   → organizar y entender las variables
   → establecer relaciones entre variables
   → encontrar patrones ocultos en los datos
   → ayuda a escoger el modelo correcto para la necesidad correcta
   → ayudarte a tomar una decision informada
- pasos (es un proceso ciclico):
   → hacer preguntas
   → determinar el tamanio de los datos
   → categorizar las variables
   → Limpieza y validacion de datos
   → establecer relaciones entre los datos
- Tipos de analitica de datos
   → descriptiva: que sucedio, a pasado
   → diagnostica: por que sucedio, a pasado
   → predictiva: que podria pasar, a futuro
   → prescriptiva: que deberia de hacerse, a futuro
- Tipos de datos y analisis de variables
   → categoricos, genero, categoria de pelis
      ⇒ ordinal, un orden natural
      ⇒ nominal, no importa el orden
   → numericos
      ⇒ discreto, numero de amigos, enteros
      ⇒ continuo, altura, pero, longitud
   → Tipos de analisis:
      ⇒ univariado, obtener analisis de las variables solas
      ⇒ bivariado, se buscan las relaciones entre variables
      ⇒ multivariado, checar todas las variables que se tienen para saber como se relacionan
- Que es la recoleccion de datos
   → nos permite obtener informacion del tema de investigacion
   → hay tipos diferentes
      ⇒ primaria, se obtienen de primera mano, experimentos, entrevistas, se tiene el total control
      ⇒ secundaria, se obtiene por medio de una fuente primaria externa, aqui no se tiene control de como se hizo el conjunto de datos
      ⇒ terciaria, fuentes completamente extenas, se obtienen a travez de terceros como datos de otras empresas
   → que es la validacion de datos, proceso de revisar que tengan consistencia y precision de los datos
      ⇒ se validan
         • modelo de datos
         • seguimiento de formato estandar de archivos
         • tipos de datos
         • rango de variables
         • unicidad
         • consistencia de expresiones
         • valores nulos

Analisis univariable
- explorando una variable categorica
   → conteos y proporciones
      ⇒ se basan en la tabulacion
      ⇒ contabiliza la frecuancia de aparicion de cada valor unico de una variable
      ⇒ generalmente se ordenan por alfabeto o numero de aparicion
      ⇒ despues se puede hacer un diagrama de frecuencias
      ⇒ las proporciones es la relacion de correspondencia entre las partes del todo
- medidas de tendencia central
   → entienden comportamiento general de los datos
   → media, promedio
   → mediana, dato central
   → moda, dato que mas se repite
   → media ponderada, media armonica, media geometrica
- medidas de dispersion
   → que tan lejos o cerca se encuentran los datos
   → rango, se toma el valor minimo y maximo de los datos
   → rango intercuartilico, comprende el 25% arriba y abajo de la mediana
   → desviacion estandar, ofrece la dispersion media de una variable
      ⇒ la regla del 65, 95, 99, a una desviacion estandar ya tendremos el 65 de los datos a dos se tiene el 95 y a tres casi todos
   → boxplot para visualizar los estadisticos del conjunto de datos
   → existen la asimetria estadistica, sesgo negativo,simetrica, sesgo positivo
   → otra medida es la curtusis, que tan juntos o dispersos estan los datos respecto a la media o promedio, si es cero estan distribuidos homogeniamente a la media, cuando es positivo, estan concentrados al rededor de la media, cuando es menor a cero, los datos se distribuyen a lo largo de la media
- distribuciones
   → histogramas
   → funcion de probabilidad de masas, nos dice la probabilidad de que una variable aleatoria discreta tome un valor determinado
   → funcion de distribucion acumulada, devuelve la probabilidad de que una variable sea igual o menos que un valor determinado
   → funcion de probabilidad de densidad, determina la probabilidad de que una probabilidad continua tome un valor determinado

Analisis Bivariado
- relaciones
   → grafica de puntos
      ⇒ como se distribuyen los puntos entre dos variables
   → no siempre se tienen graficos que luzcan bien, mas cuando se tienen muchos datos
   → se puede modificar la transparencia de la grafica para que se vean mejor
   → histogramas de dos dimensiones, se pueden tener por colores la frecuencia, nos ayuda a ver mejor como se agrupan los datos, se pueden cambiar los colores para visualizar mejor
   → graficos de violin y boxplot
   → se pueden tener huecos o en lineas rectas, generalmente son en variables discretas, se puede agregar un ruido aleatorio
   → se puede visualizar un diagrama de caja
   → matrices de correlacion
   → hasta que punto las variables estan relacionadas entre si, cambian conjuntamente
   → se usa el coeficiente de correlacion, usualmente la de person, este sirve de forma lineal
   → causalidad, cuando algo genera otra cosa
   → correlacion no implica causalidad, no siempre dos eventos estan relacionados aunque lo paresca
- generalmente en las regresiones lineales no son simetricas, a-b no es lo mismo que b-a

Paradoja de simpson
- es un fenomeno en el cual se pueden concluir dos cosas totalmente opuestas a partir de los mismos datos, dependiendo el como se clasifican estos</rich_text>
    </node>
    <node name="machine learning curso udemy" unique_id="113" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664229102" ts_lastsave="1664229117">
      <node name="que es el machine learning" unique_id="114" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664229117" ts_lastsave="1664229970">
        <rich_text>es un subdominio de la IA, proporciona a los sistemas la capacidad de aprender y mejorar automaticamente, sin ser programados para ellos, es generalmente crear un modelo que se va mejorando con mas datos hacia el modelo a lo largo del tiempo (experiencia en datos)
se obtiene experiencia pasada, se determina un algoritmo de ML para el problema, se tomand los datos y se proporcionan al algoritmo que se decidio, este entrena y se construye el modelo
si el modelo es correcto, se pone en produccion
con el modelo, la base esta en los datos</rich_text>
      </node>
    </node>
  </node>
  <node name="devops" unique_id="12" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#1e90ff" ts_creation="1619462339" ts_lastsave="1650935953">
    <node name="Curso de cloud computing" unique_id="13" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619531922" ts_lastsave="1649170170">
      <rich_text># Que es EC2
- son practicamente maquinas virtuales
- las AMI son imagenes preconfiguradas
- Se pueden escoger diferentes configuraciones, llamadas instancias

# Lightsail
- es una opción a EC2
- es un vps (virtual private server), ip publica y dominio gratis
- este inicia en segundos
- ya contiene templates preconfigurados
- Tiene precio fijo y predecible, mas barato que EC2
- bases de datos
- respaldos
- restauracion
- multiregion

# Marketplace lightsail
- es muy parecido a digital ocean
- tiene varias plantillas pero también sistemas aparte
- sigue siendo responsabilidad del admin tener todo actualizado

# ECR, ECS, EKR
- ECS, permite correr contenedores de docker, solo se paga lo que se necesita, se puede escalar de acuerdo a las necesidades, microservicios o migraciones al cloud
- EKS, implementacion de kubernetes, permite crear el ambiente de workers, corre contenedores con herramientas tradicionales, se levanta el servicio y dentro del server creado se maneja la creación de servicios

# Lambda
- funciones de codigo que implementa microservicios
- dedicado a arquitectura microservicios
- tiene su propio endpoint
- no se administra nada por parte del usuario
- es autoescalable
- tiene un millón de llamadas gratis, no expira
- soporta: JS, python, java, C#, Go
- deben de llevar el menor numero de permisos
- NOTA: lo mas importante es aprender a hacer roles que tengan permisos limitados y solo los necesarios

# Elastic beanstalk
- es un endpoint, arquitectura de deploy, incluye todo lo necesario
- tiene un load balancer
- l1 o mas EC2 instances
- software de administración
- tiene soporte para varios lenguajes
- Soporte para docker, go, java se, tomcat, .net, nodejs, php, python, ruby


-----------------------------------------------------------------
# Version 2022
</rich_text>
    </node>
    <node name="Curso de storage" unique_id="14" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617750952" ts_lastsave="1619722086">
      <rich_text>Storage en S3

Caracteristicas de S3
Almacenamiento de objetos
existen varios tipos:
- S3
- S3 IA
- S3 One Zone
- S3 Glacier
Alta durabilidad y disponibilidad

se divide en:
Bucket - donde se almacena
Objeto - el archivo guardado
Web estatica - sirve para servir paginas estativas
el bucket debe de estar lo mas cercano de nuestra infraestructura

Versionamiento
es el manejo que se puede tener estilo github, donde podemos regresar a varios archivos anteriores
los buckets deben de tener un nombre único, esto depende mas que de los que tenemos, son los que existen hasta de otros usuarios
en propiedades se activa el versionamiento de archivos
para ver las versiones hay que darle show en version

Sitios web estaticos
Se puede usar el dominio normal, Route53 para la gestión de dominios
se requiere:
- nombre, se debe llamar el bucket igual que el dominio
- archivo index y error
- se debe configurar el DNS

para activarlo se debe de ir a propiedades y activar el static website
se le debe de hacer publico a los objetos que usara el sitio web

Logs a nivel de objeto
Son usados para eventos, accesos y que acciones se hacen sobre los objetos se puede usar cloudwatch
se deben conectar a un servicio de cloudtrail, se puede crear otro bucket para el guaradado de los eventos

Transferencia acelerada
Es tomar la ventaja de las locaciones o CDN de amazon para cargar la informacion de forma mas rapida
en propiedades se muestra transferencia acelarada y se habilita, se identifica el mejor CDN con el mejor tiempo de respuesta, se da un endpoint para hacer esas cargas
mejores tiempos de carga de datos

Eventos S3
configuracion para manejar otros procesos
se puede usar SNS, sirve para el envio de notificaciones
se puede enviar a un SQS que puede procesar una lambda
se puede enviar directamente a la función lambda

Replicación
Sirve para recuperación de datos, para información crítica
esta es asincrona
en managment es donde se hace o activa la opción, se puede hacer por todo el bucket o por el subfolder
se puede pasar a un tipo distinto de almacenamiento
se utiliza muchas veces si la infraestructura esta en otra region

Clases de storage
tipos:
- s3 estandar, replicacion en 3 zonas
- s3 IA, replicado en 3 zonas
- s3 IA one zone, solo una zona
- S3 glacier, N/D

Ciclo de vida
- cambios entre storage de S3, S3-&gt;S3 IA-&gt;S3 glacier
- la modificacion es en lifecycle rule
- se pueden elegir actuales y previos
- lo minimo para envio es de 30 dias para cada uno de los cambios

Snowball
- Se usa a escala de PB
- en algunos paises aun no exiten
- Si excede a los PB se debe de usar Snowmobile
- cargas multiparte, &gt; 100MB
- 5GB de tipo PUT
- 5TB tamaño maximo en S3

Seguridad en S3
Encriptacion de objetos
tres tipos de server side:
- SSE-S3, amazon administra y gestiona las llaves, estandar AES-256, estan en IAM
- SSE-KMS, se crean en IAM, tienen factores de seguridad adicional, nosotros la creamos y amazon la administra, se elige que usuarios pueden usarla y crearla, estan en cloudtrail para auditarla, la rotacion depende del usuario
- SSE-C, el usuario provee las llaves, se generan en el sistema propio, el mismo usuario debe proveer la llave para revisar la data, las peticiones deben de ser por HTTPS, las llaves no se guardan en S3
- una de cliente

Politicas
- control de seguridad, para permisos y accesos dependiendo que se quiera para el usuario
- son de tipo JSON
- Statement es obligatorio
- Sid identificador de la politica
- Effect, allow o deny, es obligatorio
- Principal, especifica usuario o rol, es obligatorio

ACL en S3 (listas de control de acceso)
- permite que otras cuentas tengan accesos a los bukets
- se le puede dar acceso publico, se recomienda no hacer esto

# Storage gateway
- permite conectar on-premise con la nube, actua como intermediario con el data center fisico con la nube, es un almacenamiento hibrido
- Archivos, volumenes y Tapes
- almacenamiento hibrido
- se usa para backups, archiving, disaster recovery y cloud data processing
- usa protocolos NFS, SMB y iSCI
- se integra con S3, EBS y glacier
- se usa por medio de una maquina virtual
- se combina con toda la seguridad de AWS

# tipos, 3 tipos
- File gateway, accesos por SMB o NFS, es a nivel de objetos, tiene cache si se requiere que algo sea accedido de inmediato
- Virtual tape library, reemplaza el backup de cintas en el cloud, es data historica
- Volume gateway, crea cache de servicios locales, crea snapshots locales en AWS, son asincronos

# Elastic file system
- sistema de archivos
- sirve para conectar varios sistemas al mismo almacenamiento
- precio es por GB consumido
- Aumento y reduccion automatica
- Solo permite acceso a instancias Linux
- Permite IOPS
- Tiene mejor rendimiento de red
- Hay cifrado en reposo usando KMS

# Elastick block storage
- se pueden instalar sistemas y aplicaciones
- tiene replicacion
- manejo de diferentes cargas de trabajo
- solo se puede asociar a un solo EC2
- si es boot no se puede cifrar
- los adicionales si se pueden cifrar
- se monta a nivel de SO
- existen diferentes tipos
- se puede proteger el borrado
- el maximo es de 16TB

# Tipos
- GP2, discos de estado solido, proposito general, cargas de hasta 10000 IOPS, instancia entre 1GB y 16TB
- IO1, mas de 10000 IOPS, puede ser root, se usa para aplicacionse de altos volumenes de esscritura y lectura, bases de datos no relaciones
- ST1, big data, dataware, log process o streaming, no puede ser root, espacio entre 500GB y 16TB


errores: 
migracion de 5TB de info
que pasa con las versiones anteriores del versionamiento
siendo el CIO de una empresa seleccionar el tipo de almacenamiento
</rich_text>
    </node>
    <node name="IBM" unique_id="15" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619734546" ts_lastsave="1619823923">
      <rich_text># IBM cloud
- se puede conectar nuestro git con un toolchain que sirve para desplegar, muy parecido a CodeBuild, parece que tiene un web IDE donde se puede trabajar
- IBM cloud lite (dev), se pueden usar solo algunos servicios, gratis, no expira, 256MB de memoria para cloud foundry
- Pago por uso (test), 
- Subscripción (live), 
- son open source, son miembros fundadores de Apache
- Cloud foundry, sirve para desplegar servicios, virtualiza, ya tiene ambientes preinstalados
- microservicios, es construir un sistemas con diversos servicios escalables que son poliglotas (diversos lenguajes de programacion)
- se puede usar cloud foundry de dos formas
   → creación de aplicacion por la plataforma de los diversos lenguajes de programación
   → a traves del push, despliegue de push sobre linea de comando
- toolchain, permite prototipar sin costo
- el dominio ya puede estar siendo utilizado por otra persona{
- estos se pueden conectar a otros servicios de IBM
- el devops es el toolchain en IBM
- el pipeline usan jenkins, github para pushes
- se deben de ir habilitando los servicios
- usan eclipse Orion

# Economia de las API
- API Connect, solución para construir APIs en IBM cloud
- la economia de api es el valor que ofrece la rentabilidad de una empresa sobre sus datos expuestos
- Se garantiza un nivel de servicio
- Capacidades para provedores
- capacidades para los consumidores
- como interactuan entre ellos
- permite crear, depurar y deplegar api
- gestiona la seguridad
- componentes:
   → portal del desarrollador
   → api manager, guarda en parte la analitca
   → api gateway, no se interactua directamente, aqui se hacen las transacciones, como politicas de seguridad
   → entorno de desarrollo
- Catálogos, un listado de apis con documentación, se pueden hacer pruebas en linea
- Productos, este encapsula una o mas apis, sirve para registrar la aplicación
- Planes, hay diferenes tipos de planes, donde puede poner logica de consumo por precio
- Estandar que se usan;
   → Swagger 2.0
   → mensajes en JSON/XML
   → protocolos SOAP/REST
# Como hacer deploy de apps
- exite:
   → Build-from-the.scratch
      ⇒ runtimes, varios lenguajes donde se puede construir
      ⇒ boilerplates, se va mas adelante, se tiene contenedores donde estan preconfigurados con algunos otros servicios
   → Bring-your-own-app
      ⇒ CLI, teniendo el codigo propio
      ⇒ Botón de despliegue, aqui se puede usar con github, uso del toolchain

#IBM watson
- computación cognitiva, redes neuronales artificiales
   → proliferacion de datos
   → Economia de las API
   → La capacidad de procesamiento sobre la nube, aqui nace watson

ERRORES:
Después de desplegar una aplicación “starter” desde el catálogo, se puede acceder al código de muestra de la aplicación a través de la página “Getting Started”.
¿Cuál es el resultado de vincular una instancia de servicio a una aplicación Cloud Foundry en IBM Cloud?</rich_text>
    </node>
    <node name="rds" unique_id="16" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623801016" ts_lastsave="1625259642">
      <rich_text># Caracteristicas
- se tiene compatibilidad con:
   → aurora
   → mysql
   → mariadb
   → postgresql
   → oracle
   → mssql
- Caracteristicas:
   → backups automaticos de 1 a 35 dias de retencion
   → backups manuales, se pueden crear en cualquier momento y al eliminar la base
   → sistema de almacenamiento
      ⇒ proposito general (SSD)
      ⇒ Provisionado (SSD), uso intensivo de E/S
   → Cifrado de datos en reposo, se puede hacer a travez de un KMS
   → Actualizaciones automaticas de la BD
   → "DBLess"
- se pueden usar tokens con IAM, 10 a 20 conexiones por segundo
- monitoreo en tiempo real
- se pueden seleccionar varios tipos y tamaños de instancias
- oracle necesita licencia
- postgresql contiene diferentes plugins
- aurora esta optimizada para queries

# Backups y performance
- se tienen backs automaticos y manuales
- los manuales son responsabilidad del usuario, estos son incrementales, se manetienen si se borran la base de datos, se pueden mover de region
- los automaticos se crean a diario, las operaciones de E/S pueden quedar suspendidos, ser recomienda hacer uso de Deploy A/Z
- el precio depende del storage de la base de datos
- monitore de I/O, CPU, DD y memoria, num de conexiones
- replicas de lectura, mejorar el desempeño de la DB, no para Oracle y SQL Server
- IOPS provisionados
- elasticache, dividir la bd en mas pequeñas
- monitoreo d eperformance
- aws recomeinda Aurora
- despliegues multi AZ
   → despliegue en diferentes zonas
   → sirve para alta disponibilidad de la base
   → recomendadas para produccion
   → se compone de una master-standby
   → replicacion sincrona
   → failover automatico
   → conmutacion por error
   → replicacion entre AZ
   → El pricing es como tener 2
   → el Back se hace de DB standby

# Migraciones
- database migration service:
   → adaptar los recursos a la carga de trabajo, solo se paga lo que se usa en la migracion
   → no hay downtime
   → administra la arquitectura para la migracion
   → conmutación por error
   → los datos en reposo se cifran con KMS
   → si se requieren parches el mismo servicio los identifica
- puede estar en otro provedor
- se tiene un instancia intermedia que hace la replicacion y un target
- Homogeneas:
   → un origen con el mismo destino (el motor solo cambia la version pero no el tipo), aplica de mysql o postgresql a aurora
- Heterogenea:
   → Se cambia de motor completamente
   → se requiere hacer una conversion de schemas
   → se debe verificar la compatibilidad

# Aurora
- es el motor de datos mas robusto
- es una base de datos relacional
- 5 veces mejor que mysql y 3 que postgresql
- 64TB de storage
- 15 read replicas
- &lt; 10 ms replica lag
- monitoreo y failover
- usa dos endpoints:
   → el master
   → uno que es el de replica, cada replica tendra su propio endpoint, pero se tiene un tercero que especifica todo
- tiene autoreparacion, fallos de disco, guardando la data
- cache-warm, pre calienta la cache al iniciar, consultas mas comunes
- recuperación de desastres, si falla la principal toma una de las replicas y la convierte en principal
- Serverless:
   → se configura una capacidad minima y maxima
   → compatible con mysql 5.6
   → se puede poner que este inactiva cuando no se use

# DynamoDB
- caracteristicas:
   → servicio de bases de datos NoSql
   → completamente administrado
   → compuesto de varios nodos
   → disrtibuida en varias regiones
   → baja latencia
   → almacenamiento en cache
   → completamente escalable
   → unidades de lectura, bloques de 4kb por segundo
   → unidades de esritura, bloques de 1kb por segundo
   → se replica en diferentes locaciones
   → se deben de especificar las capacidades de lectura y escritura
   → la unidad fundamental son las tablas
   → se compone de item, coleccion de atributos
   → particiones son el espacio de almacenamiento que se dan por una llave, nos permite identificar un elemento en especifico
   → sort key, es una llave secundaria para el ordenamiento
   → local secondary index
- consistencia:
   → eventual:
      ⇒ puede no responder algo que recientemente se creo
      ⇒ consume 4KB de bloques por segundo
   → fuerte de lectura:
      ⇒ se toma la mas reciente, se consume el doble de la eventual
      ⇒ este son 8KB
- Casos de uso:
   → mobile
   → IoT
   → Web
   → Gaming
   → Manejo de sesiones
   → Real time

# Dynamo (rendimiento)
- particiones e indices
   → los datos se almacenan en particiones
   → dynamodb se encarga de asignar las particiones de acuerdo al desempeño aprovicionado
   → aumenta el tamaño o el numero de parciones
- limites:
   → las particiones se aumenta al llegar a las 10Gb o 3 mil de lectura o mil de escritura
   → una llave principal, se toma y se crea una funcion hash, determina en que particion se guarda
   → la preferencia es que la llave sea mas random
- clave compuesta:
   → se usan dos llaves la principal y la de ordenamiento
   → la primera define la particion y la segunda como estara acomodada
- operaciones
   → scan:
      ⇒ es la menos eficiente
      ⇒ examinda la tabla o inidce secundario en su totalidad
      ⇒ no usar con tablas grande
      ⇒ examina cada objeto
      ⇒ consume muchas unidades de lectura
      ⇒ realiza lecturas consistentes puede devolver hasta 1MB de datos (una pagina)
      ⇒ aumento de costos
      ⇒ hacer distribuciones de requests
      ⇒ es malo hacer operaciones grandes
      ⇒ se pueden mejorar:
         • se puede poner un limite para los resultados
         • se pueden duplica tablas para no afectar las principales
   → queries
      ⇒ busca elementos basados en la llave principal
      ⇒ puede consultar tablas o indices secundarios
      ⇒ se pueden usar condiciones
      ⇒ se determina por clave que valores se leeran
      ⇒ se puede expeficar el nombre de la clave y el valor
      ⇒ limites:
         • limita el numero de resultados
         • no devuelve la cantidad de lectura del query

# Stream y replicacion
- proporciona una secuencia ordenada, se mantiiene el registro de todo lo que se modifique en un campo, esto para que sea en tiempo real
- se amplia el poder de dynamo con replicas entre regioes, analisis con redshift, y muhcos otros escenarios
- captura en orden cronologico, de las modificaciones lo almancena por 24 hr.
- las aplicaciones pueden obtener accesos de como estaban
- contiene informacion sobre la modificacion
- DAX:
   → acelerator, es un cache complemtamente administrado y de alta disponibilidad
   → tiene rendimiento de hasta 10 veces
   → soporta millones de peticiones
   → permite cifrado
   → hasta 10 nodos
   → instancias small y medium y de tipo R (opt. de memoria)
   → se puede seleccionar la region</rich_text>
    </node>
    <node name="networking" unique_id="17" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625236500" ts_lastsave="1625420838">
      <rich_text># Introduccion
- Una familia de servicios
- optimización de la red
- componentes:
   → VPC, red privada virtual
   → Cloud front, acelerados de entrega de contenido web
   → Route 53, servidor de nombres de aws
   → API Gateway, direccion publica que conecta servicios

# Servidores y CDN
- Domain Name System
   → la computadora no sabe de nombres
   → pero entiende las IPV4 e IPV6
   → El nombre se traduce a una dirección tipo: 1.2.3.4
   → route 53 es el DNS / NS de aws
- CDN
   → red de distribucion de contenido, copias de sitios para que cargue mas rapido
   → pueden ser HTML, JS, CSS, imagenes y otros
   → cloudfront de aws hace esto trabajo
- Endpoint
   → un punto de contacto con el internet y la infraestructura interna

# Arquitectura en aws
- usuarios y clientes
   → mobile
   → tableta
   → computadora
   → web services
- API Gateway
   → EC2
   → aws lambda
   → otros servicios que son accesibles por un endpoint
- API gateway cache
- CloudWatch
- VPC:
   → un red privada que proporciona amazon, es virtual
   → se compone de subredes
   → se pueden asignar IPs estaticas “internas” (gratis)
   → si se tiene una IP publica y se reinicia el server cmbia de IP
   → se pueden asignar puertos (en securities group)
   → se pueden asignar multiples ip a la misma instancia
   → se le puede asignar una IPV6
   → se pueden cambiar grupos de seguridad
   → toda instancia de amazon tiene libre acceso a internet
   → controles de seguridad a nivel de red ACLs
   → si se pierde la llave se detiene la instancia se crea una copia, se restaura y se levanta con una nueva llave
   → Jumpbox

CloudFront
- es una implementacion de CDN de AWS
- estan en varios puntos geograficos
- se sincroniza rapidamente con los datacenter
- se usa mediaconvert dependiendo del dispositivo que requiera
- tambien lo manipula a nivel de velocidad de internet
- es economico, se cobra por lo que se usa
- se podria desarrollar con instancias ec2 en varias regiones (complicado)
- se usa en cualquier archivo que pueda ser compartido por un servidor web
- permite niveles de calidad de distribucion
- el contenido va sobre ssh
- se puede enviar codigos de lambda

# Route53
- es un servicio de nombre de dominio
- de route53 a ELB a las instancias
- registro de dominios
- alta disponibilidad en dominios
- se puede definir cual es el servidor principal para tener alta disponibilidad
- se puede configurar para tener baja latencia

# Api Gateway
- endpoint abierto que soporta peticiones http
- se devuelve la informacion que esta en cache
- si no esta en cache:
   → redirecciona a una funcion lambda
   → servidor web en EC2
   → elastic beanstock

</rich_text>
    </node>
    <node name="despliegue de apps" unique_id="18" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625423192" ts_lastsave="1625494503">
      <rich_text># Historia:
- apps de escritorio:
   → linea de comandos
   → base propia
- cliente-servidor:
   → recibe conexiones de muchos clientes
   → accede al servidor y consume y lee información
- monoliticas vs microservicios:
   → monoliticos: se tiene todo en un solo archivo, despues el back y front todo en el mismo servidor
   → microservicos: separemos los componentes de las apps, cada cosa en su “servidor”, filosofia UNIX

# Stacks:
- LAMP:
   → linux
   → apache
   → mysql
   → php
- MERN:
   → Mongo
   → Express
   → Reac
   → Node
- JOTL:
   → Java
   → Oracle
   → Tomcat
   → Linux
- JAM:
   → JavaScript
   → API
   → Markup

# Despliegues
- Github:
   → nombre de usuario.github.com
   → tiene que ser publico
- surge.sh
   → install npm surge
   → pide usuario y contraseña
   → surge no da ssl
- netlify.com:
   → se puede conectar con github
   → pide el repo a usar
- vercel.com
   → se conecta igual con github y se agrega el repo, parece ser una buena opcion para frameworks
   → me dio opcion de usar empresa o usuario
   → tiene acceso a fallos y revision de deploys

# Bases de datos
- mongo atlas
- elephant

# Heroku
- se pueden desplegar backends
- es un PAAS</rich_text>
    </node>
    <node name="big data" unique_id="19" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625508695" ts_lastsave="1625785979">
      <rich_text># Intro
- en la nube se puede tener un crecimiento escalable
- crecimiento por demanda, servicios escalables
- se tienen procesos automatizados
- solo se cobra por lo que se use de recursos, costo por demanda
- flexibilidad existen varios para proyectos de bigdata
- almacenamiento, se tiene que seleccionar donde se almacenara
- de la extracción se tienen que tener en cuenta las herramientas y de donde se obtendra la data
- ingesta: tomar la información y alimentar otros sistemas
- validación, proporcionar caracteristicas y garantias, de que la informacion es precisa
- verificación: que los diferentes tipos de datos sea exacta e inconsistente
- test: se ejecutan sobre un subset de la data para garantizar que el proceso este bien

condiciones son opcionales, sirve para crear algunos recursos si otros ya estan
transform opcional, son para serverles
recursos, es obligatorio y es para saber que se va a crear

# Arquitecturas
- Lambda:
   → se atribuye a Nathan Marz, arquitectura escalable, tolerante a fallos y de procesamientos de datos
   → robusta, soporta multiples cargas de trabajo
   → compuesta de 3 capas, batch, serve y speed
   → se divide la informacion en 2 servicios, batch y speed
   → alimentan queries y visualizaciones
   → se aumenta la complejidad en cargas de administración
   → es muy comun
- kappa:
   → presentada por Jay Krepsen en 2014
   → evoluciona de la lambda
   → no tiene capa de batch
   → todo es un streaming
   → informacion origen no modificada
   → solo un flujo de procesamiento
   → capaz de reprocesar
   → llega el origen de datos y se procesa en tiempo real
- batch
   → parte especifica de la arquitectura lambda
   → entra la informacion, la procesa y da una salida
   → procesar data desde el dia prensenta hacia atras
   → se puede procesar data de dias anteriores
   → se pueden alimentar sistemas de visualizacion

# Extracción de la información
- como se llevan los datos que se tiene a cloud
- SDK
   → usando un lenguaje de programación
- CLI
   → utiliza el CLI de aws
- Servicios
   → hay varios servicios que pueden recibir y extraer informacion
- API GAteway
   → es una puerta de enlace entre la data que se produce con la plataforma de nube
   → puede manejar ciento de miles de llamadas recurrentes
   → previene ataques de DNS
- se puede configurar pra que por medio de operaciones se mande a un función lambda
- Storage gateway:
   → permite informacion desde on-premise a la nube
   → se pueden enviar los logs a s3 para procesarlos
   → funciona en una VM instalada
- Kinesis data streams
   → es como pup/sub de GCP
   → recopilar y procesar grandes cantidades de stream.. tiempo real
   → procesamiento de logs, markets data feeds y web clickstream
   → se utiliza para hacer agregaciones
   → se compone de:
      ⇒ data record, unidad fundamental
      ⇒ retention period, periodo de retención, tiempo que durara la data
      ⇒ producer, se encarga de poner el data record en kinesis
      ⇒ consumes, toma el data procesado y lo deja donde se use
      ⇒ shard, es una secuencia de Data records dentro de un stream
      ⇒ Partition key, se usa para agrupar los data records
- Kinesis firehouse
   → servicio completamente administrado para la entrega de datos
   → se pueden usar lambdas para hacer las transformaciones
   → se puede integrar a S3, redshift, ElasticSearc y Splunk
- MSK
   → servicio apache kafka administrado en la nube
   → se despliega en cluster y tiene autoreparacion de nodos
   → usaba la version 1.1.1 de kafka
   → se usa cuando se requiere interactuar con apps de terceros
   → se compone de broker nodes, cuantos y donde
   → zookeeper mantiene datos de nombres y configuraciones, este es un nodo
   → nodos de zookeeper, siempre se tiene uno

# Transformación
- Glue
   → es un servicio totalmente administrado
   → provee un contexto de spark para ejecutar trabajo en Python o Scala
   → Se encarga de la creación del glue catalog, este puede ser consultado por otros servicios como Athena
   → usa DPUs, que son 4vCPU y 16GB de ram, la mínima de desarrollo es de dos
   → glue catalog, es un almacen de metadatos persistentes, cada cuante tiene uno
   → Crawler y classifier escanea e identifica la información de origen y crea su glue catalog
- Zeppelin:
   → permite notebook web que puede hacer analisis, SQL y mas cosas
   → permite ejecutar SQL, python y spark
   → tiene integracion con AWS, se puede integrar con GLUE y con EMR
   → en seguridad se le pueden especificar librerias de python y scala (en el endpoint)
   → pide llave para conectarse por ssh
- EMR
   → Elastic map reduce
   → es un cluster donde se corren cargas de trabajo grandes
   → son servicios administrados basados en Hadoop
   → Corre apps de map reduce, spark y otras opciones que usa hadoop
   → provee integracion con otros servicos de aws, s3, redshif
   → se corre sobre el cluster el script
   → tiene pasos que se ejecutan para procesar la data
   → se compone de un master y core nodes para la información en HDFS y task nodes de procesamiento
   → se puede usar autoscaling
- AWS Lambda
   → servicio para el real time
   → solo puede hacer 20000 llamadas de concurrencia
   → se puede integrar con Kinesis firehouse
   → con sqs suele utilizarse para entornos de alto procesamiento para evitar throttles
   → se puede actualizar usando codepipeline y boto3
   → librerias de python para monitoreo de ejecución de código
   → al superar los reintentos se puede enviar a una cola SQS o a un topic SNS
   → los layer sirven para:
      ⇒ cuando se manejan muchas librerias y muchas lambdas se pueden usar para compartir las utilidades

# Carga de información
- Athena:
   → permite hacer consultas interactivas para S3 de tipo SQL
   → es serverless
   → provee interacción con otros servicios como S3, Redshift, Dynamos y Kinesis
   → se puede integrar con JDBC como sql workbench
   → las queries pueden ser guardadas
   → permisos granulares por base de datos y tablas
- Redshift
   → es un data warehouse o un data lake, centra la infromacion que viene de varias fuentes de información
   → repositorio de datos centralizados, tiene una cantidad enorme de raw-data en formato nativo
   → Data mart, es un subset de datawarehouse que hace una tarea especifica
   → sirve para analizar y tomas mejores decisiones
   → diferentes fuentes de datos
   → diferentes stakeholders
   → es una base de datos columnar
   → mejora el I/O de los discos
   → se usa sobre consultas de analitica
   → servicio desplegado a nivel de PB
   → el servicio se lanza en un cluster de instancias
   → sirve para consultas complejas de sql
   → esta basado en postgresql
   → hace compresión de la data para optimizar I/O
   → utiliza cahce para ciertos tipos de valores
- AWS lake formation
   → facilita la creacion de un data lake
   → tiene integracion con diferentes fuentes usando JDBC
   → identifica origenes y crea tablas (crawlers)
   → se encarga de orquestar los ETL de glue
   → limpia y elimina la data duplicada con FindMatch
   → Optimiza las particiones de S3 para consultas mas eficientes
   → cifrado automatico de la data en S3
   → control de permisos por usuario por bases, tablas y columnas
   → logging a nivel de auditorio registrados en cloudtrail
   → owners; permite crear el lake para controlar los usuarios
   → descubre data relevanta para implementar analisis
   → analytics desde otros servicios como EMR y redshift

# Consumo de información
- ElasticSearch
   → visualización de información
   → es un motor de busqueda basado en Lucene, busca texto complejo y JSON sin esquema
   → se despliega en un cluster de AWS compuesto por varios nodos
   → la solución viene integrada con Kibana y Logstash
   → Se puede integrar con AWS Cognito para manejo de usuarios
   → Se pueden cifrar datos en reposo
   → Puede recibir información de Kinesis firehouse y lambda
   → el indice, es una base de datos que guarda informacion que llega, es un nombre lógico que distribuye en un shard
   → Estructura, ES-&gt;indices-&gt;types-&gt;Documents with properties
   → Shard, un índice se puede dividir en multiples shards y setos se almacenan en diferentes nodos
   → se recomienda usar instancias tipo I
   → El dimensionamiento del Cluster es esencial y fundamental (Cantidad de Shards, almacenamiento y la cantidad de índices)
   → Completamente integrado con LogStage y Kibana para temas de visualización
   → Siempre en ambientes productivos se debe habilitar el Cifrado de la data (De nodo a nodo y en reposo)
   → Una medida extra de seguridad. Hacer uso de Amazo Cognito para que los usuarios que van a trabajar en el cluster les aparezca el usuario y password.
- Kibana
   → se integra con ElasticSearch
   → provee diferentes opciones de visualización
   → permite el uso de plugins de terceros para visualización y analítica
- QuickSight
   → es el mas dedicado a visualización
   → es para bussiness intelligence
   → cuenta con un cliente para cel
   → puede escalar hasta 10000 usuarios y su cobro es por demanda
   → usa un motor de machine learning llamado SPICE
   → usando el api permite realizar el embebido en apps
   → permite integracion con variedad de servicios de AWS

# Seguridad, orquestacion y
- a tener en cuenta:
   → cifrado en todos los servicios posibles
   → permisos, todos los usuarios deben de tener permisos basados en granularidad
   → servicios, utilizar servicios adeministrados y servicios de seguridad, serverless
   → monitoreo, todos los servicios deben de registrar los logs
   → contingencia replicacion de datos, pruebas de multiregion, almacenar data historica
   → utilizar datsos sobre la data que llega
- AWS Macie:
   → aprendizaje automatico para conectarse a las fuentes de datos, clasifica, descubre y protege datos privados
   → es un servicio completamente administrado
   → se encuentra para proteger datos de amazon s3
   → lectura y escritura de S3 (manejo de alertas)
   → credenciales de acces
   → cambios de configuracion que puedan afectar a un servicio
   → detecta software peligroso
   → accesos a recursos desde IP o sistemas sospechosos
   → identificacion de intentos de un usuario/role para obtener privilegios elevados
   → anonymous, alguien quiere acceder a los datos y trata de hacerse pasar por alguien mas
   → permisos, identifica los permisos de los datos y las politicas
   → perdida de datos riesgos de anomalías de acceso a data importante
   → credentials, credenciales cargadas en S3
   → location, intentos de acceso desde lugares desconocidos
   → hosting, almacenamiento de software riesgoso o malintencionado
</rich_text>
    </node>
    <node name="Infra as code" unique_id="20" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625506996" ts_lastsave="1625605027">
      <rich_text># Intro
- herramientas
   → hay varias, cloudformation sera el fundamental
- servicios en cloud
   → variados
- se puede tener versionamiento
- se puede tener control y trazbilidad de quien hace los cambios
- se puede tener un set de servicios a desplegar
- se puede hacer por medio de un template
   → se tiene estandarizado
   → se puede automatizar
- es reutilizable
- infraestructura inmutable, es muy normal en Infra as code
- Herramientas:
   → Terraform:
      ⇒ multicloud
      ⇒ es open source y enterprise
   → Pulumi:
      ⇒ se puede usar un lenguaje de programacion para el despliegue
   → serverless framework
      ⇒ especializado en arquitecturas serverless
      ⇒ no se puede hacer otro tipo de infra
   → SDK
      ⇒ es la forma nativa que da el cloud
      ⇒ es el boto3 por medio de lenguaje de programacion
   → CDK
      ⇒ cloud development kit
      ⇒ se despliega con lenguajes de programacion
      ⇒ se llaman los mismo servicios
   → AWS SAM
      ⇒ aplicaciones serverless pero es de AWS
- Cloud formation:
   → flujo de despliegue
   → codigo, verificion y despliegue
   → se pueden crear los templates en JSON o YAML
   → servicios:
      ⇒ Stacks
      ⇒ Stacks set
      ⇒ Designer
   → Integracion completa con AWS
   → aws da soporte sobre el codigo
   → los nuevos servicios son accesibles
   → Se pueden crear arquitecturas de forma gráfica
   → Multi cuenta, se pueden hacer varios despliegues
   → es flexible, crear recursos dinamicos
   → es gratis
   → con una plantilla se puede desplegar varios servidores
   → todos los despliegues son cifrados
   → estable, maneja algo sslas
   → espera a los recursos por crearse

# Que es un stack y como funcionan
- Es una colección de recursos que se manejan como una unidad
- CF asegura que todos los recursos sean creados, si en dado caso falla alguno se borran todos
- se hace rollback si falla el recurso, puede que no los elimine si uno asi lo requiere
- al borrar un stack se borra todo
- Drift:
   → detecta desviación de lo que se desplego con lo que esta en la consola
   → no se debe de hacer, se debe hacer todo desde los stacks

# Stack sets:
- existen cuentas administrador y cuentas target
- se deben de hacer desde cuenta maestra
- se hace referencia a un stack dentro de una cuenta
- se pueden desplegar con diferentes parametros
- se puede actualizar desde la cuenta maestra
- solo cambia para el stack que se requiera
- se puede borrar desde la cuenta maestra
- se pueden usar diferentes tipos de roles, por eso se crea con una maestra
- generalmente se usan para despligues de alto rendimiento

# Nested stacks
- stacks en otros stacks
- limites de cloudformation
   → 100 mappins
   → 200 recursos
   → 51,200 bytes, cuerpo del template
   → 460,800 bytes tamaño maxico de template S3
- granularidad:
   → cada recurso queda con un stack independiente
- se mantiene un orden
- se puede tener interacción entre si a través de outputs

# Funciones intrínsecas
- GetAtt:
   → trae el valor de un atributo
   → se compone del nombre del recurso y del nombre del atributo
   → se tienen tres formatos, y dependera de si se esta usando yaml o json
   → se usa cuando se tiene un stack, se tienen dos recursos y un recurso depende de otro recurso
   → se referencia un recurso de otro recurso
- FindInMap:
   → Devuelve el valor correspondiente al map declarado en la sección Mappings
   → composicion: MapName, TopLevelKey y SecondLevelKey
   → se tienen tres formatos e igual depende de que se use
   → se usa cuando un Mapping tiene un valor que se requiere llamar
   → nombre del mapping, referencia al valor y se obtiene el valor interno
- Join
   → une valores diferentes en uno solo
   → usa un listado de valores
   → igual se tienen 3 tipos de formatos
   → un delimitador y una lista de valores
- Split:
   → divide una cadena de valores en valores independientes
   → se tiene 3 sintaxis
   → se usan cuando se tienen unos valores que estan unidos por un delimitador y que se requiere uno
   → se usa teniendo el delimitador y el valor que se quiere dividir
- Select:
   → se toma un valor dividio en split
   → se tiene 3 sintaxis
   → se usa el indice que se quiere (empezando en 0) y toma el argumento dividido
- Sub
   → sustituye valores por un valor especificado
   → se tiene el nombre de la variable y el valor que se quiere poner por ese valor
   → se pueden usar como si fueran variables de entorno ${variable}
- Ref
   → retorna un valor de un parámetro o un recurso
   → string varname: valuename
   → se usa cuando se necesita hacer referencia a un parametro
   → cuando se quiere hacer referencia a una propiedad de un recurso que no soporte GetAtt
   → generalmente se usa con Parameters
- ImportValue
   → devuelve el valor de una salida exportada de otro stack
   → referencia al nombre lógico del recurso exportado
- if
   → retorna un valor si la condicion se cumple y otro si no (es un if ternario)
- OR
   → regrese true si alguno de los valores es verdadero
- AND
   → solo regresa true si todo se cumple
- EQUALS:
   → compara dos valores, si son iguales hace una acción en especifico
~~~ Las condiciones logicas dependen de como se use el template

# Automatizar
- Agilidad, despliegues cortos
- control, integridad en la infraestructura
- seguridad, Pipelines seguros sin exponer datos
- Usabilidad reutilización de componentes
- manejo de errores, trazabilidad en todos los despliegues
- rollback, automático ante errores
- Servicios:
   → codecommit
      ⇒ parecido a github, el repo de aws
   → cloudformation
      ⇒ la infraestructura como codigo
   → Codepipeline:
      ⇒ Orqueta servicios del despliegue
   → Codebuild:
      ⇒ Compilacion y creación de artefactos
   → se puede usar repos github
   → IAM
      ⇒ manejo de roles en el pipeline
   → cloudwatch
      ⇒ moniitoreo de despliegues
   → S3
      ⇒ almacenamiento de artefactos
   → secrets manager:
      ⇒ gestion de secretos
   → KMS
      ⇒ Llaves de seguridad en el pipeline

# Seguridad
- informacion sensible
   → cadenas de conexion
   → token de github
   → todo aquello que se debe de proteger
- Integracion:
   → integrar diferentes tipos de seguridad
- secrets manager:
   → maneja secretos
   → manejo de datos como claves, credenciales de aws, claves y otros secretos
- Parameter stores:
   → maneja llaves y claves seguras
- uso de buckets cifrados
- tokens de repositorios para haer la integracion

# Errores:
¿Cómo se llama a dividir los stacks por recursos y orquestarlos desde un stack maestro?
¿Cuál servicio en AWS dentro de un pipeline podemos utilizar para crear el artefacto que desplegaremos en Cloudformation?
El estado UPDATE_COMPLETE a qué hace referencia? 
¿Cuál propiedad es obligatoria al desplegar una función lambda como AWS::Lambda::Function?
En la creación de un ROLE en Cloudformation, ¿qué opción debe ser habilitada en Cloudformation?
¿Cuál función utilizarías para obtener un true si alguno de los valores es falso en un arreglo?
</rich_text>
    </node>
    <node name="docker" unique_id="21" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626358714" ts_lastsave="1627941892">
      <rich_text># Problemas del desarrollo del software en general:
- Construir:
   → solo es una pequeña parte
   → los problemas complejos necesitan equipo
   → entorno de desarrollo
   → dependencias
   → entorno de ejecución
   → equivalencia con entorno productivo
   → servicios externos
- Distribuir:
   → tiene que transformarse en un artefacto, que se transportan donde se deben ejecutar
   → son por ejemplo exe, apk, jars y demas
   → divergencia de repositorios
   → divergencia de artefactos
   → versionado
- Ejecutar:
   → la maquina donde se escribe es diferente a la maquina que se ejecuta
   → compatibilidad con el entorno productivo
   → dependencias
   → disponibilidad de servicios externos
   → recursos de hardware
- docker permite construir, distribuir y ejecutar cualquier app

# Virtualización:
- forma de solucionar algunos problemas
- versión virtual de algún recurso
- permite atacar en simultaneo los tres problemas del software
- crear maquinas (por software) en otra maquinas (fisicas)
- problemas:
   → peso, en el orden de los GB, repiten archivos, inicio lento
   → costo de administración, necesita mantenimiento igual que cualquier otra maquina
   → multiples formatos: VDI, VMDK, VHD, raw y otros
- docker alterna la virtualizacion. con contenedores
- los contenedores tienen un tamaño por default preestablecido
- ventajas:
   → son flexibles
   → livianos
   → portables
   → bajo acoplamiento
   → escalables
   → seguros
   → se usa el kernel de la maquina donde se esta ejecutando

# Que se instala y que se hace:
- permite contruir, ejecutar y compartir contenedores
- es:
   → server (docker deamon)
   → Rest API
   → Client docker CLI
   → network:
      ⇒ las redes que permite a los contenedores comunicarse
   → container:
      ⇒ es la base de todo docker
      ⇒ aqui corren las apps
   → image
      ⇒ artefactos para empaquetar contenedores
   → data volumes
      ⇒ permisos para acceder con seguridad al sistema de archivos
      ⇒ no compromete la seguridad

# Primeros pasos
- un contenedor es una maquina virtual liviana
- uno o mas procesos que corren nativamente de la maquina pero estan aislados
- esta limitado a que puede ver o acceder
- inspect sirve para poder ver como estan los dockers (por nombre)
- se puede renombrar con docker rename . .
- para borrar se usa docker rm
- docker run -it {maquina}, para acceder
- se puede tener prendido asi: docker run -d ubuntu tail -f /dev/null
- el process id dentro del contenedor siempre tendra como 1 el que se manda, fuera de docker es otro process id
- puertos:
   → 8080(local):80(contenedor)

# Datos en docker
- no se puede acceder a la maquina a menos que se les permita
- no se sabe que esta dentro de otra maquina
- con -v se especifica un bind mount: -v {ruta de la maquina}:{ruta del contenedor}
- volumenes:
   → es lo mas estandar
   → es una evolucion de los bind mounts
   → para crear un volumen: docker run -d --name db --mount src=dbdata,dst=/data/db mongo
   → estos son mount creados con docker volume create
- para copiar archivos: docker cp prueba.txt copy_test:/testing/., no es necesario que el contenedor este corriendo

# Imagenes:
- como docker como soluciona la construccion y distribucion del codigo
- docker gana eficiencia haciendo capas
- una imagen es la plantilla para correr nuevos contenedores
- conjunto de capas, una va detras de la otra y estan ordenadas

# Uso
- se recomiendas mas el uso de CMD ["app", “command”] aunque tambien se puede usar CMD app command
- cada que se construye una nueva app se puede usar el layer cache
   → si la tiene ya construida y docker la encuentra ya no la vuelve a crear
   → para aprovechar mejor su uso, lo mejor es copiar primero los archivos de dependencias y luego el codigo
   → uso de volumenes para no hacer el build cada que cambia algo
- en redes la red de tipo host sirve para usar la red real de mi maquina
- none es para que no acceda de ninguna forma a la red
- docker network create --attachable platzinet, el comando en -- es para que se pueda conectar cualquiera
- docker network connect platzinet db, connect contenedores a la red

# Docker-compose
- El docker compose es una herramienta que nos permite describir de forma declarativa la arquitectura de nuestra app.
- compose override para hacer cambios al compose sin estar cambiando el archivo

# Administración
- revisar alternativas para docker desktop y revisar configuraciones
- envia señar systerm si no termina envia syskill
- docker ps -l, para mostrar el ultimo ps del proceso, si es mayor a 128 quiere decir que se forzo... 128 + 9 = 137, por lo tanto salio forzado
- se puede usar docker kill {nombre}
- docker exec looper ps -ef, muestra los procesos sin entrar el docker
- si se usa ["{programa}"] es exec form, sin corchetes es shell form, si se corre como shell se corre como programa hijo de shell
- de preferencia usar exec form
- CMD o ENTRYPOINT: usando el docker como binario
   → CMD: es el comando que se ejecutara o las opciones que se mandan
   → ENTRYPOINT: es el comando por defecto que se corra siempre
- el contexto del build, monta en un filesystem temporal todos los archivos donde se haran los copies
- con dos front lo que se hace, es definir las fases en un build, nos ayuda que de una fase posterior podemos acceder a una fase anterior
- docker en docker:
   → se habla a docker por un socket
   → montar socket a un docker
   → docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock docker:19.03.12
   → suena a manejo de administracion por medio de dockers usando el otro docker



Uso de .dockerignore:
¿Qué es Docker Cloud?
¿Qué es Docker Machine?

</rich_text>
    </node>
    <node name="Azure" unique_id="22" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1628261625" ts_lastsave="1632417701">
      <rich_text># Aprenidiendo sobre la nube
- abstraccion de muchas cosas
- que no es:
   → no son satelites con internet
- lo que si es:
   → son instalaciones de equipos de computo
   → seguridad, electricidad independiente
   → se les conoce como data centers
   → no es publica, solo ingresan las personas con ciertos permisos
   → estan distribuidos a lo largo del mundo
- curiosidades:
   → es la que tiene mas datacenters tiene
   → se busca que sean 100% sustentables
   → busqueda de llevarlos al oceano
   → centros de datos en regiones costeras
- se dedica a brindar servicios:
   → computo
   → servidores
   → redes
   → IA
   → almacenamiento
   → software y mas
- todo lo que se hace en una computadora en el provedor y muchas mas barato, agil y seguro
- se maneja a traves de servicios, no se requiere de tener infraestructura
- se escala segun la necesidad
- gastos de capital (CapEx) inversion en infraestructura fisica, deducible a largo plazo
- gastos operativos (OpEx), invecrsion en servicios o productos facturados al momento
- confiabilidad y alta disponibilidad:
   → se encarga de que no se noten los errores de forma perseptible
- es escalable:
   → vertical, en un solo equipo
   → horizontal, aumento de instancias requeridas
- elastica:
   → las apps siempre tendran los recursos necesarios
- agil:
   → es muy rapida la implementación
- distribucion geografica:
   → se asegura que este disponible para todo el mundo
- recuperacion ante desastres:
   → datos protegidos por desastres de la naturaleza
- Modelos de servicios:
   → local, depende al 100 de nosotros
   → infraestructura como servicio, administración del cliente, solo la parte virtual del equipo
   → plataforma como servicio, administración del provedor, solo te preocupas por el almacenamiento y las apps
   → software como servicio, SaaS, 100% al provedor
- Tipos de nubes:
   → publica
      ⇒ accesible a tod el mundo
      ⇒ son propiedad de un provedor
      ⇒ se distribuye a traves de internet
   → privada
      ⇒ accesible para ciertos miembros de la organizacion
      ⇒ puede ser on-premise u hospedada
   → hibrida
      ⇒ es una combinacion de ambas
      ⇒ generalmente son on-premise con nube publica

# Componentes de Azure
- mas de 100 servicios distintos para la nube
   → procesos
   → seguridad
   → servidores
   → redes
   → devops
   → desarrollo de apps
   → hartas cosas
- ventajas:
   → preparado para el futuro
   → crea a tu ritmo
   → listo para cualquier tipo de nube
   → confiable
- como funciona:
   → a traves de virtualizacion
- se organizan por fabric controller
- tiene un orquestador
- api: azure portal, azure CLI
- market place, desarrollos de terceros
- cuentas:
   → administrador
      ⇒ administran el acceso, las directivas y el cumplimiento de las subcipciones
   → suscripciones
      ⇒ agrupacion de cuentas de usuario y recursos creados por estas cuentas, se pueden tener limites o cuotas
   → grupo de recursos
      ⇒ se pueden agrupar en contenedores logicos
      ⇒ implementacion y administracion, por ej. aplicaciones web
      ⇒ si un recurso existe en un grupo solo se puede usar en ese grupo, pero se pueden comunicar con otros
   → recursos
      ⇒ instancias de los servicios disponibles
         • discos duros
         • FaaS
         • Bases de datos
- Tipos de suscripcion:
   → desarrollador
   → prueba
   → suscripcion
   → estudiante
- limites:
   → facturacion:
      ⇒ cada una es diferente
   → control de accesos:
      ⇒ algunas acciones no se pueden hacer
- se ocupan mas suscripciones cuando:
   → se necesitan separar los espacios de trabajo
   → desarrollo
   → pruebas
   → aislamiento de datos
   → depende tambien de la estructura organizacional
   → separados segun la facturacion
   → se pueden definir por el hardware, poniendo limites a los accesos de red, discos o computadoras
- grupos de administracion
   → se puede tener una jerarquia
   → se tendran accesos a solo ciertos espacios de trabajo
   → se pueden tener 10000 grupos de administracion por directorio
   → y es un arbol de hasta 6 niveles
   → cada grupo solo puede tener un grupo primario
   → pero puede tener muchos grupos secundarios
- recurso es un elemento administrable en azure
   → todos los recursos pertenecen a un grupo de recursos
   → solo puede pertenecer a un solo grupo de recursos
   → se pueden mover entre los grupos
   → no se pueden anidar grupos de recursos
   → facilita la administracion y organizacion
   → tienen un sistema de autorizacion, basado en roles
   → azure resource manager
      ⇒ se puede comunicar de varias formas, ARM, sdk, cliente rest, azure-cli, powershell o portal
      ⇒ plantillas en JSON
      ⇒ administrar grupos de recursos o recursos
      ⇒ capacidad de reutilizacion
      ⇒ Etiquetas
      ⇒ facturacion, se puede tener por etiquetado
- grupo e sla agrupacion de recursos en contenedores logicos
   → cuando se elimina se le eliminan todos los recursos que tenga dentro
- Regiones y zonas de disponiblidad:
   → las regiones son las areas geograficas donde tendremos al menos un datacenter
   → las regiones tiene ciertas herramientas que en otras no
   → hay regiones especiales que son para el uso del gobierno
   → azure tiene el mayor numero de regiones a nivel global
   → las zonas de disponibilidad, es donde hay mas de dos datacenters en una misma region, son los respaldos
   → no todas las regiones tiene zonas de disponibilidad
   → pares de regiones, son regiones que tienen por lo menos 500km de distancia para replicar ciertos recursos

# Servicios de azure
- se tiene dos tipos de bases de datos (aqui se tienen laboratorios de bases de datos, clase 11):
   → SQL:
      ⇒ Azure sql database, es un sql server es compatible con NoSQL, es un PaaS
      ⇒ Managed instance
         • comando para backup
         • common language runtime
         • transacciones entre bases de datos
         • no cuenta con escalado automático
         • nos permite migrar a la nube
      ⇒ se tiene otros servicios como:
         • mysql community, 5.6, 5.7 y 8.0
         • Postgresql
   → NoSQL:
      ⇒ azure cosmos db, es serverless es independiente al rendimiento y almacenamiento, es flexible y guarda en ARS (se abstraen y se proyectan como un API), es compatible con otros motores de datos
   → Tiene tipos de datos:
      ⇒ estructurados
      ⇒ no estructurados
      ⇒ semiestructurados
   → Servicios de analisis y big data
      ⇒ Azure synapse analytics
         • datos sin procesar, refinados o seleccionados
         • compatible con sql y spark
      ⇒ Azure hdinsight
         • se pueden crear con cluster tipo, spark, hadoop, kafka, Hbase y mas
         • admite etl
      ⇒ Azure Databricks
         • descubre informacion en volumenes muy grandes
         • compatible con apache spark
      ⇒ Azure data lake analytics
         • realiza análisis bajo demand
         • enfocado a etl
         • modelo pay as you go
- Servicios de computo
   → Azure virtual machine
      ⇒ IaaS
      ⇒ buenas para pruebas y desarrollo, ejecutar apps en la nube, extender recursos en la nube, recuperacion ante desastres
      ⇒ Migración (lift-and-shift) on-premise to cloud
   → Azure batch
      ⇒ conjunto de vms
      ⇒ configuracion rapida
      ⇒ aumento o disminución automático
   → Azure container instances
      ⇒ un maquina virtual mas liviana
      ⇒ solo se virtualizan ciertas cosas del SO
      ⇒ espacio aislados
      ⇒ PaaS
      ⇒ es sencillo
      ⇒ permite cargar contenedores
   → Azure kubernetes services
      ⇒ Orquestacion de contenedores en volumen
   → Azure app services:
      ⇒ permite crear y alojar aplicaciones conectadas a la web
      ⇒ compatible con linux y windows
      ⇒ permite web, app, segundo plano y moviles
   → Azure functions (serverless)
      ⇒ sin servidor
      ⇒ control por evento
      ⇒ pago por uso
   → Azure logic apps
      ⇒ flujos de trabajo basado en eeventos
      ⇒ se pueden crear de forma visual o en json
      ⇒ cuenta con mas de 200 conectores y bloques
      ⇒ los conectores tienen diferentes precios
- Servicios de almacenamiento:
   → Azure blob storage
      ⇒ binary large object
      ⇒ almacenamiento no estructurado
      ⇒ miles de cargas simultaneas
      ⇒ sin restricciones
      ⇒ sirve para imagenes, videos
      ⇒ acceso distribuido
      ⇒ streaming
      ⇒ backup
      ⇒ analisis de datos
      ⇒ almacenamiento de VMs &gt; 8TB
      ⇒ tiene niveles de acceso:
         • frecuente
         • esporadico (30 dias): reportes
         • archivo (180), copias de seguridad
   → Azure Files:
      ⇒ permite almacenar archivos
      ⇒ archivos administrados en SMB o NFS
      ⇒ pueden estar local o en la nube
- Servicios de red
   → Azure virtual network
      ⇒ permite a los recursos comunicarse entre si
      ⇒ se puede tener aislamiento y segmentación
      ⇒ comunicacion con internet
      ⇒ comunicacion entre recursos
         • redes virtuales
         • puntos de conexion
      ⇒ comunicacion con recursos locales
         • redes virtuales de punto a sitio
         • redes privadas (VPN)
         • azure expressroute
      ⇒ enrutamiento de trafico
         • tablas de ruta
         • protocolo de puerta de enolace de borde (BGP)
      ⇒ Filtrado del trafico de red
         • grupos de seguridad
         • aplicaciones virtuales de red
   → Azure VPN GAteway
      ⇒ conecta redes locales a azure via VPN de sitio a sitio
      ⇒ se usan protocolos IPsec e IKE
   → Azure ExpressRoute
      ⇒ genera conexiones privadas entre azure y la infraestructura, sin utilizar internet público
      ⇒ solo 10 conexiones a la vez
- Servicios de inteligencia artificial
   → Azure machine learning:
      ⇒ PaaS para realizar predicciones conectandose a datos para entrenar datos
      ⇒ ofrece control completo del diseño y entrenamiento de algoritmos
   → Azure cognitive services
      ⇒ modelos de ML que permiten ver, oir, hablar, entender y pensar
      ⇒ no se necesitan conocimientos de ML o DS
   → Azure bot service
      ⇒ permite crear bots o agentes virtuales
      ⇒ bot framework
- Servicios de devops
   → Devops services
      ⇒ azure repos, codigo fuente estilo github, estan en la misma nube y uso de la organización
      ⇒ azure boards, tableros para la gestion de proyectos y tareas un jira
      ⇒ azure pipelines, herramienta de automatizacion CI/CD
      ⇒ Azure artifacts, repositorios para guardar artefactos
      ⇒ azure test plans, herramientas de pruebas automatizadas para garantizar la calidad
      ⇒ Github &amp; github actions
   → Azure devtest labs
      ⇒ automatizado de administrar proceso de compilacion, configuracion y anulacion de VMs y otros recursos
- Servicios de supervisión y monitoreo
   → Azure advisor
      ⇒ evalua recursos
      ⇒ hace recomendacion para mejorar
      ⇒ se hace atraves de api o del portal
      ⇒ confiabilidad
      ⇒ seguridad
      ⇒ rendimiento
      ⇒ costos
      ⇒ excelencia operativa
   → Azure monitor
      ⇒ informacion de manera general
      ⇒ recopila analiza y muestra datos para tomar acciones basadas en metricas
   → Azure service health
      ⇒ brinda vista personalizada del estado de los servicios, regiones y recursos de azure
      ⇒ problemas de servicio
      ⇒ mantenimientos planeados
      ⇒ avisos de estado
- Herramientas para administracion y control
   → Visuales
      ⇒ Azure portal
      ⇒ Azure mobile app
         • compatible con iOS y Android
         • supervisa estado de azure
         • alertas, diagnosticos y correcciones
         • se peude ejecutar comando con powershell o bash
   → basadas en codigo
      ⇒ Azure powershell
      ⇒ Azure cli - para sistemas diferentes de windows
      ⇒ Azure resource manager
         • formato json
         • se comprueban antes de ejecutar
         • se define el estado y configuracion de cada recurso
- Serverless
   → azure functions
      ⇒ tiene porciones de codigo
      ⇒ se basa en eventos
         • solicitudes http
         • temporizadores
         • mensajes
         • acciones
      ⇒ tiene escalado automatico
      ⇒ pago por funcion ejecutada
      ⇒ con o sin estado
      ⇒ tareas de orquestacion
      ⇒ compatible con C#, python, ts, js, shell, java, F#
   → Azure logic apps
      ⇒ no-code, low-code
      ⇒ ideal para automatizar y organizar
      ⇒ integracion con aplicaciones
- IoT
   → red de objetos fisicos con sensores y software, con el fin de conectar e intercambiar datos con otros dispositivos a traves de internet
   → Azure IoT hub
      ⇒ conecta los dispositivos con la nube
      ⇒ controlar las apps de forma manual o automatica
      ⇒ se puede supervisar
   → Azure IoT Central
      ⇒ esta basado en hub pero con interfaz virtual
      ⇒ posee plantillas para escenarios comunes
   → Azure Sphere
      ⇒ unidad de microcontrolador
      ⇒ sistema operativo
      ⇒ servicio de seguridad (AS3)
- Servicios de seguridad administrativa
   → Azure security center
      ⇒ brinda visibilidad del nivel de los servicios de la nube y local
      ⇒ supervisa la configuracion de seguridad
      ⇒ aplica cambios automaticamente
      ⇒ brinda recomendaciones
      ⇒ detecta y bloquea amenazas
      ⇒ detecta ataques e investiga amenazas
      ⇒ proporciona control just in time
      ⇒ notifica el estado actual
      ⇒ mejora el nivel
      ⇒ comparacion con puntos de referencia
   → Azure sentinel
      ⇒ recopila datos en volumen
      ⇒ detecta amenazas
      ⇒ investiga con IA
      ⇒ responde a incidentes
   → Azure key vault
      ⇒ administrar secretos o datos confidenciales
   → Azure dedicated host
      ⇒ servidores fisico que no se comparten con nadie mas
      ⇒ tienen mayor coste
   → Seguridad en la red
      ⇒ azure firewall
      ⇒ azure DDos protection
      ⇒ combinacion de servicios
   → servicios de identidad
      ⇒ autenticacion
         • solicitar credenciales legitimas a una persona
      ⇒ autorizacion
         • establece el nivel de acceso
   → Azure active directory
      ⇒ a que se accede y que permisos
   → Multi factor authentication
      ⇒ se le pide un codigo mas aparte de las credenciales
   → Inicio de sesion unico

# Privacidad, cumplimiento y proteccion de los datos
- para microsoft: “los datos de nuestros clientes NO son nuestros datos”
- cumplimiento
   → cumplir con una ley, estandar, conjunto de directrices, normas o requerimientos
- Declaración de privacidad
   → explica que datos personales recopila de nosotros y como los usa
- Terminos de los servicios en linea
   → contrato legal entre microsoft y el cliente
   → detalla obligaciones de ambas partes respecto al procesamiento y seguridad de los datos
- Anexo de proteccion de datos
   → terminos de seguridad y procesamiento de los datos en linea

# Manejo de costos
- Calculadora de costo total de propiedad:
   → costo de azure vs local
   → definir cargas de trabajo
   → ajustar presupuestos
   → consultar informe
- opciones para comprar:
   → contratos enterprise
   → en la web
   → provedor de soluciones (partners)
- Acuerdo de nivel de servicio
   → contrato formal entre empresa de servicios y cliente
   → define estandares
   → que incluyen
      ⇒ introduccion
      ⇒ terminos generales
      ⇒ detalles de SLA
   → entender:
      ⇒ garantias de servicios
      ⇒ hacerlas efectivas
      ⇒ disponibilidades
- Ciclos de vida
   → desarrollo
   → preliminar
   → disponibilidad general
   → desansejado

culture
automatizion
measure
sharing

CT - continues testing
CM - continues monitring

modelos de madurez
- cultura
- procesos
- tecnologia</rich_text>
    </node>
    <node name="kubernetes" unique_id="23" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629332219" ts_lastsave="1629332243">
      <rich_text>para crear y acceder a un pod

kubectl run nginx --image=nginx:alpine --port=80
kubectl port-forward nginx 7000:80</rich_text>
    </node>
    <node name="AWS Certified Developer - Associate" unique_id="24" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629731276" ts_lastsave="1629737116">
      <rich_text># What to expect
- examen de opcion multiple
- es de 65 preguntas
- cuando son multiples respuestas a contestar, se deben acertar todas las respuestas
- para presentarlo no se pueden tener pantallas extras, el mic abierto y con camara

# Deployment 22%
# Security 26%
# Development with AWS 30%
# Refactoring 10%
# Monitoring and Troubleshooting 12%


lectura de dynamo pueden ser:
- eventualmente consistentes a la mitad
- fuertemenete consistentes el total
- en excritura cada una siemrpe usa una unidad aun siendo menos de 1KB
- se pueden usar streams, para ciertsa operaciones, dondelas lambdas pueden procesar esa data</rich_text>
    </node>
    <node name="azure IaaS" unique_id="25" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630695860" ts_lastsave="1640726561">
      <rich_text># Manejando la nube de azure

- Se divide en IaaS y en PaaS
   → IaaS
      ⇒ son maquinas virtuales que ofrecen el servicio
      ⇒ son mas caros, solidos y personalizables
   → PaaS
      ⇒ no se lidia con configuracion del servidor
   → depende mucho de la experiencia
   → arquitectura propuesta
      ⇒ por medio de dibujos que se va a hacer
      ⇒ saber que es capaz de hacer el sistema
- Como se elige una maquina virtual:
   → par que la voy a usar?
   → cuanto tiempo estara encendida?
   → se trata de una aplicacion de investigacion o productiva
   → cual sera el retorno de la inversion de mi MV
   → los requisitos principales pueden cambiar dependiendo de lo que se necesita
   → los tamanios se pueden especificar
   → existen 11 niveles de maquinas, las m de memory, son de las mas grandes
   → de preferencia aprendiendo se recomienda prender y apagar mientras se aprende
   → en productivo ya debe de estar prendida y deberiamos de tener 2 una de prod y otra para pruebas
   → se manejan dos tipos de sistemas operativos, windows (server y desktop) y linux
- Costos:
   → uso de calculadora para estimacion de costos
   → la region si influye
   → el tipo de discos
- todas las herramientas son hechas con python
- az vm list|create  --- para maquinas virtuales
- az vm extension --- para ejecutar scripts
- la modificacion del tama;o se hace desde Size</rich_text>
    </node>
    <node name="GCP" unique_id="54" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#3465a4" ts_creation="1636378908" ts_lastsave="1637615494">
      <rich_text># Computo en la nube
- computo bajo demanda y auto servicio
- conectado a la red
- economias de escala
- elasticidad, cuando se require que los recursos cambien rapidamente
- servicio medido
- * es una computadora enorme de escala mundial

# Esquema del datacenter
- 22 regiones, 67 zonas, 140 puntos y 96 CDN
- estan cerca de cuerpos de agua, se usa como enfriamiento
- con energia renovable para los datacenter, no todos estan al 100% con energia renovable
- PMDC - centros de distribucion de energia
- se unen por fibra optica
- networking room - se conectan la red y cluster de Jupyter, se distribuye cada una de las solicitudes
- Jupyter network equipment - se absorven la mayor cantidad de datos, conecta todos los data center a la red global de G
- LOAD BALANCER, PARA DISTRIBUIR LA CARGA de los servicios de google
- water pipes, el agua que se usa para enfriar las computadoras
- Hot hut, espacio para mover el aire caliente
- cooling plant, mueve toda el agua caliente y obtiene nueva agua fria

# Eras del computo
- maquinas virtuales
- infraestructura en la nube
- nube transformacional

# Arquitecturas en GCP
- Maquinas virtuales
- GKE, aplicaciones computarizadas (google, kubernetes engine)
- app engine, cero administacion de servidores
- cloud run, contenedores serverless
- cloud functions, funciones serverless
- firebase, PaaS de front-end y desarrollo movil

# La red de google
- basado en topologia de capas
- jupiter data, dentro del data center
- b4 backbone, datacenter to datacenter
- b2 backbone, google to internet
- espresso, SDN peering edge
- Global VPC
- Global cloud load balancing
- DNS de baja latencia
- Content delivery networks
- vpc service controls
- network monitoring
- defensa contra DDoS y los ataques web

# Regiones y zonas
- una region es donde se tienen varias zonas
- cada zona es donde vive cada un datacenter

# API abiertas
- Multi cloud patterns
   → generacion de deployments complejos, en diferentes nubes
- Anthos
   → permite crear y administrar aplicaciones modernas

# Beneficios de google cloud
- Nube inteligente
- abierta y flexible
- colaboracion y productividad
- segura
- sustentable
- ahorrar costos
- facil de usar
- soluciones de industria

# Seguridad de varias capas
- on premise de nosotros depende completamente la seguridad
- IaaS, el provedor le toca el hardware y su conectividad
- PaaS, toda la seguridad de usuario, accesos y autorizaciones
- SaaS, el usuario solo se responsabiliza de las politicas de acceso y el contenido
- usar imagenes bases seguras, usar escaneadores de seguridad, autorizacion binaria (para solo correr en la infraestructura), shielded container, sandbox de contenedores, container threat detection

# Proteccion de los datos
- se guarda la informacion en diferentes discos duros, particionada y cifrada
- las llaves se cifran con otra llave

# Presupuesto y facturacion
- las billing accounts son el vehiculo de pago para los gastos de GCP
- billing acount
- payments profiles, metodos de pago
- tipos de convenios:
   → self serve, no se require de contrato para usar GCP
   → existen otros contratos con GCP donde ellos mandan una factura y ya se paga
- se recomienda tener un solo billing account por organizacion
- exportar todos los datos de facturacion a big query

# Jerarquia de recursos
- sirve para la gestion
- todo es un recurso
- se crean en:
   → organizacion, la raiz de la jerarquia de recursos, 50 cloud identities sin costo
   → Folder, modela la estructura organizacional, contenedores para proyectos y carpetas
   → proyectos, son los que tiene los recursos computacionales

# IAM
- quien, persona o subsistema que puede hacer algo en el cloud, confianza cero
- puede hacer que, referencia a los permisos, un rol es una coleccion de permisos, los roles se asignan a grupos, las politicas se pueden asignar a varios niveles, desde organizacion hasta un solo recurso
- en cual recurso

# Roles de IAM
- se asignan roles a grupos, no a usuarios
- grupos de cajon:
   → org admin, gestionan la organizacion
   → administradores de red

# Errores:
¿Qué ha pasado desde que se usa inteligencia artificial en las instalaciones de GCP?
¿Cuál es una ventaja de usar el Cloud Marketplace al armar tu solución de software empresarial?
</rich_text>
    </node>
    <node name="digital ocean" unique_id="55" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1640736033" ts_lastsave="1640809663">
      <rich_text>Digital Ocean and IaaS
- llevando la app desde la personal a la web
- la nube son servicios accesibles gracias al navegador
- IaaS, SaaS, PaaS

Cuando usarlo
- Hosting gratuito
- Shared Hosting
- VPS, servidor privado virtual, aqui es donde es usable
- Dedicado
- Cloud (PaaS/IaaS)
- Datacenter

que es un dropley?
- es la forma de llamar a lo VSP de digital ocean
- ofrece sistemas de almacenamiento que son volumenes y spaces
   → volumes, son bloques de estados solido que se pueden conectar a los droplets, se puede aumentar su espacio
   → spaces, sistemas de almacenamiento masivo para CDNs (red de entrega de contenidos)

Market place:
- maquinas ya preconfiguradas
- es importante agregar el 2FA por si cualquier cosa llegara a pasar, account, security
- se tienen codigos backup por si se llegara a perder el celular o no llegara el mensaje
- se modifica contrase;a en acess
- sshd_config para cambiar el puerto del ssh, en /etc/ssh, systemctl reload sshd

Volumes:
- particiones de disco ssd que se pueden agregar a droplets

- resize, escalamiento vertical, se agregan mas recursos internos del droplet
   → se tiene que apagar la maquina primero
   → el cambio de disco no es reversible
- tambien hay escalamiento horizontal cuando agregamos mas droplets

Networking
- se puede crear IP privadas
   → se puede usar cuando un droplet no se quiere que tenga acceso, y alguno otros nos den salida
   → para agregar la ip privada se tiene que ejecutar: lshw -class network, se busca serial y se obtiene la MAC
   → se asocia la red con la MAC en /etc/netplan/{nombre del droplet} y se agrega en ese archivo la MAC y la IP
   → floating ip, sirve para agregar mas IPs publicas
   → ICMP para hacer ping, si se quita no se sabe si esta activa

Backups y snapshots
- los backups son copias de seguridad que se general una vez al dia, cuestan un dolar al mes, son solo archivos
- los snapshots son copias exacta de como esta el droplet, son utiles cuando se hace un cambio en el SO

History y destroy
- muestra todo lo que ha pasado con el droplet
- destroy para eliminar el droplet

Tags
- las tags son una forma de agrupar todos los droplets que se tienen
- recovery, permite inicar el droplet desde una ISO
- graphs son las estadisticas de lo que se tiene en el droplet, se pueden ver los consumos

API
- se requiere un token de acceso
- manage - API
- </rich_text>
    </node>
    <node name="K8s" unique_id="59" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641688094" ts_lastsave="1642357493">
      <rich_text>- Bases de contenedores
   → es un namespace + cgroups + chroot
      ⇒ Namespaces: vistas de los recursos del SO
      ⇒ Cgroups: Limitan y miden los recursos del SO
      ⇒ Chroot: Cambia el root directory de un proceso
   → un pod es un grupo de contenedores
   → todos los contenedores que esten dentro de un POD comparten la misma namespace de red, tienen la misma ip de red
   → al escalar se crean copias del mismo pod
- raft consensus, si llega a pasar que se pierda el nodo master
- kubelet, crawler para revisar recursos
- kube-proxy, redirecciona a donde se tienen que ir los paquetes
- Modelos
   → declarativo
      ⇒ K8s es un sistema declarativo
      ⇒ que quiero
      ⇒ es sencillo cuando se sabe que quiere
      ⇒ todo se crea desde un spec para decribir cual es el estado deseado
   → Imperativo
      ⇒ como hacer lo que quiero
- networking
   → todo el cluster es una gran red del mismo segmento
   → todos los nodos se conectan entre si, sin nat
   → todos los pod deben conectarse entre si, sin nat
   → se usa kube-proxy
   → los pods estan en capa 3 y los servicios en capa 4
   → container networking interface
</rich_text>
      <rich_text weight="heavy">Kubectl</rich_text>
      <rich_text> es la  herramienta CLI para interactuar con el cluster de kubernetes, puede  usarse para desplegar pods de pruebas, acceder a los contenedores y  realizar algunos comandos como </rich_text>
      <rich_text family="monospace">get nodes</rich_text>
      <rich_text> o </rich_text>
      <rich_text family="monospace">get services</rich_text>
      <rich_text>
En </rich_text>
      <rich_text family="monospace">.kube</rich_text>
      <rich_text> es donde se encuentra nuestro archivo </rich_text>
      <rich_text family="monospace">config</rich_text>
      <rich_text>, la configuración de kubernetes.
 </rich_text>
      <rich_text family="monospace">kubectl get nodes</rich_text>
      <rich_text>: lista todos los nodos que tiene nuestro cluster
 </rich_text>
      <rich_text family="monospace">kubectl --config</rich_text>
      <rich_text>: puedes pasarle el archivo de configuración en caso de estar usando uno diferente.
 </rich_text>
      <rich_text family="monospace">kubectl --server --user</rich_text>
      <rich_text>: especificas la configuración sin necesidad de darle un archivo.
 </rich_text>
      <rich_text family="monospace">kubectl get nodes -a wide</rich_text>
      <rich_text>: muestra más datos de los nodos
 </rich_text>
      <rich_text family="monospace">kubectl describe nodes node1</rich_text>
      <rich_text>: da mucha información de ese nodo en especifico.
 </rich_text>
      <rich_text family="monospace">kubectl explain node</rich_text>
      <rich_text>: permite ver la definición de todo lo relacionado a ese nodo
- deployment es una estructura de mas alto nivel, sirve para manejo de versiones, permite hacer rollbacks
- canary deployment, se tiene una version de app A y app B, se mueve de poco a poco a los pods para las nuevas versiones y se envia de poco a poco el trafico para ver que se comporte estable
- replicaset es una estructura de mas bajo nivel, se asegura ue exista una cantidad de pods en determinado momento

- kubectl port-forward [service] [ports]
- los deamon set no se crean a travez del kubectl
- kubectl apply -f rng.yaml --validate=false, para no validar los problemas
- max-surge, cuantos pods se crean a partir de los que se tienen
- max-unable, a lo sumo puede a ver un 25% de pods que no esten disponibles, que esten iniciando los otros 75% estan trabajando
- al tener 25 y 25 se tiene al menos un 50% de disponibilidad
- kubectl rollout undo deploy [name]
- liveness, cuando el pod no se puede recuperar
- rediness, no muestra un error, puede ser temporal, aun no esta lista, por que esta haciendo algun otro proceso
- son de tres tipos
   → http healthcheck
   → tcp probe, intenta acceder al puerto expuesto
   → command exec, se ejecuta un comando dentro del contenedor
- para acceder a los servicios con minikube se puede usar minikube service [servicio]
- se pueden configurar por diferentes maneras
   → por linea de comando, es para algo muy especial
   → otra es por variables de entorno (env map en el spec)
   → archivos de configuracion (config maps)
- </rich_text>
      <rich_text foreground="#bbbbbb">kubectl get deploy/rng -o yaml \</rich_text>
      <rich_text>
</rich_text>
      <rich_text foreground="#bbbbbb">    | yq eval 'del(.metadata.resourceVersion, .metadata.uid, .metadata.annotations, .metadata.creationTimestamp, .metadata.selfLink, .metadata.managedFields)' -</rich_text>
      <rich_text>
- visibilidad de recursos (isolacion), tipo de recurso, nombre y namespace donde vive
- </rich_text>
      <rich_text family="monospace">kubectl config set-context --current --namespace=blue</rich_text>
      <rich_text>
- namespaces
   → no provee aislacion de recursos, se usan network policies
   → un pod en A se puede conectar con B
   → desde cualquier pod en el cluster nos podemos comunicar con api del K8s
- la forma para desplegar diferentes versiones en un cluster es:
   → es usar otro namespace
- autenticacion y autorizacion
   → la autenticacion se hace por certificados TLS, bearer tokens, basic auth o proxy
   → si no pasa la auth regresa un 401
   → si no es aceptado, el usuario es anonimo
   → un usuario anonimo no puede hacer ninguna operacion
- services account tokens
   → sistema de auth de kubernetes
   → pueden crearse, eliminarse y actaulizarce
   → estan asociados a secretos
   → se usan para otorgar permisos a app y services
   → kubectl get sa default, nos da el secret
- RBAC
   → un role es un objeto con lista de rules
   → un rolebiding asocia un rol a un usuario
   → pueden existir usuarios, roles y rolebidings con el mismo nombre
   → es buena practica es tener 1-1-1 bidings
   → un pod pude estar asociado a un service account

Errores:
¿Dónde se guarda la configuración del cluster de kubernetes?
Los rolebiding son recursos que:
-- asocia un usuario a un rol
¿Cuál es el namespace utilizado por kuberentes para uso administrativo?
-- kube-system
Utilizando un maxSurge y MaxUnavailable del 25% con 3 replicas. ¿Cuál es la cantidad máxima de contenedores que pueden haber corriendo en un momento determinado?
Un DaemonSet se utiliza principalmente para..

-- K8S in GCP
- un servicio sirve para conectar un servicio con otros, o con el mundo exterior
   → Cluster IP, se enfoca a unir microservicios en la misma infraestructura
   → Node Port, permite tener diferentes pods en diferentes nodos, donde se da un mapeo de un puerto
   → Load balancer, se crean para usar con los Node port, administra el trafico para cada uno de los pods
- archivos descriptivos
   → estan en yaml
   → describen la parte logica del app, pods, deployment, services
   → no se describen clusters ni nodos
   → partes
      ⇒ kind
      ⇒ apiVersion
      ⇒ metadata
      ⇒ spec
- labels
   → metadata arbitraria, se peuden poner los nombres que sean para dar una identidad
   → son queryable, para busquedas y seleccion
- selectores
   → son para usar los labels para saber cuando usar unos u otros pods en los despliegues
- namespaces
   → separacion virtual dentro del cluster
   → se puede hacer aislamiento de los datos
   → por defecto
      ⇒ default
      ⇒ kube-system, administracion de k8s
      ⇒ kube-public, tambien es para administracion
- deployments
   → blue-green
      ⇒ tecnica de despliegue de aplicacion con zero downtime
      ⇒ dos ambientes exactamente iguales
      ⇒ solo un ambiente sirve el trafico de produccion
   → canary
      ⇒ sirve para testear nuevas funcionalidades
      ⇒ se despliega una version en produccion a un numero reducido de usuarios
- volumen
   → disco persiste
   → nfs
   → cluster
   → -- sistemas de almacenamiento de las nubes
- apps stateful
   → son aplicaciones que guardan el estado de los datos
   → son
      ⇒ bases de datos
      ⇒ datawarehouse
      ⇒ modelos predictivos
      ⇒ gestores documentales
   → con
      ⇒ pods
      ⇒ volumenes
      ⇒ servicios de tipo cluster IP
   → mayor seguridad al ser una red interna
- Istio, service mesh
   → red para servicios
   → permite fortalecer las politcas de comunicacion en K8s
   → manejo de redes dividos por subredes
   → roles. control de trafico, seguridad y fortalecimiento de las politicas
   → istio es un producto para bajar la complejidad del service mesh
      ⇒ caracteristicas
         • service discovery
         • seguridad
         • instrumentacion
         • rutas dinamicas
         • telemetria
- stackdriver
   → es para monitores, log aggregation y alerting con GCP
- CI/CD
   → google cloud buider
   → maneja cloud repository
   → contauner registry
- knative
   → solucion opensource
   → servicio serverless para contenedores
   → permite escalar
- buenas pracrticas
   → contenedores peque;os
   → organizar despliegues en namespaces
   → configurar los health checks
   → configurar limites en el numero de peticiones
   → terminar con gracia</rich_text>
    </node>
    <node name="swarm" unique_id="58" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641578803" ts_lastsave="1641672202">
      <rich_text>- Routing mesh
   → multiple tareas de un servicio corriendo y que se puede acceder por el mismo puerto
- PORT: </rich_text>
      <rich_text family="monospace">--publish published=8080,target=80</rich_text>
      <rich_text>
- NODE: </rich_text>
      <rich_text family="monospace">--constraint node.labels.region==east</rich_text>
      <rich_text>, </rich_text>
      <rich_text family="monospace">-e constraint:node==csx00153</rich_text>
      <rich_text>
- STACK: docker stack deploy {file} name, docker stack rm app
- Productivo:
   → rotacion de leader
   → los lideres siempre tiene que ser impares
   → para productivo son minimo 3
- herramientas
   → traeffic
   → portainer
- Productivo
   → house kipping, mantenimiento en espacio en disco
   → meltwater/docker-cleanup, modo global sirve para que este en todos los nodos</rich_text>
    </node>
    <node name="jenkins" unique_id="60" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642111509" ts_lastsave="1642118638">
      <rich_text>- Automatizacion
   → para reproducir procesos y nos de mayor productividad
   → que se puede automatizar
      ⇒ pruebas
      ⇒ deployment
      ⇒ verificacion (smokes)
      ⇒ ... cualquier cosa que sea programable
- Jenkins
   → herramienta opensource
   → esta echo en java
   → se puden escribir los plugins propios en java
   → permite escalar de manera vertical (una maquina) y horizontal (varias maquinas)
   → los jobs se pueden escribir en codigo
- Usarios
   → deben de ser unico
   → por temas de auditoria no se debe de usar uno
- jobs
   → la unidad mas importante dentro de jenkins
   → controlado por el build executer
   → se pueden borrar un build
   → el build es una ejecucion de un job
   → se pueden “schedular”
   → se descartan archivos y folders para no tener lleno el disco
- core jenkins
   → jenkins usa las herramientas que estan disponibles en la maquina
   → jenkins lo lee con el filesystem, para eso debe de tener los permisos necesarios
- ecosistema de plugins
   → unidades que extienden a jenkins
   → se pueden poner x versiones de lenguajes para hacer los builds
- cadenas de jobs
   → parameterized trigger plugin
   → watchers
      ⇒ ejecuta otras acciones
   → parameters
      ⇒ ejecuta con parametros
- github
   → se tiene que autoregistrar el hook en jenkins
- freestyle project
   → los mas comunes, fue con los que empezo jenkins
- pipeline
   → scripting pipeline
   → declarative pipeline
      ⇒ agent any, sirve para que se corra en donde sea
      ⇒ se crea a partir de un archivo estilo json
      ⇒ stages para cada paso que se requiere
      ⇒ se comporta muy parecido a un build_spec.yml
- como se puede acelerar?
   → pipeline syntax, en smple steps
   → en replay se puede ver cuanto se tarda sin hacer commit a git
- jenkins slaves
   → permite correr jobs distribuidos
   → conectandolo -- se tiene que entrar al jenkins master
      ⇒ por medio de ssh
      ⇒ en el slave se guarda la llave
      ⇒ se usan authorized_keys
      ⇒ en el master en mange jenkins
         • manage nodes
         • nombre, directorio remoto
         • los executors deben de ser iguales en master que en slave
         • ip o dns

¿Por qué debo tener tiempos de espera en mis “builds”?
Los plugins me ayudan a instalar herramientas exclusivas para Jenkins.</rich_text>
    </node>
    <node name="travisCI" unique_id="61" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642357503" ts_lastsave="1642805775">
      <rich_text>- travis CI
   → en teoria opensource
   → ya no existe la version free, solo por 30 dias
- se usa npm init. para generar un documento automatizado
- src para el codigo
- archivo: .travis.yml
- se establece el lenguaje: language: {lenguaje}
- sistema operativo, el default es linux, os: {nombre del os}
- configuraciones mas profundas
   → git: depth: 3
- branches: except: - legacy, - experimental...
- branches: only: - master, -stable ...
- before_install: - python
- install: - yarn install
- script: - yarn deploy, - yarn test
- before_script: - yarn test
- after_script: - yarn clean
- cache: directories: - node_modules
- jobs: include: - stage: test..script: yarn test, se pueden ejecutar varios script dentro de cada stage
- deploy: configuraciones dependiendo del caso, se requiere la doc
- el cache en el caso de node generalmente se dejar node_modules y .npm
- para deploy se requiere: 
   → provider: pages
   → un github-token
   → el local-dir
   → target-branch: {branch}
   → on: donde debe de ser el cambio para que se active el trigger
- para mandar notificaciones se agrega notification antes del deploy
   → email: recipients: - user1, - user2, on_success: always, on_failure:always
- para slack se puede agregar un app, que es travis_ci
   → nos da el acceso a la configuracion
- Buenas practicas
   → se deben de cifrar los datos sensibles con travis-cli, travis encrypt llave
- terraform validate para comprobar, terraform init para inicializar el entorno
- terraformplan -var-file {archivo}, terraform apply, -auto-approve para que no pregunte
- cuando se tiene el auto.tfvar, se interpreta este directamente
- dynamics: dynamic “ingress” {"se puede iterar sobre un objeto": for_each = var.ingress_rules / content{valores}}, el item es sobre el nombre del dynamic: ingress.value.from_port
- un recurso se llama asi: "${nombre_tf.donde.propiedad}
- los output se ponen:
   → output “varibale” {value=aws_instace.platzi-instace.*.public_ip}
- Terraforma sabe que infraestrucutar crea
   → maneja el estado de almacenaje de conficuraciones, terraform.tfstate, puede ser local y remoto</rich_text>
    </node>
    <node name="IaC" unique_id="62" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642640986" ts_lastsave="1642985336">
      <rich_text>- utilizar archviso de definicion
- sistemas y procesos autodocumentados
- versionar las coas
- preferir cambios peque;os
- mantener los servicios continuamente disponibles
- herramientas:
   → archivos de definiciones de aconfiguraciones, se usan muhco para la infraestructura
   → herramientas para configuracion de servidores, sirven apra configurar los servidores
   → aprovisionamiento, tener el recurso ya listo
- enfoques para gestion de servidores
   → configuracion de servidores
   → empequetar plantillas de servidores
   → ejecutar comando en los servidores
   → configuracion desde un registro central
- como elegir herramienta
   → modo desatendido para herramientas de lineas de comando
   → idempotencia
   → parametrizable
- configuradores: ansible, chef, puppet
- infraestructura -&gt; dependencias -&gt; app/datos
- beneficios
   → creacion rapida bajo demanda
   → automatizacion, creacion y configuracion sin intervencion humana
   → ambientes homogeneos
- Terraform
   → creada en go
   → por hashicorp
   → permite crear infraestructura por codigo
   → no solo se conecta a nubes publicas sino tambien privadas
   → tiene planes de ejecucion
   → facil de automatizar
- gestion de conf vs creacion de infraestructura
   → ansible nos permite crear el estado de la infra, nos permite crearla pero no es su funcion principal
   → infra mutable vs infra inmutable
      ⇒ mutable: cuando el estado del servidor cambia
      ⇒ inmutable: se destruye un servidor y se crea uno nuevo con los cambios
   → lenguaje declarativo vs procedural
      ⇒ declarativo, se dice a las herramientas que hacer
      ⇒ procedural: se dice como hacerlo

## Rasgos
- provider
- resources

## Packer
- permite la creacion de amis personalizadas
- elementos
   → tipo json
   → seccion de variables
   → builders, se decide de donde se construye la imagen
   → provisioners, se personaliza la imagen'
   → post-processors: para manipular la imagen despues de crearse, se ejecuta dentro de la maquina
- se crea un archivo con el nombre credentials, de la forma tradicional para aws
- packer validate para validar la sintaxis
- packer build nombre-del-archivo para ejecutar

## Terraform
- variables descriptivas
- se pueden omitir los valores en la declaracion de variables
- manejo de lenguaje HCL
- por buena practica se define primero el provider, pero no es obligatorio
- terraform validate, para validar la sintaxis, terraform init, para iniciar el entorno, terraform plan, para visualizar que recursos se van a crear, terraform apply para crear la infraestructura
- argumento -var-file {archivo de var}, para pasarle el archivo de variables
- -auto-approve para no tener interaccion con terraform
- si el archivo se llama prod.auto.tfvar, por automatico toma este archivo para las variables
- el nombre del recurso y del objeto no deben de ser igual para que funcione
- para parametrizar se puede usar “dynamic”
   → dynamic “recurso o segmento” { for_each = variable, content { llave: “recurso o segmento”.value."nombre" } }
   → para llamar otro recurso se usa ${nombre.propiedad.valor}
   → los output se manejan: output “nombre” { value = tipo.nombre.*.valorRecurso }
- Archivos de estados
   → terraform sabe que infraestructura crea
   → local, terraform.tfstate, en estado json guarda toda la informacion de lo que creo, si se borra terraform ya no sabe que paso
   → backend, permite almacenar el estado de forma remota a traves de backends (buckets)
   → ayuda para trabajar en equipo
   → mayor disponibilidad
   → para convifurar el backend se necesita:
      ⇒ terraform { backend “tipo” {bucket = “nombre bucket”, key = “como se llamara el archivo”, region="region" } }
- Cifrado del bucket,
   → en el archivo de configuracion:
      ⇒ manejo y creacion de un KMS
      ⇒ se agrega una nueva regla al bucket, con server side encryption
   → en el backend, se agrega el valor encrypt = true, kms_key_id = arn de la key
- versionar es bueno tambien para la IAC
- se puede separar por modulos
- para craer un modulo:
   → module “nombre del modulo”
   → source = “directorio donde se encuentran los modulos”
   → para las variables se tienen que referenciar, nombre=var.nombre
   → las variables deben de estar en la carpeta donde esta el modulo
   → se debe agregar el provider para ue funcione
- modulos remotos, se hace por medio de control de versiones
   → se versiona haciendo de los modulos a git init
   → en el source que se tiene en app, se cambia el nombre de la carpeta por el nombre del repositorio + carpeta de donde esta
- provisioner, permite configurar servidores despues de crearlos, permite conectarse via remota: provisioner “remote-exec” {connection {type = “ssh”, user = “”, private_key = “archivo”, host = self.public_ip}, inline = [comandos]}
- con 0 y -1 se le dice que me puedo conectar a cualquier protocolo con cualquier puerto</rich_text>
    </node>
    <node name="full-course" unique_id="68" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643840042" ts_lastsave="1643840910">
      <rich_text>- less Dockerfile
- se crea usuario devops
- su - devops &amp;&amp; ssh-keygen -t rsa
- docker run -itd --privileged --name servera centros7ss
- docker inspect servera
- ssh devops@192.17.0.3
- se requiere ser el mismo usuario devops</rich_text>
    </node>
    <node name="Devops y Gitlab" unique_id="67" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643838980" ts_lastsave="1645722537">
      <rich_text>-----


# Planificacion
- Agile
   → proceso iterativo
   → uso de spints
      ⇒ se define al alcance
      ⇒ hacemos pruebas
      ⇒ generamos artefactos
      ⇒ release
      ⇒ se vuelve a definir
- Waterfall
   → proceso previamente definido
   → se entrega hasta el final del proyecto
   → se crean la doc mucho antes
   → modelo secuencial
      ⇒ se define el proyecto
      ⇒ build del proyecto
      ⇒ test
      ⇒ release
- Issues
   → empieza una conversacion al rededor del codigo
   → permiten empezar a colaborar sobre una idea antes de codificar
   → sugerir propuestas
   → hacer preguntas
   → reportar bugs y soporte
   → nuevas implementaciones
   → se pueden crear templates para estandarizar
- labels
   → se usa para clasificarlos
   → sirve para filtar y buscar en gitlab
   → permite seguir temas a traves de notificaciones
   → tipos aqui se anaden tambien las eitquetas:
      ⇒ grupo
      ⇒ proyecto
   → ya estan en infromacion del proyecto... cambio
- milestones
   → permite agrupar issues para alcaznar un objetivo determinado en un tiempo especifico
   → agrupador de releases para los clientes
   → burndown chart, se require varios dias de desarrollo
- boards
   → son una forma de visualizar los flujops de trabajo
   → es una de las herramientas de planificacion mas importante de gitlab
   → columnas que agrupan issues por labels
      ⇒ crear labels: ToDo, Doing, Review
- service desk
   → capacidad de abrir issues a traves de correo electronico
   → soporte para los clientes
   → permite para que el equipo no tecnico abra issues o bugs
   → se crea un correo unico para el proyecto cuando se activa
- Merge requests
   → son parte importante para devops, es puerta de la entrada de codigo
   → es la base de colaboraion de gitlab
   → para saber
      ⇒ si se resolvio
      ⇒ si el performance se reduce
      ⇒ si el codigo es seguro
      ⇒ si las nuevas librerias usan licencias abiertas o cerradas
   → deberian de ser de pocos archivos
- CI
   → automatizacion es fundamental
   → es una practica en la que devs manda sus cambios, detona builds y pruebas
   → ayuda a encontrar bugs
   → aumenta la velocidad de releases
   → automatiza el pipeline que lleva el codigo de local a prod
   → hub central ded automatizacion
   → construye, prueba y despliega cambios paquenios en codigo
   → se configura con .gitlab-ci.yml
   → tambien se tiene CDelivery y Cdeployment
      ⇒ en delivery siempre se tiene un artefacto listo para enviar a prod
      ⇒ en deployment se actualiza continuamente el flujo con los cambios
      ⇒ review app, para buena practica, son pruebas en prod por cada branch
   → </rich_text>
      <rich_text link="webs https://gitlab.com/neimv/platzi-devops/-/ci/lint">https://gitlab.com/neimv/platzi-devops/-/ci/lint</rich_text>
      <rich_text>
- github pages
   → hosting estatico
   → integracion con gitlab ci
   → dominios personalizados
- Desarrollo agil
   → capacidad de responder al cambio
   → es un framework basado en 12 reglas
   → manejo de cambios con exito
- Autodevops
   → permite crear un flujo de devops inmediato con la creacion del proyecto
   → se corren analisis estaticos y dinamicos de seguridad
   → requisitos
      ⇒ gitlab runner
      ⇒ kubernetes
      ⇒ base domain
      ⇒ prometheus
   → nosotros podemos crear
      ⇒ dockerfile
      ⇒ .gitlab-ci.yml
      ⇒ Variables
- container registry
   → cuando se hace un build se genera una imagen de docker
   → empaquetado de codigo
   → permite almacenar imagenes de docjer
   → cada que se creat un build la nueva imagen se envia al container registry
- DevSecOps
   → el equipo de seg trabajaba aislado y ejecutaba al final del proceso
   → no habia problemas en esos entonces
   → es pensar en la seguridad en app y en infra
   → trata de automatizar la seguridad e incluirla en el ciclo de vida de la app
   → devsecops manifiest:
      ⇒ los datos y la ciencia de la seguridad siempre anteponer ante el miedo insertidumbre y duda
      ⇒ los scores de seguridad debn de ser automatizados
      ⇒ el monitoreo tiene que ser 24/7
      ⇒ se deben de cerrar todas y cada una de las puertas abiertas
- firmas de seguridad
   → GPG
      ⇒ permite identificar en los commit
      ⇒ anade una capa adicional de seguridad a git
      ⇒ gitlab despliega un banner junto a los commits, para mostrar que dichos commits estan verificados
- pruebas estaticas de seguridad
   → buscan en los archivos, patrones inseguros de codigo
   → verifica que no halla secrets en el codigo
   → crea reportes
   → utiliza la imagen de docker sst de gitlab
   → tipos de vulnerabilidades
      ⇒ criticas
      ⇒ altas
      ⇒ medianas
      ⇒ bajas
      ⇒ desconocidas
   → de acuerdo a las vulnerabilidades se puede saber que hacer para que no afecte
   → diferentes herramientas para el escaneo
- escaneo ded contenedores
   → clair y clair-scanner para verificar contenedores
   → se pueden omitir vulnerabilidades con el archivo clair-whitelist.yaml
- escaneo de dependencias
   → analiza estaticamente las dependencias del proyecto
   → genera reportes
   → utiliza docker dependency scanning de gitlab
   → include: template: dependency-scanning.gitlab-ci.yaml
- pruebas dinamicas de seguridad
   → asume que hay un atacante exetrno
   → utiliza OWSAP ZAP proxy y ZAP basline
   → correa analisis pasivo
   → genera reporte con el merrge request
   → solo corre pruebas pasivas
      ⇒ no uso de cokies inseguras
      ⇒ JS sin acceso a cookies
   → DAST.gitlab-ci.yaml
- gitlab security dashboard
   → es un hub centralizado donde se pueden ver las vulnerabilidades de prod
   → permite acceder rapidamente a los riesgos detectados
   → permite validar vulnerabilidades como invalida o no ap[licable
   → genera vinculos a los reportes de seguridad
   → esta en project-&gt; security dashboard
- CD
   → continuos delivery, siempre listo para mandar a prod un artifact
   → continuos deployment, directo a produccion el codigo
   → se puede poner en riesgo directo a prod con bugs o con downtime
   → feature flags para activar diferentes features en codigo en prod
   → sla, son contratos firmados donde se debe tener cierto downtime
      ⇒ aqui entran los SRE, que basicamente ponen freno a los deployment, para no incumplir
      ⇒ gitlab contiene diferentes tipos de estrategias
         • se pueden usar variables de ambiente
         • se tiene un CI/CD
         • 3 tipos de estrategias
            ◇ rapidamente se manda el codigo a prod
            ◇ se manda continuamente pero no se activa en todos los pods, se activa de poco en poco
            ◇ se manda a staging primero y a prod se envia manualmente
- Ambientes
   → permiten realizar pruebas en diferentes ambientes
   → lo mas normal es:
      ⇒ prod
      ⇒ stage
      ⇒ dev
      ⇒ local
   → hay 3 tipos
      ⇒ estaticos, no cambian, es la misma infraestructura
      ⇒ dinamicos, se puede crear un ambiente por cada branch de desarrollo
      ⇒ protegidos, ambientes donde se definen ciertas personas que pueden hacer deployment a estos ambientes
   → se pueden definir usando el keyword, enviroment, se puede poner nombre y url, y cuando se hacen los deployment, manuales o automaticos
   → la variable de REPLICAS puede crear varias insatancias (pods)
- Review apps
   → permite ver los cambios de un feature branch
   → los disenadores y PM pueden ver los cambios sin necesidad de levantar un ambiente local
   → el merge request es aprobado el feature branch se borra, se detiene el review app y se destruye la infra
   → generan un ambiente completo por cada branch
   → se tienen dos job distintos
      ⇒ uno genera el ambiente y hace el deploy, para borrar el branche se necesita un on_stop: stop_review
      ⇒ el otro es stop_review
- rollback
   → algunas veces pasan cosas que no pensabamos que iban a pasar
   → mas complejo es mas dificil de entender
   → permite automatizar el regreso a ambientes libres de bugs
- monitoreo
   → antes no se preocupaba muhco, el codigo era estatico
   → casi no se tenian cambios anteriormente
   → se volvio indispensable con devops
   → se obtiene la visibilidad de la salud y el performance del equipo
   → todos los ambientes se monitorean
   → familiarizarce con las metricas
   → automatizar el monitoreo
      ⇒ se debe generar alertas cuando algo falla
      ⇒ esto para prevenir que el ambiente caiga
   → compartir datos
      ⇒ reportes
      ⇒ accesos privilegiados
   → monitoreo de apps, infra y equipo
   → cuando se establecen metricas, el ser humano se dedica a maximizar esas metricas
- performance metrics
   → nos dan una idea de que tanto esta creciendo la infra y que capacidad de respuesta se tiene
   → se da una idea de como afinar un workload o una query
- health metrics
   → permite entender si la infra esta a punto de fallar
      ⇒ cuando la memoria o cpu esta llegando al limite de lo que se esperaba
   → se permite generar alertas para cada metrica
      ⇒ generalmente se quieren tener buenas metricas de performance, pero las de salud tambien son importantes
- metricas de equipo
   → cycle analytics
   → lenguajes de programacion
   → commits por dia, semana, mes, hora
   → pipelines existosos
   → contribuciones personales del equipo
   → git timeline
   → ahora es value stream
- rastro de errores
   → se debe generar el stack trace para saber donde esta fallando
   → para esto se maneja sentry
   → 



---
</rich_text>
      <rich_text family="monospace"># know if gpg is installed
which gpg

# install gpg with brew
brew install gpg

# installa pineentry mac (pgp handler)
brew install pinentry-mac

# generate key
gpg --full-gen-key
# 1. select algorith (RSA and RSA for default)
# 2. select keysize (the longer the better)
# 3. specify key time validation (1 year could be fine)
# 4. Fill out your personal data. Email must the same as your GitLab account mail

# list keys
# GPG key ID -&gt; that starts with sec i.e. rsa4096/&lt;GPG-key-ID&gt;
gpg -k --keyid-format LONG
gpg --list-secret-key --keyid-format LONG
gpg --list-secret-key --keyid-format LONG &lt;your-email&gt;

# export the public key and add to GitLab (User Settings &gt; GPG Keys)
gpg --armor --export &lt;GPG-key-ID&gt; | pbcopy

# configure git to use the public key to sign commits
git config --global user.signingkey &lt;GPG-key-ID&gt;
git config --global gpg.program gpg

# commit with -S flag to sign
git commit -S -m "My signed commit"

# or tell Git to sign your commits automatically
git config --global commit.gpgsign true


# DELETE GPG KEYS

# first delete private key
gpg --delete-secret-key key-ID

# then delete public key
gpg --delete-key key-ID </rich_text>
      <rich_text>

---
devops avanzado
- Implementacion de pruebas
   → sin pruebas no hay confianza
   → correr test manualmente no se debe de hacer
   → unit + integration + acceptance
   → unit test using mocks
   → integration test usan dependencias reales con fixtures
   → acceptance test usan un ambiente con todos los servicios, muy parecido a prod
- Continuos integration y artifacts
   → empieza con git
   → unit test
      ⇒ sirve como historial
      ⇒ saber que ocurrio, como y quien
   → code analysis
      ⇒ se debe de tener codigo limpio
      ⇒ codigo seguro
   → test coverage
   → release
   → la salida de un CI es un artifact
   → en auditoria se puede llegar a pedir hasta un año de artifacts
   → tiene mas alcance un integration test que un unit test
   → se requiere que el codigo tenga un X porcentaje de pruebas, puede ser bueno y malo a la vez
   → pull requests reviews
- jenkins
   → el build no se hace local, por que queremos que sea centralizado, jenkins debe tener el cache
- herramientas externas
   → sonarqube
   → plugis de jenkins
   → circle CI
   → tienen analisis de complejidad, de coverage
   → ventaja de que jenkins corre localmente
- CD (continuos deployment)
   → CD vs CD vs mano
      ⇒ Delivery, es complemente manual el deploy a prod
      ⇒ Deployment se envia a prod directamente
      ⇒ move fast and brek less things
      ⇒ tipos
         • blue/geen, uno actualiza mientras el otro sirve el trafico, este hace un update inmutable
         • canay, se tiene un pool de nodos, se van modificando de poco a poco, mientras se monitorea el cambio, se peude revertir rapidamente
         • rolling, el mas conocido y mas viejo, de una por una se hace el update
- Incident response
   → las personas que hacen la infraestructura no deben de resolver las incidencias, deberian de ser los que mandana prod
   → pagerduty tiene una guia
   → llega la llamada y debe verse quien
   → se debe checar el tamanio del incidente
   → se debe de llevar un historial de lo que se hizo, para no repetir
   → se deben escribir postmorten
      ⇒ que paso
      ⇒ que aprendimos
      ⇒ y que podemos mejorar
- SLO/SLI
   → es como las companias miden que el producto esta bien
   → se puede aplicar ciertas partes dentro de las empresas
   → Service level objectives/Service level indicator
      ⇒ mide los success
      ⇒ latencia, cantidad de errores, envio de email y respuesta
      ⇒ son metricas que se pueden medir o se pueden definir
   → si se rompe la metrica, se consume el uso de deploys por mes, se debe de tener cuidado con esto y tener un sla constante
- uptime monitoring
   → lo primero que se observa es si esta arriba o no
   → pruebas de smokes
   → se debe de usar un provedor externo que nos ayude a eso
      ⇒ apex.sh
      ⇒ pingdom
- trackers y logs
   → para debuggear
   → en prod son menores
   → se tiene que ser moderado con esos por que cuestan
   → los level en los logs son sumanente importantes
   → deben de tener una buena estructura
   → sentry
- metricas
   → se deben mirar los logs cuando algo se hizo complemente mal
   → se pasa mas tiempo viendo metricas
   → generar metricas tambien cuesta</rich_text>
    </node>
    <node name="testing" unique_id="69" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1645384680" ts_lastsave="1646033637">
      <rich_text>
- que son pruebas y por que deberiamos hacerlas
   → mejores practicas para no introducir errores
   → es el proceso de evaluar un producto, exploracion y experimentacion, se requiere de entenderlo
   → no existe un software sin errores
   → razones
      ⇒ un problema o un resultado no deseado
      ⇒ costo algo o fuera de presupuesto
      ⇒ implicaicones legales o de estandares
- proceso de pruebas y estandares
   → se deben de tener
      ⇒ herramientas
      ⇒ recursos
      ⇒ metodologias
   → un tester debe documentar, identificar problemas y comunicarlos
   → la falta de comunicacion puede hacer retrabajo
   → la mayoria de errores se cometen en el analisis y dise;o del software
   → definir la falta de calidad
      ⇒ detectar y corregir la falta de calidad
   → calidad del software
      ⇒ calidad del producto
         • lo que la gente produce
      ⇒ calidad del proceso
         • como lo hace la gente
   → cerficaciones, estandares y metodologias para
      ⇒ individuos
      ⇒ procesos
      ⇒ empresas
      ⇒ servicios
      ⇒ tipo de industria
   → ISTQB
   → IEEE
   → TPI
- Calidad y defectos
   → es una perscepcion entre lo deseado, analizado y el entregable
   → esta la define el cliente
   → es parte del proceso, se va mejorando conforme se va creando el sistema
   → grado con el que un sistema cumple los necesidades o expectativas del cliente, IEEE
   → anomalia, no siempre es reproducible
   → defecto, problema que se puede reproducir una y otra vez
   → fallo, situaciones no asociadas al software
   → error, accion humana incorrecta
- principios del testing moderno
   → allan page
   → evolucion del testing agile
   → el tester debe enforcarse en la calidad del software y el dev en desarrollar la app
   → los 7 principios
      ⇒ se pasa de ser dueno de defectos a trabajar con el dev y mejorar el proceso
      ⇒ 1 la prioridad es mejorar el negocio
      ⇒ 2 se acelera al equipo, modelos como Lean Thinking y teoria de las restricciones, para identificar, priorizar y mitigar cuellos de botella
      ⇒ 3 para la mejora continua hay que adaptarse y optimizar
      ⇒ 4 preocuparse profundamente acerca de la cultura de calidad
      ⇒ 5 nosotroe creemos que el cliente es el unico capaz ded juzgar y evaluar la calidad
      ⇒ 6 nosotros usamos datos de manera extensa y profunda para entender los casos de uso del cliente
      ⇒ 7 expandir las habilidades de testing y el conocimiento en todo el equipo
- especialidades del testing
   → automation tester
   → security tester
   → data science tester
   → SDET, es un desarrollador que hace pruebas, automatiza y hace uso de herramientas para automatizar la entrega
   → devops
   → qa engineer, mas enfocado en el producto y proceso
   → qe, quality enginer, es un coach
   → manual tester

- presupuesto, recursos, tiempo y actividades clave
   → presupuesto
   → recursos
   → tiempo
   → una mala planeacion del proyecto puede hacer que los costos se incrementen y el proyecto puede ser cerrado
   → ciclo de desarrollo del software
      ⇒ definicion de necesidades
      ⇒ analisis
      ⇒ disenio
      ⇒ codificacion
      ⇒ pruebas
      ⇒ validacion
      ⇒ mantenimiento y evolucion
      ⇒ se repite
   → testing moderno
      ⇒ se puede llevar en cada etapa del ciclo
         • analisis: documentacion de especificacion, requerimientos ambiguos, que hace o no el software no cumple la peticion
         • disenio, se establece lo que el cliente quiere ver, rango de los campos de captura, como deben de ser las reglas de estos mismo, que pasa si, se cumple o no cumple la condicion
         • codigo, se pueden tener modulos o funciones, pruebas sobre datos de desarrollo, revisar si se esta haciendo bien el software
         • pruebas, en el back-end, se puede testear en cada uno de los requerimientos o parte funcional que pidio el cliente
            ◇ validacion
            ◇ verificacion
            ◇ aceptacion, usuario final
- Estrategia de pruebas
   → nos permite conocer por donde comenzar
   → planearlas
   → todos los tester necesitan saber por donde van a empezar
   → escenarios, depende mucho de variables como costos
      ⇒ arquitectura
      ⇒ seguridad
         • plataformas(dependera de las plataformas que este conectadas), SO, accesos(diferentes tipos de perfiles), datos, reportes(no todos deben de acceder, solo casos especiales)
      ⇒ performance
      ⇒ usabilidad
      ⇒ escalabilidad
- testing en desarrollo de software
   → testing
      ⇒ exploracion de una idea, como sucede el flujo, crean datos que generen nuevos resultados, este nunca termina, siempre hay escenarios nuevos
   → checking
      ⇒ saber que sucede y verificar que siga pasando
      ⇒ liberacion de nuevo codigo
      ⇒ pruebas duplicadas, pruebas similares, pruebas sin valor agregado, pruebas caducadas
   → automatizacion de pruebas, es un checking repetitivo y automatizado, la exploracion manual se la mejor apra checar nuevos cambio
   → desventajas
      ⇒ pobre cobertura de pruebas
      ⇒ falta de actualizacion
      ⇒ mal manejo de versiones
   → ventajas
      ⇒ correr pruebas en paralelo
      ⇒ reduccion de error humano
      ⇒ probargrandes cantidades de datos
   → el coverage es muy importante para saber que algo ya este probado y que no
   → la automatizacion es escencial cuando las tareas no cambian, generalmente en la parte de operaciones, devops
- Testing agile
   → involucra a todos los miembros de un equipo, el tester e un experto multifuncional
   → estrategias
      ⇒ el testing es de todo el equipo
      ⇒ debe ser independiente
      ⇒ integracion continua
      ⇒ testing guiado por pruebas
      ⇒ desarrollo guido por comportamiento
      ⇒ desarrollo guiado por pruebas de aceptacion
- Niveles de pruebas
   → nivel de pruebas de componentes
      ⇒ aquella que se puede ver, que inteactua
   → pruebas de integracion
      ⇒ prueba entre sistemas, flujp completo, como entran y salen datos
   → pruebas del sistema
      ⇒  se tiene el contexto
      ⇒ multiples sistemas
   → prueba de aceptacion
      ⇒ el entregable al cliente
- tipos de pruebas
   → tecnicas a emplear para encontrar defectos
   → pruebas funcionales
      ⇒ que debe de hacer el sistema
      ⇒ de caja negra
   → pruebas no funcionales
      ⇒ lentitud
      ⇒ otros colores
      ⇒ no lee o no ve bien
      ⇒ usabilidad y accesibilidad
   → pruebas estructurales
      ⇒ tecnologia y stack que se esta usando
      ⇒ debe funcionar bien con la estructura
      ⇒ de caja blanca
   → pruebas de manejo de cambios
      ⇒ verificando que impacta con un nuevo cambio
- pruebas estaticas y dinamicas
   → estaticas
      ⇒ doc, software, comparacion, planteamiento o plan de pruebas
      ⇒ contratos, planes, calendarios del proyecto
      ⇒ examinacion manual
      ⇒ analisis de requerimientos
      ⇒ especififcaciones o reglas de negocio
   → dinamicas
      ⇒ demostrar en la ejecucion como esta funionando el software
   → historias de usuario
   → criterios de aceptacion
   → mockups
   → diseño arq.
   → las pruebas
   → guias de usuario
   → evaluacion/revision del codigo
   → beneficios
      ⇒ detectar y corregir defectps
      ⇒ identificar y priorizar
      ⇒ prevenir defecto
         • no tan facil en pruebas dinamicas
         • durante la etapa de analisis y diseño
      ⇒ cubrir aspectos que parecen inconsistentes o ambiguos
      ⇒ se reduce el retrabajo
      ⇒ reduce costo y el tiempo
      ⇒ mejora la comunicacion entre todos los miembros del equipo
- definicion y plan de pruebas
   → si haces testing sabes para que se hacen
   → encontrar problemas
   → documentar problemas
   → comunicar problemas
   → si no sabes documentar, esto provocara retrabajo
   → se debe de poder comunicar, servicio al cliente o usuario
   → la ejecucion de pruebas debe de ser clara para todo el equipo
- Pruebas de caja blanca, gris y negra
   → negra
      ⇒ no se ve como fue construida
      ⇒ solo se tiene la interfaz con que se interactua
      ⇒ en esta se trata de no tener sesgo por lo tanto a veces se busca a lguien externo
      ⇒ particion de equivalencia
         • grupos de datos que pueden entrar para casos exitosos y no exitosos
      ⇒ valores limites
         • se pueden tener valores como flotantes
      ⇒ tabla de decisiones
         • se tiene un ticpo de valor de checkboxes o listas
         • son valores fijos que no puede introducir el usuario
      ⇒ transicion de estados
         • estados activos en formularios dependiendo de los otros campos
      ⇒ casos de uso
         • casos que ya se definieron
      ⇒ back
   → blanca
      ⇒ es como una caja de cristal
      ⇒ se puede ver todo lo que contiene el software
      ⇒ cobertura de declaraciones
         • todo lo que se tiene en el codigo que debe de hacer
         • dependiendo del software y reqs. se establece un procentaje de cobertura
         • todas las lineas de codigo deben ejecutarse al menos una vez
      ⇒ cobertura de codigo
         • sentencias
         • decisiones
         • condiciones
         • no debe de tener codigo inutil
      ⇒ front
   → gris
      ⇒ se pueden tener las integraciones
      ⇒ datos de como fluyes
      ⇒ no veo codigo o una interfaz
      ⇒ casos de negocio
      ⇒ pruebas end to end
      ⇒ pruebas de integracion
      ⇒ datos y servicios
- gestion de monitoreo y control
   → planeacion
      ⇒ se deben definir el objetivo de las pruebas
   → monitoreo y control
      ⇒ se buscan las metricas para saber si estamos llevando avances o un retraso
   → analisis
      ⇒ decision de prioridades, las prioridades de cobertura son escenciales
   → diseño
      ⇒ diseño de casos de alto nivel
      ⇒ diseñar y priorizar las pruebas
      ⇒ identificar los datos de pruebas
      ⇒ identificar el entorno de pruebas
      ⇒ hacer trazabilidad entre pruebas y sus condiciones
   → implementacion
      ⇒ debemos contar con estructura suficiente para hacer las pruebas
      ⇒ un ambiente de pruebas
   → ejecucion
      ⇒ los suites de pruebas se deben ejecutar de acuerdo al plan que se decidio
   → finalizacion
      ⇒ defectos con el estatus correcto
      ⇒ reorte para comunicar los resultados de las pruebas
      ⇒ finalizar y archivar el ambiente de pruebas y datos
      ⇒ entregar el testware al equipo de manto de pruebas
      ⇒ analizar lecciones aprendidas
      ⇒ recopilar la informacion para ayudar a la madurez del proceso de pruebas
   → solo es concentrado en las pruebas, cuando y quien las lleva
- Roles y Responsabilidades
   → especialista en pruebas manueales
   → pruebas tecnicas
   → lider del equipo de preubas
   → ingeniero de calidad
- Retrabajo
   → un dashboard es bastante util para mantener informado a todo el equipo
   → acciones de control
      ⇒ si identificamos un riesgo
      ⇒ siidentificamos falta de ambientes
      ⇒ si el criterio de salida no se cumple
   → resultado de las pruebas
      ⇒ todo lo que va pasando
   → desempeno del equipo del testing
      ⇒ que pasa con la gente, equipo, internet, circunstancias personales
   → es un esfuerzo adicional necesario apra la correccion de una inconformidad de algun producto
   → falta o mala documentacion
   → falta de capacitacion o dominio en las herramientas
   → falta de capacidad o dominio en el software
   → falta de comunicacion
- Sistema seguimiento de bugs
   → bugs o defectos
   → hay presion de entrega del software
   → descuidos en el disenio
   → inexperiencia
   → falta de comunicacion de reqs
   → disno complejo de codigo
   → desconocimiento de tecnologias
   → como gestionar
      ⇒ que se debe de hacer al encontrarlo
      ⇒ que herramienta para documentar
      ⇒ que informacion se necesita saber
      ⇒ cual es el estatus
      ⇒ como saber cuando ya se resolvio
   → repositorio y monitoreo de dectos
      ⇒ no cualquiera puede cambiar los estatus, debe de ser alguien responsable de revisar todo
- defectos y sugerencias
   → no siempre quieren que el qa de sugerencias solo buscar defectos
   → defectos
      ⇒ es aquello que no cumple requerimientos, consecuencia de un error humano
   → sugerencia
      ⇒ como la experiencia de usuario es afectada, velocidad, fluides de navegacion
- depuracion
   → es una tecnica como herramienta para saber como esta ocurriendo el defecto
   → los errores de ejecucion es el tiempo que se pierde por cada cierto numero de lineas, un tiempo generalmetne por cada 10 lineas
   → debuger es una herramienta que ayuda para checar esos errores
   → encontrar, analizar y encontrar defectos
   → se deben analizar, variables, como se transfieren los datos, como se transforma esa informacion
   → siempre va a ver errores
   → sintomas de errores
      ⇒ salida incorrecta
      ⇒ operacion fuera de lo normal
      ⇒ no finalizacion del programa
      ⇒ caidas del programa
   → tipos de herramientas
      ⇒ debugger
      ⇒ manual
      ⇒ local/remota
   → mensajes de advertencia
   → estandares de compilacion
   → verificacion sintactica y logica
- pruebas de verificacion
   → sirve para confiramr que un cambio se hizo o un defecto se corrigio
   → debe funcionar a los requerimientos
   → se buscan nuevos escenadios donde se utilicen valores relativos
   → pruebas de regresion
      ⇒ la matriz de pruebas durante el debuggin, modulos impactados que requieres regresion
      ⇒ las preubas de regresion ya fallaron la primera vez al no tener suficiente cobertura, se incorporan nuevos
   → documentacion
      ⇒ actualizar
      ⇒ comentarios en el codigo
      ⇒ documentacion tecnica
      ⇒ pruebas unitarias
      ⇒ pruebas especificas
      ⇒ matrices de pruebas
      ⇒ plan de pruebas
- tecnicas de depuracion
   → deben de cambiar de ser reactivas a preventivas
   → debugging debe de ser la ultima tecnica en utilizarce
   → debugging
   → logs
      ⇒ almacenar los valores
      ⇒ rastreo de informacion
   → historial
      ⇒ capacidad de analisis forense
      ⇒ agrupar informacion
   → monitor reporter
      ⇒ prevenir ataques o fallas
      ⇒ observar anomalias
      ⇒ acelerar tiempos de respuesta
      ⇒ aplicar tecnicas de machine learning
      ⇒ mejorar gestion y el control de la informacion
      ⇒ detectar amenazas de red
      ⇒ prevenir fugas de iinformacion
- automatizacion de pruebas
   → se tienen pruebas repetitivas
   → script para que las haga un maquina
   → se define un framework, se debe estandarizar
   → falla si solo se crean scripts sin documentar las pruebas
   → se puede automatizar
      ⇒ pruebas unitarias
      ⇒ pruebas de integracion
      ⇒ funcionales o de aceptacion
   → BDD
      ⇒ desarrollo guiado por comportamiento
      ⇒ son pruebas que se escriben para verificar que el comportamiento es correcto desde el punto de vista de negocio
      ⇒ lenguaje sencillo y simple para que todos en el equipo comprendan el por que las pruebas
- gherkin
   → el retrabajo se deriva de malas practicas y no de tomar el trabajo enserio
   → reduccion de tiempo de casos de pruebas manual a automatizada
   → lenguaje de texto plano con estructura, facil de aprender
   → ventajas
      ⇒ simple
      ⇒ palabras claves
      ⇒ estandariza
      ⇒ reduce tiempo de disenio
   → principales keywords
      ⇒ feature
      ⇒ scenario
      ⇒ given, when, then, and, but
      ⇒ background
      ⇒ scenario outline
      ⇒ examples
      ⇒ comentarios con #


ERRORES
¿Cuál es la forma correcta de pasar opciones a un programa?
Utilizando un maxSurge y MaxUnavailable del 25% con 3 replicas. ¿Cuál es la cantidad máxima de contenedores que pueden haber corriendo en un momento determinado?
¿Cuál de los siguiente tipos de servicio aloca una entrada de DNS para acceder al servicio?
Sistema numÃ©rico utilizado para cambiar permisos de forma numÃ©rica en Linux:
¿Cuándo es útil la opción recovery en un droplet?</rich_text>
    </node>
    <node name="Gerencia de tecnologia" unique_id="71" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649170301" ts_lastsave="1649170306"/>
    <node name="new-aws-platzi" unique_id="74" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649794315" ts_lastsave="1649799252">
      <rich_text>TI tradicionales

¿Cuál de las siguientes es una característica de la computación en la nube?
Necesitas  concentrarte en el código y gestión de tu aplicación SIN preocuparte por  la infraestructura. ¿Qué tipo de cómputo en la nube recomendarías  utilizar?
Necesitas tener control sobre el sistema operativo de tus servidores. ¿Qué tipo de cómputo en la nube recomendarías utilizar?
¿Cuál de los siguientes servicios es de tipo SaaS?
</rich_text>
    </node>
    <node name="cloudformation" unique_id="77" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650935953" ts_lastsave="1650935953"/>
  </node>
  <node name="JS" unique_id="26" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625688385" ts_lastsave="1630594410">
    <node name="fontend-dev" unique_id="27" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625688395" ts_lastsave="1626215724">
      <rich_text>- html, es un lenguaje de marcado, estructura el sitio web
- css, permita crear un diseño agradable

# renderizado:
- DOM
   → document object model
   → se transforman las etiquetas a objetos que entiende el navegador
- CSSOM
   → es casi lo mismo que el DOM, solo que es para CSS
- RenderTree
   → es el arbol que uno al dom y al cssom
   → los pasa:
      ⇒ Bytes
      ⇒ characteres
      ⇒ tokens
      ⇒ nodes
      ⇒ dom
   → se crea primero el arbol y luego se le asigna el css que le corresponde
- el navegador hace:
   → procesa html y construlle el dom
   → procesa el css y construye el cssom
   → dom + cssom = render tree
   → ejecuta el diseño en el render tree
   → pinta el nodo en la pantalla

# HTML
- la etiqueta i es para italica mientras que em enfatiza

# CSS
- con nth-child -&gt; se agrega 2n para los parrafos pares
- con el resolutor de ambito (::) se usan los pseudoelementos (first-letter para jugar con elementos de P)
- Valores relativos y absolutos
   → absolutas, cm, in, mm, px, pt, pc
   → no se fijan en la medida de algo más
   → relativas: relativas a otra unidad de medida u otro elemento
      ⇒ vmax, em...
- Arquitecturas:
   → deben ser:
      ⇒ predecibles
      ⇒ reutilizables
      ⇒ mantenible
      ⇒ escalable
   → buenas practicas:
      ⇒ establecer reglas
      ⇒ explicar la estructura base
      ⇒ establecer estandares de codificacion
      ⇒ evitar largas hojas de estilo
      ⇒ documentación
   → OOCSS
      ⇒ orientado a objetos
      ⇒ diseño separado del contenido
   → BEM
      ⇒ block element modify
      ⇒ separa los bloques, elementos y modificadores
      ⇒ se usa “__” para agregar el nuevo elemento de clase y “--” para agregar el elemento modificador
   → SMACSS
      ⇒ arquitectura de CSS escalable y modular
      ⇒ se divide en
         • base: componentes que se usan en toda la applicacion como botones
         • layout: elementos que solo se usan en la pagina una vez
         • module: componentes que se usan en la app mas de una vez
         • state: cambios que se ven en ciertos elementos
         • theme: cuando halla cambios en temas sea facil hacer esos cambios
   → ITCSS
      ⇒ Triangulo invertido de CSS
      ⇒ se dividen los archivos de CSS en ciertas partes:
         • ajustes
         • herramientas
         • genericos
         • elementos
         • objetos
         • componentes
         • utilidades
   → Atomic Desing
      ⇒ atomos: elementos mas chicos
      ⇒ moleculas: conjuntos de atomos
      ⇒ organismos
      ⇒ templates
      ⇒ paginas

# Construcción de componentes
- es un elemento muy pequeño que sirve para construir componentes mas grandes en un futuro
- se deben de identificar para ver donde se pueden volver a utilizar
- strorybooks -&gt; npm

# Flexbox
- sirve para ayudar en la parte de la alineacion
- se tiene un contenedor con sus items:
   → container maneja el display
   → flex-direction para manejar hacia donde queremos que vallan los elementos
   → se le puede colocar orders
- CSS grid
   → nos ayuda a manejar todo el layout
   → maquetación y diseño responsivo
   → display en grid
   → se pueden por porcentaje cuantas columnas y filas
- media queries:
   → se pueden ajustar diseños a dispositivos mas pequeños

# Preprocesadores
- sass:
   → manejo de variables, se definen con $
   → anidamiento, se puede tener una clase que tenga varios elementos
   → herencia, se usa con @extend {nombre de la clase}
   → mixin, reutilización de codigo, se hace para cuando se tienen propiedades que se repiten varias veces y se usa @include

# Accesibilidad:
- manejo de buena semantica
- uso de lectores de pantalla
- voiceover
- ANDI
- todos los input deben tener una etiqueta label (aria-label)

Errores:
La etiqueta </rich_text>
      <rich_text family="monospace">&lt;em&gt;</rich_text>
      <rich_text> y la etiqueta </rich_text>
      <rich_text family="monospace">&lt;i&gt;</rich_text>
      <rich_text> hacen que el texto contenido en ellas sea itĂ¡lico, sin embargo, </rich_text>
      <rich_text family="monospace">&lt;em&gt;</rich_text>
      <rich_text> influye en:
Un pseudo-elemento se puede utilizar para:
Es la manera mediante la cual los navegadores deciden qué valores de una propiedad de CSS son más relevantes para un elemento:
La abreviaciÃ³n de grid-column-start: 2; y grid-column-end: 5; es:
Con flexbox (con el valor por defecto de flex-direction) para centrar un elemento de manera horizontal debo usar:
¿Cuál de los siguientes </rich_text>
      <rich_text weight="heavy">NO</rich_text>
      <rich_text> es un lector de pantalla?
</rich_text>
    </node>
    <node name="Curso practico JS" unique_id="28" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626820687" ts_lastsave="1626820687"/>
    <node name="closures y scope" unique_id="29" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1628030397" ts_lastsave="1628030400">
      <rich_text>el scope es el alcance que tiene la variable dentro del codigo
- local
- global</rich_text>
    </node>
    <node name="node1" unique_id="30" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630594410" ts_lastsave="1632930112">
      <rich_text>¿Cuál es el comando para listar los paquetes y módulos instalados?
npm list -g --depth 0
¿Cuál es el comando que nos permite ver todo el output en la terminal/consola?
npm run build --dd
¿Cuál es el comando que nos permite ver una auditoría en formato json?
npm audit --json


# EventLoop
- bucle que se ejecta todo el tiempo
- todo funciona de forma asincrona
- se ejecuta aparte del eventloop
- se evian los eventos a traves del event queue
- thread pool si no se puede ejecutar al momento... por ejemplo consultas a bases de datos
- cada thread pool levanta un hilo por cada peticion
- problemas de seguridad

Para leer un archivo de manera síncrona, mediante el módulo fs hacemos uso de
Para crear un servidor en Node.js es necesario ejecutar el método del módulo http:
Cuando hacemos tests para nuestros servicios lo importante es probar los llamados de las librerĂ­as u otros servicios.
La manera como creamos una consola personalizada es mediante la instanciaciĂ³n de:
¿Cuál es una característica de Express?
¿Cuál de los siguientes es un verbo HTTP?
Para retornar una respuesta con cache es necesario establecer el header:
La clase EventEmitter se obtiene del módulo:
Los métodos mas populares de un Readable stream son:
</rich_text>
    </node>
  </node>
  <node name="Juegos" unique_id="31" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625265105" ts_lastsave="1625265105"/>
  <node name="English" unique_id="32" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624747640" ts_lastsave="1664132487">
    <node name="Descriptions" unique_id="33" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624747655" ts_lastsave="1625432347">
      <rich_text>Discovery learning
	fill in the gaps
	Guessing
	Match columns
	Organize words
complete sentences
play games
use pen and paper
play and pause is very important
interactive discution panel (Q&amp;A)
uses of like
places
weather
peaple
character and personality
describe pictures
using descriptive language to compare

What is it like? (como es, description, forma de ser) != what does your best friend like? (comments, interests)
what does your best friend like? (comments, interests)
Intelligent, friendly, 
what does you best friend look like? (description of physical aparience)
############
1-b
2-a
3-d
4-c
############
Class 2 ex 2
1-d
2-c
3-b
4-a
############

# The weather
good, great, nice, fine, lovely, beautiful, wonderful, excellent, gorgeous, fair, pleasant, balmy;
bad, awful, terrible, nasty, lousy, foul, rotten, miserable, unpleasant, dull, gloomy, ugly;
sunny, warm, hot, mild, cool, chilly, cold, freezing, icy, frosty; very cold; bitter cold;
rainy, wet, humid, dry, arid, frigid, foggy, windy, stormy, breezy, windless, calm, still;
a spell of good weather; a two-day spell of sunny weather; a spell of rainy weather;
Sky: cloudy, overcast, cloudless, clear, bright, blue, gray (BrE grey), dark; a patch of blue sky.

rainy - lluvioso
foogy - niebla
cloudy - cielo nublado
warm - caluroso

class 3 ex 1
caluroso
1-warm
2-boiling
3-mild
4-
frio
1-cool
2-freezing
3-chilly
4-icy
--
1-windy
2-breezy
3-cool
4-icy

places:
bright
dark
big
small
Noise - ruidoso
Quiet - silencioso

more advance
crowded: (of a space) full of people, leaving little or no room for movement; packed
expensive
famous
fascinating
lively - full of life and energy; active and outgoing
spectacular

ancient
boring
charming - special place
exciting
dangerous
awesomw

Describir el clima de la ciudad
Describir tu hogar

</rich_text>
    </node>
    <node name="reto" unique_id="34" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1627231811" ts_lastsave="1627232346">
      <rich_text>What is your favorite place? Describe it below in the comments!
In this moment my favorite place is my house, especially my “office”, it is a special room in my house where I have my computer, my controls, my chair, one large desktop in L and other things. It is a medium room, whit 3 windows, one door, it very bright and quiet, the weather is little warm, this is caused by the computer when is worked.



bright
dark
big
small
Noise - ruidoso
Quiet - silencioso</rich_text>
    </node>
    <node name="reto" unique_id="35" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1627536290" ts_lastsave="1627539650">
      <rich_text>• Wake up (Despertarse)
• Get up (Levantarse)
• Take a shower ( tomar una ducha)
• Check email (Revisar email)
• Have breakfast (Desayunar)
• Go to work ( ir al trabajo)
• Have lunch (Almorzar)
• Have coffee (Tomar un cafe)
• Drive ( Conducir)
• Have dinner (Cenar)
• Watch TV (Mirar Television)
• Chat (Platicar o charlar)
• Go to bed ( ir a la cama)
• Sleep (dormir)

describe daily routine
I wake up at 7:30 am, I get up at 8:00 am, I have breakfast at 9:00 am, usually I study from 8:00 am to 10:30 am, I take a meet at 10:30 am, I have lunch at 11:00 am, I work from 12:00 pm to 6:00 pm and from 6:00 pm to 7:00 pm I study basic courses of programming or I review some themes of libraries, I have another lunch at 5:00pm, I take shower at 7:00 pm, I have dinner at 9:00 pm and I go to bed at 12:00 am

describe family
We are three in my family, my son, my wife and I, my son is a little boy, his name is Leo, he has various play doh of color blue, green, yellow and white, my wife is student of gastronomy, her name is Stephani, I'm engineer, I'm work in my house and I have one computer of color grey</rich_text>
    </node>
    <node name="metas" unique_id="36" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629764315" ts_lastsave="1629764319">
      <rich_text>asimilar 10 palabras
anotar palabras nuevas
asimilar dos frases diarias
pensar 10 minutos en ingles
aplicar lecturas en ingles</rich_text>
    </node>
    <node name="JJ" unique_id="37" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631657919" ts_lastsave="1649720295">
      <rich_text>
In the morning
In the afternoon
In the evening
at night

In, On &amp; At
 I go to the park ON saturday AT ten IN THE MORNING
 I go to school ON monday AT nine IN THE MORNING
 
introducing yourself
"(Greeting)!
My name is  (Your name) .
My last name is spelled  (spell your last name) .
I am _____ years old. I am from (your country)
(Farewell)!"

how are you?

See you later, take care

bakery
hospital
airport
school
pharmacy

Prepositions of place
	- on - arriba
	- between - entre
	- behind - detras
	- in front of - delante de
	- next to - al lado de

What do you like? (hobbies)
- I like to ...
- in dislike use I don't like

Adverbs of frecuenct
- never - 0%
- rarely - 5%
- seldom - 10%
- occasionally - 30%
- sometimes - 50%
- often - 70%
- frequenly - 80%
- usually - 90%
- alway - 100%
- Noun + frequency + action, ex: I never drink coffe
- Exception: Sometimes I go to the beach
- hardly ever, casi nunca

tarea
- numeros
- spell de palabras que nos aventamos

----------------------------------------------------------------------------------
---
----------------------------------------------------------------------------------
Can you speak slowly please? *
Can you repeat please? *
nice to meet you * to *
It's a plaesure to meet you

I'm well | great | fine | sad | sad

how old are you?
How do you spell that?
I´m twenty-eight years old. I´m 28.

what do you do?
this, cerca (mio)
that, lejos (no mio)
theese, cerca (mio)
those, lejos (no mio)

I
You
We
They
He
She
It

My
your
his
her
Its
Their
Our


# When is your birthday?
- my birthday is in June
- my birthday is on June 21st

formats of dates is
- most countries: dd/mm/yyyy
- USA mm/dd/yy

Telling time
- o' clock
- quarter past
- prepositions past between 0 and half
- prepositions to between half and 0
- it's five past four
- it's quarter past four
- it's half past four
- it's twnety-five to five
- it's quarter to five
- it's five o'clock
- se usa para para cuartos de horas, para los pasados, para numero mas puntuales, it's nine four, it's nine ‘ou’ four

What time is your country?

Which - Cuál -option
Who - Quién
What - Qué -- tjhis is more general
When - Cuándo 
Where - Dónde -- 
Why - Por qué -- reason
How - Cómo
	old
	far
	long
	many -- quantities
	much -- cost of something
	
Prepositions
- beside
- behind
- under
- over

- in - more general use with countries/cities
- on - use with streets/avenues
- at - is more especific, fro example addresses

# Wants and Wishes
- i would like -&gt; i'd like, more formal
- I want, is more direct as boss be careful

# have or have got
- is very similar
- have, I have a car
- have got, is very more informal, I've got a car
- use with Illnesses
- for third person or it is used with has, for another is used have
- is used with do and does with question and negatives

# Can vs Can't
- is used to:
   → Ability
   → Permission
   → Requests
   → Possibility
- POSSESSION ('s)
   → when the word ends with s ex: *the dogs' plates are full* or *James's office is spacious*, the pronuntiotion is necesary
   → when exists two sustantives and only one 's the possession is of both
   → when exists more 's each one has a one personal possession
- there is and there are
   → something exists or not exists
   → there is for singular or incontable nouns
   → there are for plural
- subject and object pronouns
   → subjects as I, you, he, she..., used with person animal or thing that does the action
   → objects as me, you, her, it...., receives the action
- Likes dislikes and opinios
   → express feelings
      ⇒ like, love, enjoy to likes
      ⇒ don't like, hate, dislike to dislikes
      ⇒ think, consider or believe to express opinion
- present simple vs present continuos
   → things that are always true (present simple) or habits or repeated actions
   → action happening at the momento of speaking, use verb to be + ing

boot
but
bot
both

four aspects:
- Name
- Nationality
- Job/Occupation
- Age

Hi!, My name is Pedro, I'm Mexician, I'm SysAdmin, I'm 33 years old

introducing others:
- use of this: this is my friend Ana
- with multiple people is used these are

Simple present:
- two options:
   → Verb to be
      ⇒ caracteristics of people (am, is are), details
   → other verbs
      ⇒ use ‘s’, ‘es’, ‘ies (when ends with y)’ when is the third person
      ⇒ use of do in negative and questions

# Wh questions
- who - people
- where - places
- when - times
- what - things
- why - reasons
- which - options

Places:
- in - dentro
- on - sobre
- in front of - en frente de
- next to - al lado
- behind - detras
- under - debajo
- over - sobre

# have or has
- deppending of pronoun, (irregular verb), with thid persons is used has, with negative and question form is used have, changing only do -&gt; does

Irregular plural nouns
- book -&gt; books
- apple -&gt; apples
- man -&gt; men
- life -&gt; lives
- foot -&gt; feet
- hero -&gt; heroes
- child -&gt; children
- fish -&gt; fish  ?_?
- knife -&gt; knives

Imperative form
- use to: instructions, order or advices

And vs but:
- and, similar ideas
- but, show contrast

A vs An:
- this is similar, but the use is depending of first sound of new word

I got late -&gt; llegue tarde

may - permission not can
restroom =&gt; caoque tenga WC, bathroom =&gt; cuanto tiene regadera, bedroom =&gt; habitacion

as, like -&gt; check differences

# Descriptions and comparatives
What is it like?
- what is your best friend like? - this is for descriptions, this is a description in personality
- what does your best friend like? - this for likes as food or music, interest
- what does you best friend look like? - this for phisical aparience
- what is your bedroom like? - this is used with profesions and places

The weather

yesterdar, I was only documenting regarding this point, checar los “ando endo” del ingles
I'm Swiss knife
I am sleepy
verify to check
generate

-- new words to use with reflexive pronouns
- blame, culpar
- cur, cortar
- enjoy, disfrutar
- help, ayudar
- hurt, lastimar
- introduce, presentarse
- prepare, preparar
- teach, enseñar

this is possible change the mean of verb: Help </rich_text>
      <rich_text weight="heavy">yourself</rich_text>
      <rich_text> to some coffee.(Sírvete un café tú mismo.)

preposition of time
- at is used for a specific time
- exception:
   → at night, I go to sleep
   → at christmas, we eat a lot of food (for holydays)

Use of ing
- I like|love|hate + verb+ing

expressing intentions
- I want to + verb (intention)
   → I want to swim
- I don't want to... (negative)

How far, long and often?
- long for duration of time
- far is for distance (kilometers)
- often is for frequency (every 15 mins)

Common past verbs
- eat  -- ate
- give   -- gave
- write  -- wrote
- go  -- went
- see  -- saw
- feel  -- felt
- make  -- made
- do  -- did
- drink  -- drank
- know  -- knew
- fly  -- flew
- Another
   → put, cut, read and hit
- To be, feeling states and physical places:
   → was, i she and he
   → were, we they and you

Did and Didn't
- is an auxiliar verb in past, negatives and questions

Preposition
- in - dentro
- on - sobre de algo
- above - encima algo
- between - entre
- under (belove)- abajo
- next to - al lado
- near - cerca

- past = after
- to = before

Wh question:
- what, asking about things or information
- where, location or place
- why, reason of something
- when, for time
- which, two options or objects
- who. a person o people

# A2 - preguntas y respuestas
- Determiners singular and plural:
   → this, is for singular and close distances
   → that, is for singular and far distances
   → these, plural and near
   → those, plural and far
- Present continuos
   → at the moment
   → subject + verb-be = verb(ing)
- Contables an uncountables
   → contables
      ⇒ 
   → uncontables
      ⇒ sugar
      ⇒ beer
      ⇒ food
      ⇒ money
      ⇒ time
      ⇒ fruit
   → quantifiers
      ⇒ any: questions and negative
      ⇒ some: affirmative
- how much and how many:
   → much:
      ⇒ for uncontable nouns
      ⇒ price of something
      ⇒ with singular and plural nouns
   → many:
      ⇒ contable nouns
      ⇒ quantity of something
      ⇒ only with plural nouns
- At as preposition of time
   → for specific time
      ⇒ used with at night:
         • at night, i go to sleep
         • at christmas, we eat a lot of food
- references using ing
   → used with like, love and hate + verb-ing
   → or used with like, love and hate + to + verb (without ing)
- Expressing intentions
   → I want to + verb (intention)
      ⇒ I want to swim
      ⇒ I wanto to eat pizza
   → Negative: I don't want to...
- How long, far and often
   → How long
      ⇒ use for duration or time 
   → How far
      ⇒ is use for distances
   → How often
      ⇒ is for frequency
- especial words: “lift-and-shift”, “lift, tinker, and shift,”
- past verbs include was and were
   →  Eat, ate
   → give, gave
   → write, wrote
   → go, went
   → see, saw
   → feel, felt
   → make, made
   → do, did
   → drink, drank
   → know, knew
   → fly, flew
   → other interesting
      ⇒ put, cut, read and hit
      ⇒ was: I, she and he, it
      ⇒ were: we, they and you
- do and did
   → is an auxiliar verb, in negative form is necesary
   → with positive is posible use “I did go to the park” or “I went to the park”
- with possessives “estructure”: that|those is|are {person}'s {object}
- Preposition of place
   → In, dentro
   → on, sobre algo
   → above, encima de algo
   → between, entre
   → under o below, bajo
   → next to, al lado de, de formar cercana
   → near, al lado de de forma lejana
- Past == after
- to == before
- Wh questions:
   → What, asking about things or information
   → where, location or place
   → why, reason for something
   → when, for time
   → which, two options or objects
   → who, a person or people

Which question is correct?
Did you finish your homework?


# Basic connectors
- connects ideas
- and, is used by addition
- but, is used to contrast
- or, is used for multiple options

# Articles
- the, refer to specific, people, things or situations
   → specific people:
      ⇒ only one or unique person
      ⇒ particular person
      ⇒ groups of people
      ⇒ families
   → things:
      ⇒ musical instruments
      ⇒ only one in the place
      ⇒ particular place
      ⇒ famouse monuments, builds, museums
      ⇒ hotels, bars and restaurants
      ⇒ unique things
   → situations
   → ordinal numbers
   → decades
   → geografical areas
   → countries (plural names, republic, kingdom, states)
- Not use:
   → name of cities or countries
   → years
   → professions
   → people's names and titles combined wirh names
   → meals
   → languages

kick boxing
korn flakes

kill
killed
laidback - relajado

blue
megaman
switch
share button
sad
innocence
sky


park
cat
plump
father in law
cook
sister
pizza

I can dance, I can play guitar, I can sing


--------
</rich_text>
    </node>
    <node name="estrategias" unique_id="72" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649291591" ts_lastsave="1649448348">
      <rich_text>Evaluar las competencias es importante porque:
Son factores que determinan la importancia de las estrategias de estudio en el proceso de aprendizaje:

</rich_text>
    </node>
    <node name="hours and requests" unique_id="73" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649720590" ts_lastsave="1649805908">
      <rich_text>Basic connectors
- And, addition
- but, contrast, one option but another no
- or, options, use one or another

The, article
- people, only one person or particle (unique), group of people, families
- things, musical instruments, only one in the place, particular place, monuments, buildings, museums, hotels, bars, restaurants, unique things
- situations, ordinal numbers, decades, geographical areas, countries
- not use, name of cities and contries, years, professions, people's names and titles combined with names, meals, languages</rich_text>
    </node>
    <node name="Basi A1" unique_id="100" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656369145" ts_lastsave="1656619420">
      <rich_text>Introduction
- Who am I?
- Interesting facts
- in english alphabet has 26 letters
- A and An
   → refer to only this or noun
   → an is used as article with vowels
   → a is used with consonants
- It's sentences (it is)
   → it's + article (a/and) + (person, place, thing or item)
- Plural forms
   → mean many things
- there are
   → this is a plural form
   → there are + plural noun

Second module
- is very important from 1 to 20
- 100 - one hundred
- subject pronouns
   → I am
   → he/she/it is
   → they/we/you are

good morning
good afternoon
good evening</rich_text>
    </node>
    <node name="describing my house" unique_id="102" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656878011" ts_lastsave="1656984966">
      <rich_text>Home Sweet home
- Was, this ‘era’, be past, singular
- Were, plural of be past
- I didn't do that, irregualr verbs to past
   → only is used when is negative or question
- like doing, liked doing
- love doing, loving “playing” - verb in ing
- hate doing, hated

Describe my child home
things that I did in my child home and I didn't
Actions that I like, love or hate doing

What are we doing here?
- this is in the moment
- verbs with be + verb(ing)
- right on time:
   → present simple
   → present continues
   → at - for time
- count on me
   → how many, contable nouns
   → how much, uncontable nouns
- we have com this far
   → How long, time to execute a task
   → how often, time between tasks
   → how far, distance
- things in my house
   → prepositions:
      ⇒ on
      ⇒ in
      ⇒ next to
      ⇒ in front of
      ⇒ behind

My dream house
- whose things are those in your dream house?
- use of whose, de quien es
- linking verbs
   → how to describe your dream house, using adjectives</rich_text>
    </node>
    <node name="Articles and modal verbs" unique_id="104" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1657152267" ts_lastsave="1663367835">
      <rich_text>Basic connectors:
- connects two ideas
- And, is for additions
- But, contrast
- or, show options

article THE
- refer to specific
   → people
      ⇒ only one person
      ⇒ particular person
      ⇒ groups of people
      ⇒ families
   → things
      ⇒ musical instruments
      ⇒ only one in the place
      ⇒ particular place
      ⇒ famous monuments, buildings, museums, etc
      ⇒ hotels, bars and restaurant
      ⇒ unique things
   → situations
      ⇒ ordinal numbers
      ⇒ decades
      ⇒ gegraphical areas, groups of rivers, mountains ranges, oseans
      ⇒ Countries, plural names
- not use:
   → name of cities and countries
   → years
   → professions
   → people`s names and titles combined with names
   → meals
   → languages

Weather coonditions
- it's
   → it's + condition (inpersonal subject)
   → use with distance or time
   → ex
      ⇒ It's sunny
      ⇒ it's cloudy
      ⇒ it's rainy
      ⇒ it's windy

Not Here/There
- adverbs of place
   → here, speaker point of view
   → there, listener point of view

Frequency
- forming question
   → how often + do/does + subject + verb?
- Adverbs of frequency
   → - never - 0%
   → rarely - 5%, hardly ever
   → seldom - 10%
   → occasionally - 30%
   → sometimes - 50%
   → often - 70%
   → frequenly - 80%
   → usually - 90%
   → alway - 100%
- frequency
   → time expresions
   → adverb of frequency
      ⇒ before the main verb
      ⇒ after the verb be

Permission
- Can and Can't, permissions and denied permission
   → ask, structure Can + subject + action?
   → give, subject + can + action
   → deny, subject + can't + action
- Offer, Request, suggestion
   → is possible use can to offers, ex: Can I help you?
   → request: can we finish this tomorrow?
   → suggestions: can you tell her you're sorry?
- decline offers and invitations
   → use can't
      ⇒ thanks, but I can't have any sugar
   → invites:
      ⇒ Sorry. We can't be there tomorrow

Making requests
- would like
   → is used to offer or requests:
      ⇒ offer, Would you like some coffee?
      ⇒ requests: I would like a beer, please, contraction: I'd like a beer
- Could
   → is used to making requests
      ⇒ is more polite, is more formal
      ⇒ could we finish this tomorrow?

Time
- What time is it?
   → learn the time in english
   → past, present, future
   → numbers: It's seven o'clock
   → past/to: it's ten to two
   → divide the clock in two parts, from 0 to 6 is past/after minutes (word) hour, and 6 to 12 is “to” minutes left to next hour
   → a quarter in 15 and 45, half in 30
   → it's four oh five
   → it's a quarter past six
   → it's half past two
   → it's a quartet to twelve
- time expressions
   → past, yesterday, last
   → present, today, 
   → future, tomorrow, later, next, this
- prepossitions
   → at, used in precise time, hours, dates for example holydays, mealtimes, specific times ex: at bedtime
   → on, days and dates, , on april x or on saturday, parts of specific days, special dates ex: on my birthday

How to do things
- how to create, execute things
- ex:
   → how do you open this?
   → how do you say that in spanish?
   → how do you spell your name?</rich_text>
    </node>
    <node name="elementos y expresiones de trabajo" unique_id="112" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664132487" ts_lastsave="1664132487"/>
  </node>
  <node name="New Relic" unique_id="105" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1658543494" ts_lastsave="1658543502">
    <node name="Observabilidad" unique_id="106" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1658543502" ts_lastsave="1658873170">
      <rich_text>Introduccion
- como se pueden recolectar datos, trackear apps, visualizar la info y analizarla, para comprender un sistema digital complejo
- hacer que los sistemas sean mas observables
- mejor experiencia de usuario
- no afectacion de la complejidad del desarrollo
- la observabilidad es una habilidad muy necesaria para el ciclo de vida de una app
- las companias de hoy en dia deberian de usar observabilidad para saber el estado de sus apps, sitios web mas robustos, no se es inmune a bug
- desarrollo no a prueba de fallos sino con rapidez para encontrar y arreglar las fallas
- implementacion de observabilidad en un proyecto ya hecho

Como se pasa de paginas web a apps web?
- antes teniamos paginas estaticas
- manejo de cliente-servidor, el flujo simple que se ha usado hasta el momento
- antes teniamos monolitos y se usaba metodologia waterfall
- hoy se tienen mucho deploys y muchos stack tecnologicos
- en velocidad de negocio si no se tienen varios despliegues quiere decir que se esta llendo lento
- se tienen ya varios lenguajes y tecnologias para el desarrollo del negocio, dependiendo del problema se busca la tecnologia que sea util
- desacoplamiento de la complejidad de solo tener un servidor potente, tener varias apps que viven en la nube
- arquitectura de microservicios
- uso y manejo de CI/CD
- cada microservicio debe de usarse para una sola tarea especifica
- todo debe de trabajar de modo orquestada

Contenedores y alojamiento en la nube
- un contenedor es donde se guardan los microservicios
- se tiene todo lo necesario para que se ejcute el codigo
- tener muchos contenedores discretos se conoce como contenerization
- un tiempo atras se guardaban las apps en servidores, que estaban en la misma oficina
- se tienen contenedores que se activan y mueren dependiendo que se requiera, estos viven en la nube
- se vuelve algo complicado monitorear todos los contenedores
- la observabilidad, que esta pasando en la app

Cloud native
- son las app y sistemas que han sido desarrolladas en los tiempos del computo de la nube
- no existe algo como tal que diga que es cloud native
- se repiten algunos elementos que los hacen cloud native

Monitoreo de un sistema
- como saber la salud de los contenedores y K8s
- recolectar datos sobre los sistemas y el rendimiento, monitoreo
- el monitoreo mas comun es el APM, application performance monitoring
- se monitorean puntos de datos, por ejemplo que tanto computo se esta usando, cuanto tiempo de carga se tiene en las paginas
- el monitore se usan estos 4 pasos:
   → plan, se decide que forma de anticipada que debe ser monitoreado
   → intrument, se crean de tal manera que se recolecte la informacion
   → observe, se observan los datos, de lo que se intrumento
   → detect and resolve, cuando el problema se detecta la persona encargada lo dice y se toman acciones
- nucleo
   → nuestro servicio esta en linea y disponible?
   → esta funcionando correctamente?
   → esta funcionando bien?
- Mean time-to-detection (MTTD), este debe de ser el mas bajo, tiempo en que empieza el problema y el equipo lo detecto
- Mean Time-to-resolution (MTTR), tiempo que se tardo uno en resolver el problema que se detecto

Tipos de herramientas y limitaciones del monitoreo
- software as a service
   → se vende en un modelo de suscripcion
   → herramientas para recolectar, almacenar, queries, visualizar, identificar problemas, resolver probelmas
   → son faciles de implementar
   → de acuerdo al negocio
- open source software
   → se especializan en algo especifico
   → se pueden pagar por soporte
- limitaciones:
   → cuando se tienen muchas herramientas que se vuelven un caos, muchisimas herramientas, esto es por que monitorean diferente tipos de targets
   → no conocer donde puede fallar nuestra aplicacion, se decide que se va a monitoreas, hoy son muy complejas las apps

Que es observabilidad
- es la mejora del monitoreo
- dice que tan bien se puede entender el sistema complejo
- instrumentar los sistemas, para datos accionables, te dice por que ocurren los problemas
- es end to end

Pilares de la observabilidad
- Intrumentacion abierta, open instrumentation, los equipos recolectan la telemetria o datos y ponerlos en una locacion central, nos permite medir el rendimiento, se evita tener tantas herramientas para monitorear
- entidades conectadas, connected entities, todos los datos recolectados se conectan de forma organizada, relaciones entre un sistema y sus muchas entidades y relaciones claves del negocio, clarificar que data se tiene y de donde viene, crear significado y contexto, mapea y oraganiza los datos de manera que tengan sentido
- Correlacion y contexto, correlation &amp; context, usar los datos recolectados para tener tablas custom que nos sirve para metricas y objetivos

MELT, mirada de cerca a traces
- 4 tipos de telemetria:
   → M, metrics, usar data de los eventos, sirve para calcular metricas, nos ayudan a hacer preguntas elementares para el negocio, conforme el negocio crece se debe de saber como esta creciendo para saber si vamos por buen camino, sirven cuando se tienen grandes cantidades de datos y sabes que preguntas se requieren
   → E, events, accion distintiva en algun momento del tiempo, generalmente estan en tablas como metadata, se pueden tener n cantidad de tipos de eventos, pocos datos y aun no sabemos que preguntas necesitamos ahcer
   → L, logs, estos van mas granulares, son similares a los eventos, son discretos y especificos de una app, son mas grandes que los eventos y mas especificos, explica cada paso del evento, casi todo en un sistema moderno emite logs, vista detallada
   → T, traces, distributed traces, se llevan a cabo de acuerdo a multiples componentes del sistema, captura la ruta de transaccion que viaja en el sistema distribuido
- si se compara metricas vs eventos, los eventos son irregulares por que no sabemos cuando van a pasar, las metricas sabemos que pueden pasar por x periodo de tiempo, aunque no se tenga en x tiempo se toma por valor 0

Anatomia de una query
- event data types
   → browser
   → APM mobile
   → Synthetics Infraestructure
   → Page view MobileRequest
   → Transaction SystemSample
   → SyntheticCheck
- se ha usado Browser y Page view hasta este momento
- NRQL
   → SELECT function(attribute) FROM EventType (PageView, Transaction, SyntheticCheck, MobileRequest, SystemSample)
   → mas complejo con WHERE {condition} FACET {gruoupby}
   → se puede dar un timeframe
- Query de datos de consulta
   → cuando no se sabe que elementos se deben de consultar
   → dos herramientas
      ⇒ uso de las docs, se puede usar el API de newrelic
      ⇒ data explorer, no se require manualmente construir una query
- Queries avanzadas
   → agregacion de datos, average, max, min,count, uniqueCount
   → intervalos de tiempo
      ⇒ timeseries, ayuda a generar una serie de tiempo, historial respecto al tiempo
      ⇒ since, desde, complementacion de las queries, el tiempo que se requiere
      ⇒ until, se puede tener un periodo unico, desde-hasta
      ⇒ compare with, se toman dos periodos de tiempo y se comparan
   → Multi Facet
      ⇒ agrupacion por varios elementos

Creacion de alertas y buenas practicas
- prevencion de slo dentro del sistema, con alertas proactivas

Synthetics
- para empezar con la observabilidad
- si llega a fallar algo o no se complementa uno de los flujos

Alertas avanzadas
- son un tema muy importanteen observabilidad
- las alertas las administramos con las politicas, es un contenedor para una o mas alertas se compone de dos cosas
   → condiciones, lo que se quiere notificar o sobre que
   → canales de notificacion, por que medio se va a hacer la notificacion
- una politica es una coleccion de algo sobre lo que se quiere ser notificado, se puede notificar por email, slack, webhook, app movil
- preferencias de incidentes: nos determinan que tan seguido un incidente se toma en cuenta en una politica, que tan seguido se es notificado, se configuran en by oplicy
   → si se tienen dos condiciones que no son similares entre ellas, se usa by condition incident, la notificacion se envia si cualquiera de las condiciones se cumple
- by condition and signal, correo por cada violacion que ocurre, es sobrecargada
- se debe ser cuidado de como se configuran las alertas y de como se van a usar

Baseline Thresholds, umbrales de referencia
- statics, barreras o limites estaticas, esta es la mas simple, se usa un numero limite
- dynamics, si la app no es tan simple, se esta esperando que en ciertas horas o dias no se tenga tanta demanda, este se adapta a la realidad del producto, se fija en el historico de las metricas y cuando halla un cambio avisa, aqui no se pone un valor especifico para la condicion
- existen thresholds:
   → criticos, estos se deben resolver de forma inmediata
   → warning, puede dar lugar a una infraccion critica, no critico para el negocio, no es inmediato pero se debe tomar en cuenta

Tipos sinteticos
- proactivo, encuentra problemas antes de que los usuarios los vean
- consistente y predecible, bueno para alertar
- ademas de monitorear esto, las apps tiene usuario reales, estos datos se llaman RUM (Real User Monitoring), se analizan con los sinteticos para tener una mejor forma de asegurar la plataforma
- se juntan datos y se visualizan junsto con los real user, se debn combinar ambas cosas
- tipos
   → pring, checa disponibilidad del sitio web
   → simple browser monitor, monitorea el rendiento de carga de la pagina
   → scripted browser monitor, flujos completos
   → API monitor, asegura que las API clave estan operativas

Rendimiento frontend y core vitals
- el rendimiento en frontend se sigue:
   → availability, es accesible
   → functionality, esta funcionando bien, pueden resolver las tareas criticas
   → performance, esta funcionando lo suficientemente rapido
- core web vitals, es un standard
   → LCP, larguest Contentful Paint
   → FID, First Input Delay
   → CLS, Cumulative Layout Shift, pasa cuando carga una cosa antes de terminar otra accion y se da donde no quiere

Apdex, application performance index
- se mide el rendimiento
- una tabla por defecto se crea
- medicion de sentimientos del usuario
- la medicion es entre 0 y 1, 0 no se tiene usuarios, los usuarios estan frustados con el software
- experiencia de usuario o app como esta funcionando
- apdex = (satisfied request + (Tolerating request / 2)) / Total number of requests</rich_text>
    </node>
  </node>
  <node name="Teoricos" unique_id="56" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641243242" ts_lastsave="1641243251">
    <node name="Arquitectura backend" unique_id="57" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641243251" ts_lastsave="1641243649">
      <rich_text>- Que es backend
   → es el sofware que se ejecuta en el servidor
- sistemas distribuidos
   → </rich_text>
    </node>
  </node>
  <node name="Matematicas" unique_id="38" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617840054" ts_lastsave="1657140372">
    <node name="calculo" unique_id="39" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617840076" ts_lastsave="1617842337">
      <rich_text>Funcion
- es una relación entre dos conjuntos a traves de la cual cada elemento le corresponde un unico elemento ninguno del otro conjunto
- hay variables dependientes e independientes

Dominio
- al conjunto de partida se le llama dominio - X

Contradominio
- al conjunto de llegada contradominio - y

Representación gráfica
- debe poseer pares ordenados X, y sobre R2

Que es una derivada?
</rich_text>
    </node>
    <node name="estadistica" unique_id="103" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1657140372" ts_lastsave="1663530127">
      <rich_text>base de la estaditica
- que es una muestra?
   → subconjunto de algo mas grande (poblacion)
   → tiene que ser representativa
- variable aleatoria
   → es una caracteristica de la poblacion
   → siguen un determinado comportamiento llamado distribucion
   → dos tipos
      ⇒ discreta, enteros, contables, opciones finitas
      ⇒ continua, valores reales en un intervalo, dificilmente repetidos

Los tres analisis descriptivos fundamentales
- estadistica descriptiva, describe los datos que se tiene a mano
   → resumir analizar y sacar conclusiones
   → se divide en
      ⇒ tablas de frecuencia
         •  se tienen las clases
         • frecuencia absoluta, total por clase
         • frecuencia absoluta acumulativa, sumar una tras la otra
         • frecuencia relativa, la frecuencia absoluta / total (porcentaje)
         • frecuencia relativa acumulada, frecuencia absoluta acumulativa / total
      ⇒ graficos
         • torta o pastel
         • histograma, se puede ver la distribucion de la variable
      ⇒ resumenes numericos
         • medidas de centralizacion
            ◇ media, promedio
            ◇ mediana, el numero que se tiene en medio
            ◇ moda, el que mas se repite
         • medidas de dispersion, cuan alejados estan los datos de la media
            ◇ varianza, s^2=(Sum_i=1~n(x_i-X)^2)/n, ~=hasta
            ◇ desviacion tipica, raiz cuadrada de la varianza

Datos agrupados con frecuencias
- formas distintas de repesentar y analizar la informacion
   → datos no agrupados, tal y cual vienen en su forma original
   → los agrupados se tienen los valores o intervalos de valores que tomo la variable y la frecuencia de ocurrencia
- Valores con frecuencia
   → es mas facil calcular la media, la mediana y la moda, se tiene que multiplicar el numero de la frecuencia por el valor
- valores por intervalos
   → se puede crear una columna con marca de clase, suele ser el punto intermedio (promedio)
   → columna de cada marca de clase por el total de frecuencias
   → la mediana es:
      ⇒ L_i + (((n/2) - N_(i-1))/ n_i) t_i
         • L_i extrem inferior del intervalo
         • n/2 la mitad del total
         • N_(i-1) frecuencia absoluta acumulada de la clase anterior
         • n_i frecuencia absoluta de la calse de la mediana
         • t_i tamanio del intervalo
   → Moda, el intervalo modal, donde se tiene mayor frecuencia absoluta
      ⇒ M = L_i + ((n_i - n_(i-1)) / ((n_i - n_(i-1)) + (n_i - n_(i+1))))

Cuantiles, Deciles y Percentiles
- las medidas de posicion son los cuantiles y se clasifican en 
   → cuartiles, dividos en 4 grupos
      ⇒ se ordenan los datos de mayor a menor, primero el minimo y el ultimo el maximo
      ⇒ se determina la posicion que ocupa cada cuartil
      ⇒ el segundo cuartil coincide con la mediana de los datos
      ⇒ estos se pueden calcular de diferentes formas
      ⇒ se basa en la mediana, ya que esta parte a los datos en dos, en cada grupo que queda se pueden tomar la mediana y asi obtener los demas cuartiles
      ⇒ si es un numero impar de variables se puede o no incluir la mediana, generalmente no se incluye
      ⇒ formula = Q_k = (K * (n + 1))  / 4
   → quintiles, en 5 partes
   → Deciles, en las 10 partes
   → percentiles, divididos en 100 partes
      ⇒ formula = p * n / 100, donde p = percentil, n = numero de valores
- los datos deben d estar ordenados

Diagramas de cajas y bigotes (box and wisjers plot)
- se usan los cuartiles
- la caja que se ve nos muestra el primer, segun y tercer cuartile, los bigotes son el minimo y el maximo, formado de primer y tercer cuartil
- la longitud de la caja se llama rango intercuartil  Ri = Q_3 - Q_1
- valores atipicos, cuando hay valores muhco muy grande o muy pequenios, (outliers), son un punto o cuadrado pequenio en el grafico
- para un valor atipico tiene que ser mayor a Q_3 + 1.5 * RI veces al rango intercuartil o menor a Q_1 - 1.5 * RI, se les puede llamar barreras

Asimetria:
- para cumplir simetria se tiene que la media, mediana y moda son iguales
- la negativa es cuando la media va a estar por debajo de la media y asi de la moda
- la positiva es cuando la media esta arriba de la medianada y esta de la moda
- se puede calcular:
   → Karl Pearson: A_s = (3(media - mediana)) / desviacion tipica, este varia entre -3 y 3, si es negativo es asimetrica negativa, 0 es simetrica, mayor que sera es simetria positivo
   → Yule Bowley o medida cuartilica: A_s = (Q_1 + Q_3 - 2*Q_2) / Q_3 - Q_1 varia entre -1 y 1 y la regla aplica igual que con pearson
   → Medida de Fisher (datos sin agrupar):  A_s = Sum_i^n(x_i - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica)
      ⇒ con datos agrupados con frecuencias: A_s = Sum_i^n n_i*(x_i - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica)
      ⇒ con datos agrupados por intervalos: A_s = Sum_i^n n_i*(x_mi - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica, m_i marca de clase, n_i frecuencia por cada clase)</rich_text>
    </node>
  </node>
  <node name="Nuevos lenguajes" unique_id="40" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617842349" ts_lastsave="1619532722">
    <node name="Algoritmos y pensamiento logico" unique_id="41" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617842365" ts_lastsave="1641884633">
      <rich_text>en python con () se hace un generator expresion que es como una lista pero mas potente, ocupa menos memoria

Un mÃ³dulo es un conjunto de paquetes
¿Qué módulo se utiliza para verificar los errores de tipado en un proyecto escrito en Python?
¿Cuál de las siguientes condiciones no es necesaria para encontrar un closure?
¿Qué es un decorador?
¿Cuántas veces puede iterarse un generador?
</rich_text>
    </node>
    <node name="python" unique_id="63" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642641418" ts_lastsave="1646234786">
      <rich_text>Starlette permite:
¿Qué es OpenAPI?
La Ășltima parte de una URL, despuĂ©s del primer "/", recibe el nombre de:
¿Con cuál de las siguientes operations no deberías enviar jamás un request body?
</rich_text>
      <node name="selenium" unique_id="64" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642694106" ts_lastsave="1643388034">
        <rich_text>- que es selenium
- cominucar con el navegador web
- automatizar pruebas unitarias y funcionales
- generar reportes de pruebas

- historia de selenium
   → suite de herramientas para automatizacion de navegadores
   → es compatible con varios lenguajes
   → no es una herramienta de testing ni de web scraping
   → es para curar el envenenamiento por mercurio (competencia de seleium en su momento)
   → pros
      ⇒ excelente para iniciar
      ⇒ no requiere saber programar
      ⇒ exporta scripts para slm RC y webdriver
      ⇒ genera reportes
      ⇒ soporte para varias platofrmas
      ⇒ operaciones logicas y condicionales
      ⇒ ddt
      ⇒ posee un api madura
   → contras
      ⇒ es mas complejo de instalar
      ⇒ necesita de un servidor corriendo
      ⇒ comandos redundantes en su api
      ⇒ navegacion no tan realista
   → selenium webdriver
      ⇒ soporta multiples lenguajes
      ⇒ facil de instalar
      ⇒ comunicacion directa con el navegador
      ⇒ interaccion mas realista
      ⇒ no soporta nuevos navegadores de forma rapida
      ⇒ no genera reportes o resultados
      ⇒ requiere saber programar
   → selenium grid
   → se utiliza junto a slenium rc
   → permite correr pruebas en paralelo
   → conveniente para ahorra tiempo

ERRORES
¿Qué lenguaje no es soportado oficialmente con Selenium?
java, c#, kotlin, perl, php, python, ruby, js
¿Qué assertion te permite validar el que el título del sitio web es el siguiente?
equals
¿Qué es y para qué nos sirven las test suites?
coleccion de pruebas unificadas en un solo archivo
¿Con qué me permite interactuar la clase WebDriver de Selenium?
ventana del navegador y sus elementos relacionados, como pop ups o alerts
¿Con qué me permite interactuar la clase WebElement de Selenium?
elementos de los sitios web
¿Qué acciones podemos utilizar para interactuar con un alert de JavaScript?
switch_to_alert, accept
Son todos mÃ©todos para automatizar la navegaciÃ³n:
back, forward, refresh
¿Qué hace el siguiente código?
implicita espera a que este el dom si lo encuentra continua
explicita utiliza condiciones de espera, continua hasta que la ecnuetnra
¿Por qué debemos utilizar la menor cantidad de esperas implícitas posibles?

¿Qué es una expected condition (condición esperada)?
¿Cuándo es conveniente utilizar try y except en nuestra prueba?
¿Por qué no debería automatizar o hacer testing en sitios que explícitamente lo prohíben?
bloqueo</rich_text>
      </node>
    </node>
  </node>
  <node name="Go" unique_id="42" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619532722" ts_lastsave="1663354176">
    <node name="Arquitectura de software" unique_id="43" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619532733" ts_lastsave="1619638128">
      <rich_text>Que es?
Estructuras, modelos con daigramas, comunicacion entre diferentes modulos del sistema
Entender el rol del arquitecto

# Etapas del proceso
- Analisis de requerimientos, nace de una idea o problema, aqui se entiende que se va a construir, requerimientos de negocio, usuario, funcionales y no funcionales
- Diseño de la solucion, Analisis profundo de los problemas, ya se tiene una propuesta de esas soluciones, aqui empieza el arquitecto
- Desarrollo y evaluacion, programacion y pruebas, criterios de aceptacion, set necesario para construirla
- Despliegue, infraestructura y roles de operacion para que este disponible
- Mantenimiento y evolucion, Arreglo de errores y agregado de nuevas funcionalidades

# Dificultades en el desarrollo
- Esenciales, Entender el diseño y concepto
   → complejidad, que tan dificil se vuelve, por ejemplo el calculo de rutas
   → conformidad, el contexto que se va a usar y como adaptarlo, por ejemplo la efectividad del software
   → tolerancia al cambio, se puede cambiar o ya no, que tanto cambia el problema que se resolvio
   → invisibilidad, se vuelve dificil al no ser tangible
- Accidentales, tecnologia y plaforma que se usara, conexiones entre tecnologias o resoluciones en esa tecnologia
   → Lenguajes de alto nivel, elmismo lenguahe se vuelve una dificultad
   → multiprocesamiento, el poder hacer mas de una tarea en la computadora
   → Entornos de programación, algo asi como el que nos ayude con las propiedades que tengan clases o funciones del lenguaje
como se resuelve?? No desarrollar, comprar OSS, Prototipado rapido, Desarrollo evolutivo, Grandes diseñadores (arquitectos)

# Roles
- Experto del dominio, es elq ue sabia que se requeria del dominio, en las nuevas metodologias son las partes interesadas
- Analista, es el que indaga en que se debe de resolver, este es el cliente/dueño del producto, en las nuevas metodologias
- Admininistrador de sistemas, los viejos sysadmin, hoy DevOps/SRE
- Equipo de desarrollo, los qa, desarrollador, arquitecto
- gestor del proyeco, se encarga de las entregas y que se cumpla con el plan, facilitador

# Que es la arquitectura de software
- modelos y diagramas con conexiones entre cajas
- este es muy sesgado, pero se deben de hacer analisis profundos sobre que es lo que hay que contruir, y como es que el sistema los va a resolver
- la arquitectura es algo estructural, por medio de objetos ocultando propiedades, es un conjunto de decisiones para el diseño del sistema
- la arquitectura se reduce a cuaqluier cosa importante

# La importancia de la comunicación (ley de conway)
- la comunicación es fundamental para la arquitectura del software
- la comunicación dara estructura

# Objetivos del arquitecto
- tiene varias partes interesadas:
   → cliente, quiere un sistema en presupuesto y a tiempo
   → desarrollador, facil de implementar y mantener
   → manager, desarrollar software de forma independiente, asi como lo mismo que el usuario
   → qa, facil de probar
   → usuario, debe de ser disponible y confiable

# Arquitectura y metodologías
- nace en metodologias tradicionales, para encontrar problemas y solucionar a gran escala, las agiles emerge de un equipo autogestionado
- En la tradicional se toman la definicion, restricciones, requerimientos, riesgos, le falta feedback, este se viene hasta que se termina la solución
- en metodologia agil, se puede planear en el planteamiento del sprint, se debe de poder medir, se pueden hacer esqueletos de solución
- lo mas importante para ser agiles es el feedback

# Entender el problema
- Separar la comprension del problema de la propuesta
- Espacio del problema, idea, criterios de exito, historias de usuario
- Espacio de la solucion, diseño, desarrollo, evaluacion, criterios de aceptacion, despliegue
- se narran historias para llegar a eso
- es la limitante del problema

# Requerimientos
- despues de entender el problema
- de producto, que es lo que necesita
   → negocio, consta de reglas de negocio
   → usuario, como el usuario usa el sistema, atributos de calidad, garantia de que tenga calidad y es enfatizado
   → funcionales, se alimentan de todo, para saber que se debe de hacer especificamente, tambien tienen requerimientos de sistemas, son afectados por las restricciones
- de proyecto, no tiene que ver con la arquitectura
   → recursos
   → capacitacion
   → certificaciones
   → documentacion de usuario
   → infraestructura
   → licencias
   → plan de despliegue
   → plan de transicion
   → acuerdos de servicio
- funcionales, como se va a comportar el sistema
- no funciones, tiene que ver mas con calidad, son parte de los funcionales, siempre los vinculaban con la arquitectura
- requerimientos significativos, afectan el diseño la arquitectura

# Riesgos
- para describir, usar escenarios de fracaso que sean medibles y accionables
- de ingenieria, se mitiga a traves de diseño e implementacion
- de gestion de proyecto, relacinados mas con la planeacion
- se identifican con la toma de requerimientos (difcultad y complejidad), atributos de calidad (insertidumbre) y conocimiento del dominio (riesgo prototipico)
- Se priorizan riesgos y se solucionan los mas criticos, no se pueden solucionar todos

# Restricciones
- limita las opciones de diseño o implementacion
- las partes interesadas, integraciones con otros sistemas, ciclo de vida del producto

# Arquitectura, panorama y definición
- se debe de saber que pasa en el software
- verificacion de sacrificios y beneficios
- un estilo de arquitectura es algo generico, resolución a nivel de conectores
- es una coleccion de decisiones de diseño

# Llamada y retorno
- hacen invocaciones a otros componentes y espera una respuesta
- programa principal y subrutinas, estilo C
- OOP, para aplicaciones que se van a mantener mucho tiempo
- Multinivel, se tienen diferentes niveles que se van a comunicar por nivel

# Flujo de datos
- Lote secuencial, lo importante es ejecutar una pieza de codigo y cuando termine pase a otra etapa
- Tubos y filtros, es un streaming

# Centrada en datos
- Pizarron, se tienen diferentes componentes que interractuan con un componente principal, cada componente recibe y procesa los datos y envia al pizarron, también el pizarron puede tener procesamiento y dar una salida
- Centrada en base de datos, generalmente es usado con una base de datos y los componentes comparten esa base de datos, los componentes no se comunican entre si, la base es el puente
- Sistema experto, estilo basado en reglas, un cliente se comunica con otro que infiere las reglas o consultas, este se comunica con un tercero que es una base de datos de conocimiento

# Componentes independientes
- Invocación implicita, basada en eventos, aplicaciones que se mandan mensajes entre si, sin que sepan quien invoca a aquien, se usa un bus para comunicarse
- Invocación explicita, aqui se sabe que se invoca, pero no son dependientes uno del otro, su comunicación es directa, hay un registro central que le dice quien puede rexolver un problema

# Como se elije?
- Monoliticos, eficiencia, curva de aprendizaje, capacidad de prueba, capacidad de modificacion
- Distribuidos, mocularidad, disopnibilidad, uso de recursos, adaptabilidad

Errores:
¿Por qué no existe la bala de plata que resuelva las dificultades del desarrollo de software?
¿Cuál de los siguientes requerimientos funcionales incluye explícitamente un requerimiento no funcional?
De las formas en las que podemos trabajar con las dificultades esenciales, ¿cuál es la que más involucra a los arquitectos de software?
El usuario podrá comprar con tarjeta de crédito a través del sistema, ¿qué tipo de requerimiento es?
En los frameworks web modernos existe el concepto de middleware,  que describe una forma de interceptar el pedido o la respuesta del  sistema con componentes desarrollados independientes uno del otro. ¿Qué  estilo de arquitectura están implementando?</rich_text>
    </node>
    <node name="bases de datos" unique_id="44" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623801007" ts_lastsave="1625170789">
      <rich_text>## Necesitamos crear un proyecto que haga de GO el backend y si es posible de front con angular... dockers o kubernetes

- crear entidades de la base de datos
- hacer atributos

# Historia
- lenguaje de consultas estructurado
- se crea para la consulta de datos... no exisitia de principio una forma estandar
- tiene una estructura clara y definida
- DDL, data definition language, ayuda a crear los simientos de la base de datos
- DML, sirve para el manejado de datos

# NoSQL
- existen varios tipos:
   → clave- valor
      ⇒ DynamoDB o Cassandra
   → en documentos
      ⇒ MongoDB o Firestore, generalmente son json
   → en grafos
      ⇒ neo4j o TITAN, relaciones complejas
   → en memoria
      ⇒ Memcache o redis
   → optimizadas para busquedas
      ⇒ BigQuery o Elasticsearch

# Firestore
- uso de cloud para las bases de datos
- Jerarquia:
   → base de datos que contiene toda la información
   → colección de documentos
   → documento
- top-level conections:
   → conecciones que se tienen de inmediato
- </rich_text>
      <rich_text style="italic">Tipos de datos en Firestore</rich_text>
      <rich_text>:
   1) </rich_text>
      <rich_text weight="heavy">String</rich_text>
      <rich_text>: Cualquier tipo de valor alfanumérico
   2) </rich_text>
      <rich_text weight="heavy">Number</rich_text>
      <rich_text>: Soporta enteros y flotantes.
   3) </rich_text>
      <rich_text weight="heavy">Boolenan</rich_text>
      <rich_text>: Los clásicos valores True y False
   4) </rich_text>
      <rich_text weight="heavy">Map</rich_text>
      <rich_text>: Permite agregar un documento dentro de otro.
   5) </rich_text>
      <rich_text weight="heavy">Array</rich_text>
      <rich_text>: Permite agregar un conjunto de datos (soporte multi type) sin nombre e identificador.
   6) </rich_text>
      <rich_text weight="heavy">Null</rich_text>
      <rich_text>: Indica que no se ha definido un valor.
   7) </rich_text>
      <rich_text weight="heavy">Timestamp</rich_text>
      <rich_text>: Permite almacenar fechas (guarda el año, mes, día y hora).
   8) </rich_text>
      <rich_text weight="heavy">Geopoint</rich_text>
      <rich_text>: Guarda una localización geográfica (coordenadas latitud-longitud).
   9) </rich_text>
      <rich_text weight="heavy">Reference</rich_text>
      <rich_text>: Permite referencia un documento (relaciona dos documentos, no importa su colección).

# Uso en la vida real
- no existen bases de datos unitalla
- big data
   → concepto de grandes cantidades de datos
   → es una serie de soluciones para almacenar una cantidad de datos grandes y en poco tiempo
   → se puede usar en business inteligent
   → es un movimiento de varios tipos de bases de datos
   → Cassandra -&gt; checar
- data warehouse
   → almacenaje de datos masivos
   → no guarda mucho por segundo
   → se guarda mas por manera historica
   → es el archivo muerto
   → big table, es una sola tabla, muy grande
   → nos permita hacer consultas sobre este mismo
   → resuelve preguntas acerca de que ha pasado
- data mining
   → es minar datos
   → </rich_text>
      <rich_text family="monospace">"Data mining, consiste en torturar los datos hasta que confiesen"</rich_text>
      <rich_text>
   → se dedica a extraer los datos donde quiera que esten, hace sentido de esta data
- ETL
   → la metodologia de extraer, transformar y cargar
   → es la tecnica que nos ayuda a saber cada cuando aplicar la extraccion
   → muchas veces las bases de datos no son suficientes para este tipo de cosas
   → transformar de la base o app a algo con valor
   → es mas una idea que algo especifico de una tecnologia en si
   → pasarlo por una serie de cambios y guardarlo cuando sean utiles
- Business intellingence
   → inteligencia para el negocio
   → se usa para tomar decisiones indicadas
   → se podria decir que es lo final de la cadenita
   → se pueden entender los usos que se le da a las cosas
- machine learning
   → son tecnicas para el tratamiento de datos
   → nos ayuda a crear modelos que no van por patrones fortuitos
   → encuentra correlacion que no se ven a simple vista
   → se agrupan en:
      ⇒ clasificacion
      ⇒ prediccion
- data science
   → es algo preciso
   → poca gente lo hace
   → son mas estadisticos
   → se debe desarrollar y saber para que sirve cada herramienta
</rich_text>
    </node>
    <node name="basico" unique_id="78" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651004263" ts_lastsave="1652135240">
      <rich_text>la ffrma de crear modulos cambio del 1.15 en adelante antes se tenia que agregar al path los datos de GO

Variables, funciones y constantes
la declaracion de variables se hace:
	const {nombre} {tipo de dato} = {valor} // para constante
	const {nombre} = {valor} // se define por el valor el tipo
	{nombre} := {valor}
	var {nombre} {tipo} = {valor}
	var {nombre} {tipo}
	
	los zero values son valores por defectos que tienen las variables
		int -&gt; 0
		float64 -&gt; 0
		string -&gt; ‘’
		bool -&gt; false
		
	el incremental se maneja igual que en C
	
	primivitos:
		//Numeros enteros
		//int = Depende del OS (32 o 64 bits)
		//int8 = 8bits = -128 a 127
		//int16 = 16bits = -2^15 a 2^15-1
		//int32 = 32bits = -2^31 a 2^31-1
		//int64 = 64bits = -2^63 a 2^63-1

		//Optimizar memoria cuando sabemos que el dato simpre va ser positivo
		//uint = Depende del OS (32 o 64 bits)
		//uint8 = 8bits = 0 a 127
		//uint16 = 16bits = 0 a 2^15-1
		//uint32 = 32bits = 0 a 2^31-1
		//uint64 = 64bits = 0 a 2^63-1

		//numeros decimales
		// float32 = 32 bits = +/- 1.18e^-38 +/- -3.4e^38
		// float64 = 64 bits = +/- 2.23e^-308 +/- -1.8e^308

		//textos y booleanos
		//string = ""
		//bool = true or false

		//numeros complejos
		//Complex64 = Real e Imaginario float32
		//Complex128 = Real e Imaginario float64
		//Ejemplo : c:=10 + 8i
		
		en go toas las cadenas siempre tiene que ser en comillas dobles
		
	fmt:
		Printf, para usar en format, si se sab el tipo de se ve se usan los %tipo, si en dado caso no se sab ecual es se usa %v
		Sprintf, genera un string pero no lo muestra en consola, regresa el string es como un format
		printf mse sirve para saber el tipo de variable usando %T
		
	funciones
		se declaran: fun {nombre}({variables}) {!retorno!} {}
		si las variables son del mismo tipo se pueden declarar asi: a,b {tipo}
		si se requieren regresar mas de un valor se guardan en un ()
		una forma de ignorar un valor que no se requier usar se puede poner: _

Ciclos
- cada lenguaje tiene un tipo de ciclo, en el caso de golang solo existe for:
   → for condicional: for i:=0; i&lt;10; i++ {}
   → for while: for {!condicion!} {}
   → for forever: for {}
- ifs
   → if: if {condicion}
   → if-else: if {condicion} {} else {}
   → se usa &amp;&amp; para and
   → || se usa para or
- switch
   → se usa para sentencias multiples
   → switch {variable} {case value: {instruciones}  default: {instrucciones}}
   → generalmente se usa: switch {declaracion de variable}; {variable} {}
   → se puede usar: switch {case {variable-condicion} {}}
   → siempre es buena practica usar un default al final del switch
- defer
   → se usa para que antes de morirse el main ejecuta el codigo que este en defer
- break
   → sirve para romper un ciclo for
- continue
   → sirve para saltarse una iteracion en el ciclo for

Estructuras de datos basicas
- array y slices
   → array: var {nombre} [{tamanio}]{tipo de dato}, es inmutable
      ⇒ metodos: len() -&gt; tamanio y cap() -&gt; cuantos elementos puede guardar
   → slice: {nombre var} := []{tipo}{{lista de valores}}, son mutables
      ⇒ tiene los mismos metodos que el array
   → slicing: se usa para interactuar con los elementos, se hace igual que con python slice[{inicio}:{fin}]
   → para agregar un valor a un slice: slice = append(slice, {valor})
   → para agregar otra lista: slice = append(slice, {nueva lista}...)
   → se recorren usando un for: for index, value := range slice
- Maps
   → se creand: {nombre variable} := make(map[{tipo de dato:key}]{tipo de dato:value})
   → con for: for i, v := range {nombre map}, es de forma concurrente
   → obtencion de valores: value, err := {nombre map}[{key}], si la llave no existe en el diccionario el value es el zero value
- Structs
   → creacion type {nomre del struct} struct {atributos}
   → se instancia: {nombre} := {nombre del struct}{atributos}
- modificadores de acceso
   → si es minuscula el metodo es publico
   → si es mayuscula es privado
   → todo lo publico debe de llevar comentario

- structs and pointes
   → los punteros son para el acceso a la memoria
   → para dar el acceso se hace {nombre variable} := &amp;{otra variable}
   → al imprimir este nos dara la direccion de memoria
   → para acceder al valor se debe de poner: *{variable}
   → si se modifica el *{variable}, se cambia el valor de la variable original
   → se hace para hacer mas eficiente el codigo
   → los metodos de clase se definen: func ({nombre struct} {struct}) {nombre funcion} ({parametros}) {logica}
   → este modifica las variables de la clase: func ({nombre struct} *{struct}) {nombre funcion} ({parametros}) {logica}
   → para el custom de un struct: func ({nombre struct} {struct}) String() string { return fmt.Sprintf("{formato}") }
   → interfaces: es un metodo donde se pueden compartir entre varios structs
      ⇒ sirve para poder compartir un nombre de funcion para varias structs
      ⇒ se declaran: type {nombre interface} interface {{shared methods}}

Concurrencia y channels
- estar lidiando al mismo tiempo, paralelismo las hace
- hilo = tarea que ejecuta el procesador
- paralelo se usa los tres hilos
- concurrencia tiene todo disponible, mientras empieza con uno el otro se va creando, cuando se crea puede pausar la tarea para la que sigue
- en las gorutines, si muere primero el main que la rutina no la ejecuta
- se puede agregar time.Sleep para esperar pero no es recomendable
- para esperar se debe de agragar un WaitGroup: var wg sync.WaitGroup
- se agrega al WaitGroup con: wg.Add(1), ese mismo se debe de agregar a la funcion como un pointer
- en funcion se agrega: defer wg.Done()
- en main se debe de decir que espere: wg.Wait()
- generalmente las goroutines se usan con funciones anonimas
- channels
   → permiten compartir informacion entre rutinas
   → es un conducto donde solo se puede manejar un tipo de dato
   → se declaran: {name_channel} := make(chan {tipo de dato}, {cuantos valores puede manejar al momento})
   → se envian los datos con {name_channel} &lt;- {value} y se debe de pasar a la funcion
   → la salida se obtiene: &lt;-{name_channel}
   → buena practica, se debe indicar si el canal es de entrada o de salida:
      ⇒ channel &lt;-chan solo salida
- range, ayuda a hacer recorrido en todos los mensajes que se quedan en el channel
- select, sirve para buscar un mensaje desde un channel y hacer cierta operacion
- close, sirve para cerrar un canal y no volverse a usar



----------------------------------
76.70, 1600
100, 2050
75, 1600

5500</rich_text>
    </node>
    <node name="basic practico WS" unique_id="88" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652631305" ts_lastsave="1655058099">
      <rich_text>comando Go:
- go build: compila el archivo
- go run: crea el binario y ejecuta
- el package se le puede poner cualquier nombre, pero son usados para los modulos
- main es una funcion principal y necesaria
- se crean scanner para leer la entrada de los usuarios: {scanner} := bufio.NewScanner(os.Stdin); {scanner}.Scan(); {var} := {scanner}.Text()
- al trabajar con cadenas hay que tener mucho cuidado con los errores, ya que Go no lanza un exception

Structs and receivers
- las estructuras son tipos de datos que define el mismo usuario
- type {name} struct {body}
- method: func ({name} {name_struct}) {name_function} ({parameters}) {body}
- cuando se pasa un valor a una funcion generalmente se pasa un valor con una direccion completamente diferente
- para modificar valores de una estructura desde un metodo se requiere ahce por medio de apuntadores, pasando la struct con un *
- en go las listas son estaticas para algo dinamico se requiere hacer un slice:
   → type {name} struct {{name} []*{structure}}

Interfaces
- maps, son como diccionarios en python: {name} := make(map[{tipo llave}]{tipo de valores})
- sirven para evitar la creacion de codigo de mas, los metodos deben de tener el mismo nombre para ser tomados por la interfaz
- se declara la interfaz: type {nombre} interface { {funciones a meter} }

Concurrencia
- por medio de la palabra go se crean hilos o procesos de la funcion que se desea ejecutar
- las subrutinas no se conocen, se debe de usar algo para que se comuniquen
- channels
   → tiene que ser definidos con el tipo que recibira
   → se maneja el operador flecha para dar el mensaje
   → se crean: {name} := make(chan {type})
   → in function as parameter: {name} chan {datatype}
   → se transmiten: canal&lt;- {respuesta}
   → para leer: &lt;-canal
   → se tiene que hacer la llamada del canal las veces que se espera la respuesta</rich_text>
    </node>
    <node name="POO y concurrencia" unique_id="97" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655072393" ts_lastsave="1656538415">
      <rich_text>Go es orientado a objetos
- es un paradigma, uno de los predominantes en la industria
- llega a ser muy riguroso, pero permite alta reutilizacion de codigo y muchos patrones de disenio
- se puede alcanzar los conceptos de POO, pero muy diferente a Python y Java
- el equivalente de clases en Go son las structs
- para la modificacion de los datos de la estructura recordar que se debe de pasar como referencia
- cuando se quiere usar como todo es un objeto a veces no es algo aplicable
- se puede crear un objeto usando la palabra new, este retorna una referencia
- por medio de una funcion donde se resivan los parametros y regrese el apuntador del objeto a crear
- en la herencia solo se puede acceder a metodos publicos no a los privados
   → se usa para imprimir un mensaje solo usando una misma funcion para multiples clases que heredan de la misma
   → en go la herencia como tal no existe, se usa composicion, una clase contiene a la otra asi las clases son independientes
   → declarando la composicion sin necesidad de asignar la otra estructura nos ayuda a llamar directamente a las propiedades
- Interfaces
   → para ejecutar la composicion entre clases, se usa la palabra interface para declarar las interfaces, cada struct debe de tener el nombre de las funciones que tiene la interface
- para el abstract solo es necesario crear la estructura la cual se le pasa una variable de cadena que por medio de un if decide cual regresara
- Funciones anonimas, son solo para cuando se sta muy seguro que solo se usara una vez
   → se crea con: func() {tipo_retorno} {cuerpo}()
- Funciones variadicas y retornos con nombre
   → varios parametros en una funcion se declara: func {nombre}({nombre} ...{tipo_valor}) {tipo_retorno} {cuerpo de la funcion}

Testing:
- se crean los archivos: {nombre_archivo}_test.go
- para usar el testing se debe de estar dentro de un modulo
- code coverage
   → go test --cover, checa el porcentaje de cuanto codigo esta testeadp
   → go test --coverprofile={file.out}, crea un archivo para saber que esta probado
   → go tool cover --func={file.out}
   → go tool cover --html={file.out}
- profiling
   → go test --cpuprofile={file.out}
   → go tool pprof {file.out}

Concurrencia
- channels
   → unbuffered, se debe de estar 100 seguros de que alguien va a recibir el valor, se crean con {nombre} := make(chan {tipo})
   → buffered, se pueden usar en la misma funcion, se le da una cantidad limitada de valores, se crean con {nombre} := make(chan {tipo}, {number})
- waitgroup
   → se usa para tener una sincronizacion entre las rutinas
   → la rutina principal solo se dedica a lanzar las goroutines pero no las monitorea
   → para monitorearlas se puede usar un waitgroup
   → la funcion debe de recibir el parametro waitgroup
   → antes de llamar la rutina se dene de agregar uno al waitgruop
   → se debe de poner con defer wg.Done() en la rutina para que sepa que ya se ejecuto
   → para esperar: waitgroup.Wait()
   → con los canales podemos limitar el numero de procesos concurrentes que se pueden ejecutar, para eso los canales buffered
- Channels de lectura y escritura
   → declaracion de escritura, {nombre} chan&lt;-
   → declaracion de lectura, {nombre} &lt;-chan
- Worker pools
   → se crean trabajadores que hagan tareas especificas
- multiplexacion con select y case
   → se bloquean con canales si se quiere ver los datos
   → para que las rutinas se ejecuten en paralelo sin importar cual termine primero, de los canales se usa select</rich_text>
    </node>
    <node name="Concurrencia y patrones" unique_id="101" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656612399" ts_lastsave="1656867608">
      <rich_text>- las goruotines se inician con un stack de 2KB

Race condition
- pasa cuando varias rutinas tratan de entrar a la misma variable al mismo tiempo, puede que alguna operacion no sea exitosa
- Sync mutex, Lock y Unlock
   → para conocer el riesgo de carrera een el programa: go build --race {file.go}, crea un binario y se debe de executar para saber si existe el race condition
   → se deve de crear un lock de tipo sync.Mutex
   → se tiene que enviar a la funcion
   → y en la funcion se debe de usar lock.Lock() y al finalizar el lock.Unlock
- mutex lectura y escritura
   → se usa cuando se tienen diferentes subrutinas, que pueden estar escribiendo y leyendo valores
   → se crea usando sync.RWMutex, para lectura se usa RLock y RUnlock

Patrones
- ayudan a resolver problemas muy comunes, se pueden adaptar a nuestra necesidad y en el lenguaje de programacion de nuestro grusto
- son conceptos generales que premiten resolver un problema
- patrones de los GoF, se tiene 23 en la primer version del libro
- estan por
   → creacionales, se dice como crear nuevos objetos, flexibles y reutilizables
   → estructurales, como  se crean los obejtos en estructuras mas grandes
   → de comportamiento, comuniciacion entre objetos, para comunicar diferentes objetos
- Factory, se pueden crear diferentes tipos y de acuerdo a lo que necesitamos regresamos el que sea especifico
- singleton, se usan para tener una clase que sea accesible de manera global</rich_text>
    </node>
    <node name="grpc" unique_id="107" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1659297652" ts_lastsave="1659651795">
      <rich_text>Add to .bashrc
</rich_text>
      <rich_text family="monospace">export GOPATH=$HOME/go
export PATH=$PATH:$GOPATH/bin</rich_text>
      <rich_text>

sudo apt install -y protobuf-compiler
</rich_text>
      <rich_text family="monospace">go install google.golang.org/protobuf/cmd/protoc-gen-go@latest</rich_text>
      <rich_text>
</rich_text>
      <rich_text family="monospace">go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest</rich_text>
      <rich_text>

protoc --go_out=. --go_opt=paths=source_relative --go-grpc_out=. --go-grpc_opt=paths=source_relative proto/student.proto</rich_text>
    </node>
    <node name="CQRS" unique_id="108" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663354176" ts_lastsave="1663367712">
      <rich_text>CQRS -&gt; command query responsability segregarion
un manejo de colas para los servicios, no es necesario que se conecten entre ellos
RS -&gt; habilidades unicas cada servicio de Q y de C
C -&gt; solo se dedica a escribir en la base de datos, commands
Q -&gt; solo se encarga de lecturas, queries
esto sirve para solo escalar el servicio necesario, ya sea escalar solo la lectura o solo la escritura o ambos

Heeramientas:
- docker compose
- elastic search
- nats</rich_text>
    </node>
  </node>
  <node name="Crecimiento personal" unique_id="45" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617947184" ts_lastsave="1625837110">
    <node name="Personalidad y productividad" unique_id="46" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617947195" ts_lastsave="1620506007">
      <rich_text>Que es ser productivo??
- no es ser una persona que trabaja mucho, implica tener mas cuidado de uno que del trabajo como tal
- una persona productiva es capaz de producir, hacer uso de sus recursos, tiempo y espacio de manera efectiva para entregar un producto
- eficacia -&gt; medios llegar a un fin empleando los mejores medios
- eficiencia -&gt; lograr el efecto que se desea
Principios de productividad;
- lo que se mide mejora
- dejar de hacer
- explotar oportunidades
- encontrar mejores usos
Personal UX:
- tener mirada critica
- analisis, elaborar objetivos y probar
Herramientas:
- 16 personalidades - hecho
- big 5 personalities - falta
- eneagrama - hecho
- disc - estilos de liderasgo - falta
- high5 test - carrera, creo hecho

Auditoria de la vida
Mi vida en semanas:
- principales objetivos que he logrado conseguir
- revisa la vida de acuerdo como se ha vivido

Conocer la persona productiva
- Agilismo al personal de UX
   → autoconocimiento
   → perspectiva
   → Planear
   → Revisar
   → Focalizar
   → tomar decisiones
      ⇒ volver a pivotear
   → Medición
   → Documentar y compratir
   → Reflexionar
   → Poner en acción
      ⇒ Iterar
- Buenas practicas siendo agil
   → orientar objetivos a una meta
   → renuncia y ganar (decir si y no)
   → el metodo de planeacion es flexible
   → vive y despierta a la realidad
   → cumple con las tareas
   → aprender a perdonarse
   → encuentra equilibrio
- Principios
   → curiosidad: actitud de acercarse a la vida con una curiosidad insaciable y la busqueda continua del conocimiento
   → conexión: interconexion entre todos los conocimientos

Autoconocimiento:
- es la gestion de uno mismo, tiempo, recursos, canales y habitos
- las 6 w's, aplicadas a la vida
   → When
   → Where
   → What
   → Who
   → How
   → Why
- DOFA:
   → Fortalezas
   → Oportunidades
   → Debilidades
   → Amenazas

La rueda de la vida
- revisar salud
- tiempo de amigos y familia
- tu otro significativo, vida en pareja
- crecimiento personal, se logra todo lo que se ha planeado
- hobbies y tiempo libre
- actividades en casa, cuanto tiempo pasas en casa y le dedicas a eso
- carrera, que se requiere estudiar y cual es el plan
- dinero, compensación

Hero canvas:
- la coducta, pensamientos, actitudes y comportamientos, son un estimulo
- pensamiento, que tan proactiva
- relaciones, verbal y no verbal
- liderazgo, como gestiona los proyectos
- emocion, autoconocimiento y regulacion
- autodireccion, vision, rapidez del aprendizaje
- actitud, accion de la persona y propagacion de conocimiento
- resultado al cual se orienta

Definir User persona
- se resuelven problemas de la vida de uno mismo
- se tiene que llevar lo obtenido a valores cualitativas y cuantitativas
- definir atributos
- conectar pasiones, sueños y propositos
- ikigai

# Crea tu proposito y tu propuesta de vida
- mision manifesta, un enunciado que te motiva o a donde quieres llegar
- como realizar ese manifesto:
   → examinar la vida de otros
   → determina tu yo ideal
   → considera tu legado
   → determina tu proposito
   → clarifica tu aptitudes
   → escribe tu declaración
   → refínala

# Proyecta tu plan de vida
- se trata de estructura las herramientas en la vida en un determinado tiempo
- proyeccion: emociones y sentimientos de hacer algo que te hace sentir bien

# Rediseña tu vida
- tener una vida sistemica
- Brainstorming:
   → reestructura de la vida a cosas mas importantes
   → palabras claves, en el centro lo mas importante
- Meta: son fines ultimos, mas globales, que pasos se deben de seguir
- Warren Buffet:
   → tomarse un tiempo a solas
   → realizar lista de 25 cosas a realizar
   → señala los 5 realmente relevantes, deshacerse de las otras 20

# Aplicando desing thinking
- revisar todo lo anterior y simplificarlo en keywords
- Areas importantes:
   → amor
   → intelecto
   → salud
   → espiritu
- affinity map
   → escribir cualquier objetivo importante
   → escribir valores o actividades importantes a priorizar
   → clasifique las ideas en las declaraciones “yo quiero”
   → crear grupos organizados en las categorias de los yo quiero

# Objetivos estrategicos (OKR)
- metodologia dada por google
- son respuestas a las preguntas
-  SMART:
   → specific
   → measurable
   → achievable
   → relevant
   → time based
- los objetivos deben ser:
   → anuales
   → mensuales
   → diarios
   → horas
   → minutos
-GOST:
   → marco de referencia entre meta, objetivo, estrategia, tactica
- Eisenhower
   → otra metodologia para la priorizacion, donde esta urgente y poco urgente, importante y poco importante

# Las 3 P de la productividad
- Planificar, calendizar, metas y actividades, creacion de listas, priorizar listas, conmutar tareas
- Persuadir, tecnica de la persuacion empatica, seguridad, siempre un porqué, no tomar desicion fuera de la personalidad, genera valor, libertad y comprension
- Persistir, resilencia es la adaptacion a la adversidad, no vida dura, solo momentos dificiles

# Getting things done
- has que las cosas pasen, preguntas con flujo
- que harias si lo que hicieras no te lo pagaran
- por que lo harias
- mejor practica, hacer lo que te propones
   → averigua lo que es importante
   → priorizalo
   → hazlo

# Como dejar de perder el tiempo
- hacer kanban

# Espacio y ciclos
- espacio
   → lugar sin interrupciones
   → designar una zona de trabajo
   → fijar un horario laboral
   → acceso a internet
   → iluminación
   → tecnologia + accesorios
   → mesa + silla de trabajo
- ciclos
   → definir cronotipo de trabajo

# Rituales y rutinas
- ritual
   → primer estadio de interaccion con otras personas
   → una tarea critica al dia
   → registra tu tiempo
   → duerme entre 6 y 8 ocho horas
   → nuestra eficacia depende de que tiempo es mejor
   → las primeras 4 horas despues de despertar son efectivas
   → pasear, hacer ejercicio, desconectarte
   → cambiar de habitacion y delimitar el espacio de trabajo
   → largas duchas
   → tener un cuaderno de notas
   → rompe la rutina
- rituales con rutina: la salud mental es la capacidad de trabajar y amar
   → cuando tenemos un habito ya no cuenta tanto hemos establecido una rutina
   → minimizar la incertidumbre
   → ahorrar energia mental
   → mantener la concentracion

# Habitos
- 10 tipos de hábitos para tener dentro de nuestra rutina:
   ◇ Físicos
   ◇ Afectivos
   ◇ Sociales
   ◇ Morales
   ◇ Intelectuales
   ◇ Mentales
   ◇ De higiene
   ◇ Costumbristas
   ◇ Saludables
   ◇ Recreativos
- habitos son conductas repetitivas y elegidas concientemente
- recordatorio: estimulo
- rutina: la accion ejecutada
- recompensa: benefico obtenido

# Metodo Ivy lee
- lista diaria de tareas
   → 6 tareas importante
   → priorizar y ordenas las 6 tareas
   → concertrarse en una sola tarea hasta terminarla
   → si alguna no termina, se pasa al dia siguiente
- deepwork
   → para producir a un nivel maximo, se debe trabajar con periodos prolongados con mayor concentracion
   → la filosofia monastica
   → la filosofia bimodal
   → la filosofia ritmica
   → la filosofia periodistica
- la tecnica de los grandes gestos
   → realizar un cambio radical dentro de la rutina
   → cultivar el ocio, desconectarse completamente

## Trabajo remoto
- el flujo es un estado de inmersión total
- remote first, administran trabajo distribuido, tienen procesos de reclutamiento bastante refinados, se tiene todo documentado
- automattic, basecamp, buffer - hay que investigarlas
- trabajo asincrono, no importa cuanto trabajes, sino lo que se entregue
- se buscan resultados
- niveles
   → accion no deliberada
   → recrear la oficina, online
   → adaptarse al medio
   → comunicación asíncrona
- se dice que es un privilegio y no un derecho
- seis tipos de reunion:
   → actualizacion de estatus
   → toma de decisiones
   → compartir informacion
   → resolucion de problemas
   → diseño e innovación
   → fortalecimiento de equipo
- mensajes asincronos
   → proporcionan detalles
   → acciones claras
   → fecha de vencimiento
   → acceso de un recurso en caso de tener dudas

# Home office
- 23 minutos para reconectarse en lo que anda entre tarea y tarea
- revisar el equipo y trabajar con ellos
- andar motivado
- ergonomiía
- establecer límites

# metodos de productividad
- calcular el objetivo al que llegar
- escribe la meta
- establece una fecha
- enumera los pasos
- clasifica los pasos
- a trabajar
- Moscow
   → must
   → should
   → could
   → would

</rich_text>
    </node>
    <node name="notion" unique_id="47" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625837110" ts_lastsave="1627235312">
      <rich_text>las bases de datos son practicamente tablas para organizar la información
se diferencias las tablas (simples) de las bases de datos en que tiene estructura

El método PARA es una forma de organizar información creado por Tiago Forte que te permite establecer una estructura simple y escalable para categorizar cada aspecto de interés, desde documentos hasta metas que quieras lograr. Este sistema consiste de cuatro espacios llamados Proyectos, Áreas, Recursos y Archivo (por eso se llama PARA), con los cuales podrás clasificar información relevante a distintos aspectos de tu vida de manera ordenada.

El sistema Getting Things Done o GTD, creado por David Allen, consiste en 5 pasos simples que debes aplicar para ordenar y filtrar la forma en la que recolectas información y tareas en el día a día. El método promete brindar un espacio estructurado para poder concentrarse y ser estratégico en la manera en la que completas tareas y obtienes información.
	- capturar
	- clasificar
	- organizar
	- refelexionar
	- enganche

La matriz Eisenhower es una herramienta creada por Stephen Covey y basada en la forma de trabajar del ex-presidente de los Estados Unidos Dwight D. Eisenhower. Esta se basa en las cualidades de importancia y urgencia de una tarea para determinar cómo completarla, en qué momento y organizar una lista de tareas según 4 niveles de prioridad. Es especialmente útil para definir un orden específico en el que completar tareas, saber cuándo delegar y liberar un poco de tiempo dentro de lo posible.
	- urgente e importante
	- urgente y no importante
	- no urgente e importante
	- No urgente y no importante
	
	
-----------------------------------------------------------------------------------------------------------------------------------
# Que es notion
- Herramienta de edición colaborativa y sistemas personalizados
- toma de notas
- se pueden tener bases de datos
   → reuniones
   → tareas
   → y varias cosas mas
- creación de dashboards
- nuevas paginas con canvas en blanco

# Bloques basicos
- se pueden escribir texto libre o 3 encabezados (h1, h2, h3), se les puede editar, hasta en ecuaciones
- se pueden crear varios tipos de listas:
   → tareas
   → viñetas
   → lista numerada
   → lista toggle, permite esconder contenido dentro de cada una
   → resaltadores, bloques de cita, bloque callout (permite escoger un icono)
   → separadores, separados en dos secciones diferentes
   → columnas y colores para personalizar bloques basicos
   → con los 6 puntitos se pueden mover y crear bloques --- crear landing page

# Bloques avanzados
- se encuentran en la segunda seccion
   → tablas de contenido, se dedican a conseguir todos los encabezados en la pagina y se muestran en forma de jerarquia
   → breadcrumbs
      ⇒ muestra el camino que se debe de tomar para llegar a la pagina donde estoy
   → ecuaciones, ecuaciones de bloque o de linea (estilo latex)
   → botones de plantilla, sirve para usar una configuracion de bloques y usarse en otros
   → mensiones: de persona (@), fecha (@, se pueden escoger recordatorios) o pagina

# bloques de media o embends:
- se puede agregar contenido de otras paginas
- imagenes
- paginas web
- archivos
- contenidos de aplicaciones

# Bases de datos
- en notion en base de datos
- generalmente se ven en tablas
- una forma en la que se organiza la información
- se pueden diferenciar los tipos de datos
- hay propiedades:
   → basicas:
      ⇒ titulo (texto)
      ⇒ texto, esta es opcional la anterior no
      ⇒ número para escribir numeros y darles formato
      ⇒ seleccion unica, menu de opciones (solo una)
      ⇒ seleccion multiple, aqui se pueden elegir varias etiquetas
      ⇒ fecha y persona son identicas a las mensiones
      ⇒ archivos que se pueden subir
      ⇒ chackbox para verdadero/falso
      ⇒ manejo de url con boton para ir a la pagina web
      ⇒ uso email y correo
   → avanzadas:
      ⇒ se actualizan automaticamente
      ⇒ fecha de creacion para cuando se captura cierta entrada
      ⇒ creado por, dice quien lo creo
- se puede organizar desde sort (nos permite ordenar por la propiedad que hallamos creado)
- los filtros para solo ver un subconjunto de la informacion, es como un sql
- vistas, nos deja ver diferentes formas de ver la información, nos permite calcular totales para cada final de columna
- tablero, nos muestra la informacion como tarjetas (un trello), se pueden calcular valores
- galeria, muestra toda la informacion en tarjetas estilo tablero
- lista, se ven los iconos y al lado las propiedades
- calendario, nos muestra las tareas y propiedades que se estan haciendo
- linea de tiempo, calendario con tabla... gantt
- relacionadas:
   → se crea una propiedad con nombre y el tipo en avanzados sera con relacion
   → rollup para obtener otras propiedades de la tabla relacionada, se puede hacer operaciones estadisticas basicas
   → se pueden relacionar consigo misma
   → sincronizada con dos columnas
   → no sincronizada solo crea una columna
- plantillas:
   → se pueden agregar plantillas para repetir bases de datos

# formulas
- puede calcular datos de 4 tipos diferentes
   → numeros
   → texto
   → fechas
   → booleanos
- las funciones se dividen:
   → numericas
   → texto
   → fechas
   → booleanos
   → de conversion

# Sistemas de productividad y organizacion
- El método PARA es una forma de organizar información creado por Tiago Forte que te permite establecer una estructura simple y escalable para categorizar cada aspecto de interés, desde documentos hasta metas que quieras lograr. Este sistema consiste de cuatro espacios llamados Proyectos, Áreas, Recursos y Archivo (por eso se llama PARA), con los cuales podrás clasificar información relevante a distintos aspectos de tu vida de manera ordenada.

- El sistema Getting Things Done o GTD, creado por David Allen, consiste en 5 pasos simples que debes aplicar para ordenar y filtrar la forma en la que recolectas información y tareas en el día a día. El método promete brindar un espacio estructurado para poder concentrarse y ser estratégico en la manera en la que completas tareas y obtienes información.
   → capturar
   → clasificar
   → organizar
   → refelexionar
   → enganche

- La matriz Eisenhower es una herramienta creada por Stephen Covey y basada en la forma de trabajar del ex-presidente de los Estados Unidos Dwight D. Eisenhower. Esta se basa en las cualidades de importancia y urgencia de una tarea para determinar cómo completarla, en qué momento y organizar una lista de tareas según 4 niveles de prioridad. Es especialmente útil para definir un orden específico en el que completar tareas, saber cuándo delegar y liberar un poco de tiempo dentro de lo posible.
   → urgente e importante
   → urgente y no importante
   → no urgente e importante
   → No urgente y no importante

# Proyectos:
- marco de tiempo
- tareas
- estado
- progreso
- se pueden organizar en un tabla
- tareas:
   → estado
   → fecha limite
   → prioridad
   → proyecto
- centro de conocimiento:
   → cualquier aspecto de la vida
   → tipo
   → links
   → archivos
   → etiquetas
   → fecha de creacion
- dashboards
   → puede ser compartido
   → puede ser personal

# Trucos e integraciones
- creacion de pagina web:
   → agregar permisos para compartir en web
   → se puede comentar
   → editar
   → y compartir como plantilla
- super.so
   → link y dominio propio
   → google analytics
- fruituonsite:
   → opensource
   → mas manual y permite personalizar paginas
- vercel:
   → se puede crear un pagina web a partir de una base de de datos de notion
- hostNotion y Notion2site:
   → lo mismo que site pero mas nativa
- Bloques globales:
   → primero se crea un bloque simple
   → 6 puntitos, se copia el link
   → despues del so/, se incluye todo el link sobrante
   → se queda el link con los datos desde el numeral
   → se pega en un subpagina
- agregado de mas columnas:
   → crear la estructura en paginas aparte
   → se one la pagina en el boton y se transforma el texto
   → se elimina el texto de la pagina creada
- bases de datos de color:
   → crear la base
   → crear un toggle y poner la base dentro del toogle
- widgets:
   → indify:
      ⇒ widgets personalizados
      ⇒ progreso de vida
      ⇒ contador
      ⇒ clima
      ⇒ countdown
   → aption:
      ⇒ paypal
      ⇒ omnicalculator
      ⇒ formulario de correo
      ⇒ link-copy
      ⇒ codigo html, copiar codigo y ponerlo en las paginas
- Web clipper:
   → sirve para guardar cosas en notion desde chrome
- app externas:
   → slack:
      ⇒ se pueden mandar actualizaciones de pagina a cualquier canal
   → chillipepper:
      ⇒ formularios enviados a las personas
   → wunderpresentation:
      ⇒ crear en una pagina la presentacion, toma el h1 como cada plantilla de la presentacion

Errores:
¿Qué símbolo se usa para crear y mencionar una nueva página?
¿A cuál de estos formatos no se puede exportar una página de Notion?
Una tabla, una lista y un calendario son diferentes ejemplos de bases de datos
Al usar un toggle para darle color a una base de datos, si luego saco la base de datos del toggle, no mantendrĂ¡ su color -- verdadero
Se pueden agregar múltiples columnas a las páginas, toggles y botones de plantilla arrastrando los bloques hacia la izquierda o derecha de otros bloques -- falso

</rich_text>
    </node>
  </node>
  <node name="Road to code" unique_id="48" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#a52a2a" ts_creation="1620311539" ts_lastsave="1651193071">
    <node name="introducion al desarrollo web" unique_id="49" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1620311550" ts_lastsave="1621279593">
      <rich_text># Como empezo todo
- se queria resolver calculos
   → ábaco
   → calculadoras mecanicas
   → computadoras humanas, libros de calculos
   → primeras computadoras, los primeros programadores fueron mujeres
   → tarjetas perforadas
   → codigo maquina
   → primeros lenguajes de programación

# inputs y outputs
- entrada -&gt; proceso -&gt; salida
</rich_text>
    </node>
    <node name="javascript" unique_id="50" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621291941" ts_lastsave="1621433512">
      <rich_text># Tipos de variables
- representacion de un valor en memoria
- var [nombre] = [valor], sirve para que sepa que es una variable

# Hoisting
- solo funciona con ecmascript 5 para abajo
- variables y funciones se declaran antes que se procese el codigo

# Coherción
- dos tipos:
   → implicitas, cuando el lenguaje nos ayuda de una tipo a otro
   → explicitas, forma en que obligamos de un tipo a otro

# Truthy and Falsy
- valores verdaderos y falsos por defecto</rich_text>
    </node>
    <node name="ecmaScript" unique_id="51" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621639589" ts_lastsave="1621639756">
      <rich_text># Default params y concatenacion
- </rich_text>
    </node>
    <node name="js y v8" unique_id="52" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622420892" ts_lastsave="1622467957">
      <rich_text># Javascript engine</rich_text>
    </node>
    <node name="asincronismo" unique_id="53" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622842506" ts_lastsave="1623727205">
      <rich_text># Asincronismo
- acción que no ocurre al mismo tiempo

# Que es un callback
- es una funcion que al crearla se le pasa como parametro otra funciòn

¿Cual es el método recomendando por la comunidad para manejar asincronismo en JavaScript?
¿Nos permite definir una función así­ncrona? 
¿Para qué nos sirve el método "catch()"?
</rich_text>
    </node>
  </node>
  <node name="AWS" unique_id="79" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651193071" ts_lastsave="1651197180">
    <node name="Basic" unique_id="80" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651193078" ts_lastsave="1652038925">
      <rich_text>IAM
- service global
- root shouldn't be used or shared
- User can be grouped and are people
- permissions are JSON Document called policies
   → This defines the permissions
   → apply the least privilege principle
- tags is for information of users
- change num of account by alias
- policies:
   → Version, 2012-10-17
   → ID, identifier optional
   → Statement, one or more (required)
      ⇒ Sid, identifier (optional)
      ⇒ Effect (ALLOW; DENY)
      ⇒ principal, account, user, role to applied
      ⇒ action: action
      ⇒ resources: list of resources
      ⇒ condition: applies or not (optional)
- MFA is for more security
   → protects root and aws accounts
   → you know + you own
   → authy multidevice
   → yubikey, multiple root and user
- strong passwords, with expiration, not reuse
- to access in AWS exists
   → AWS managment console
   → AWS CLI, command in the shell
   → AWS SDK, for code, in C++, Python, Go, Java, Javascript, Php, Ruby, Nodejs, Android, Ios, Embedded C arduino
- CloudShell
   → icon in aws ui, is not in all regions
   → is possible use the cli, and use v2
- Roles for services
   → some services need to perform actions
   → assing role to services
   → common
      ⇒ EC2
      ⇒ Lambda
      ⇒ Cloudformation
- IAM Security Tools
   → IAM Credentials Report (account-level)
      ⇒ lists all account's users and the status
   → IAM Access Advisor (user-level)
      ⇒ show the service permissions granted to a user, and last accessed
      ⇒ use to check policies
- Best practices
   → don't use the root account
   → phyisical user = aws user
   → users in groups, permissions to groups
   → strong password policyenforce MFA
   → create and use roles to aws services
   → use keys only for programmatic access
   → audit permissions of tha accounts an reports
   → never share IAM &amp; access keys
- Responsability
   → aws
      ⇒ infraestructure
      ⇒ configuration and vulnerability
      ⇒ compliance validation
   → you
      ⇒ User, grouos, roles, policies
      ⇒ enable MFA
      ⇒ rotate all keys
      ⇒ IAM to apply appropiate permissions
      ⇒ Analyze access and review permissions

EC2
- billing, IAM user hasn't permissions in billing, is necesary root, or grant permissions to IAM
- AWS free tier
- create a budget to calculate use of costs, configure alerts, in percent use of budget to email
- is one most popular aws offering
- elastic compute cloud = IAAS
- renting virtual machine
- storing data EBS
- Distributing load across machines ELB
- Scaling services (ASG
- this is fundamental to understand how cloud workd
- operating system: linux, windows or macos
- ram and CPU is selected
- storage space:
   → network attached (EBS &amp; EFS)
   → hardware (EC2 instance Store)
   → Network card speed of the card, Public Ip Address
   → firewall rules: security groups
- bootstrap script (configure at first launch), EC2 User Data
- is possible use one script when is launched he first time the EC2 instance
- EC2 data scripts runs as root
- t2.micro is in free tier
   → selecy user data in configure instance details to execute a script
   → key pairs is required to conect is type RSA
- the ip public is change when is sopped the manchine and restart
- instances types:
   → 7 types
   → eveery type is for specific purpose
   → nom (m5.2xlarge):
      ⇒ m: instance class
      ⇒ 5: generation
      ⇒ 2xlarge: size of instance
- security groups
   → fundamentals of network security
   → control how traffic is allowed in ec2
   → only contains allow rules
   → can refered by IP or by security group
   → they regulate
      ⇒ access to ports
      ⇒ ip range
      ⇒ control of inbound and outbound network
      ⇒ can attached to multiple ionstance
      ⇒ down to a region / vpc combination
      ⇒ live outside the ec2
      ⇒ all inbound traffic is blocked by default, all outbound traffic is authorised by default
- for linux server is important use the ssh, this is secure, it is a command line mac, linux and win 10 and latest, to minor version of windows is necesary Putty
- in web browser is possible in all OS
- is the most importante function to connect to EC2, generally is used the port 22
- is necesary the pem key to connect to ssh: ssh -i pem user@ip
- is required que key has the permission 0400
- in windows is necessary save key to use with putty as PPk to connect
- in windows 10 is possible use powershell, is necessary chenge teh security, the owneto change, and remove another users 
- all instance has aws-cli
- is necesary to cofigure a role to access the services, use key and secrets is not a good idea
- instances pruchase options:
   → On demand instances, short workloads, predicitble pricing, pay by second
   → reserver (1 to 3 years), long workloads, flexible instances convertible, specific instance attributes, different types of payments
   → saving plans (1 to 3 years), commitment to an amout of usage, is more specific time use, region and family
   → spot, short workloads cheap can los instances, use with data process and not database
   → dedicates hosts, book and entire physical serverl
   → dedicated instance, no other customer will share your hardware
   → capacity reservation, reserve capacity in a specific AZ no discounts
- shared rsponsabilities
   → aws
      ⇒ insfraestrtructure
      ⇒ isolation on physical hosts
      ⇒ replacing faulty hardware
      ⇒ compliance validation
   → own
      ⇒ security groups
      ⇒ Os Patches and updates
      ⇒ software and utilities installed
      ⇒ AM roles in EC2</rich_text>
    </node>
    <node name="CloudFormation" unique_id="81" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651197180" ts_lastsave="1657057643">
      <rich_text>JSON is horrible to CF
is possible use CF and JSON
-- YAML
Key values pair
Nested objects
support arrays
Multi line string with |
can include comments

for every resource in CF is necesary define in:
</rich_text>
      <rich_text foreground="#6262a0a0eaea">Resources:
	Name:
		type:
		properties</rich_text>
      <rich_text>:

EC2 update with some interruption
Iam with no interruption
replacement
	recreating the resource
	creates the new resource or change logical ID
	for example an RDS DB

Options
	tags
	permission, specified by role
	notifications options
	timeout
	rollback on failure
	Rollback configuration
	Stack policy
	termination protection
	quick start link
	
Designer
	visualize a cloudformation stack
	used to verify if template us well created
	
building blocks
	AWSTemplateFormatVersion: 2010-09-09
	Description: Comments
	Transforma: specifies one or more Macros to process the template
	Metadata
	Resources: aws resources (mandatory)
	Parameters: the dynamic inputs
	Mappings: the static variables
	Outputs: Referencies
	condicionalust: list of conditions to perform resource creation
	Rules: validate a parameters durin stack creation/update
	helpers:
		references
		functions
		
deploying:
	manual way
	automated way
		using aws cli
		
Cost:
	use the link stimated cost, sends to simple monthlu calculatos

what is a parameters
	are a way to provide inputs to templates
	they're important:
		to reuse the templates across company
		some inputs can not be determined ahead of time
	controlled and can prevent errors from happening in your templates
	parameters can be cross-validate using rules
	
Parameters:
	{name}:
		Type: {type}
		Description: {description}
		ConstraintDescription: {string}
		Min/MaxLength: number
		Min/MaxValue
		AllowedValues: {array}
		AllowedPattern: {regex}
		NoEcho: {Boolean}
	reference parameter:
		with the function Fn::Ref
	Not used in:
		AWSTemplateFormatVersion
		Description
		Transform
		Mappings
	the shorthand for this in YAML is !Ref
	
SSMParameters:
	references to SSM
	specify ssm parameter key as the value
	CF fetches the lastest value
	CF doesn't store security string values
	validation on ssm parameter keys, but no values
	Supported ssm parameters
		- AWS::SSM::Parameter::Name
		- AWS::SSM::Parameter::Value&lt;String&gt;
		- AWS::SSM::Parameter::Value&lt;List&lt;String&gt;&gt;
		- AWS::SSM::Parameter::Value&lt;CommaDelimitedList&gt;
		- AWS::SSM::Parameter::Value&lt;AWS-Specific Parameter&gt;
		- AWS::SSM::Parameter::Value&lt;List&lt;AWS-Specific Parameter&gt;&gt;
		
Resources
	are the core of CF (Mandatory)
	represent aws components, will be created and configured
	resources are declarad and can reference
	Specified creation, updates and deletes
	more of 700 types
	identifiers ar form:
		AWS::aws-product-name::data-type-name
	optional atts:
		dependsOn:
			use to draw a dependency between two resources
			ex: create ecs after of ASG
			use with any resource
		deletionPolicy:
			protect resource from being deleted even if CF stack is
			 deleted (RDS)
			control what happens if a resource or CF is delete from CF
			Retain: resources is preserved or backup, to keep a resource
			Snapshot: EBS Volume, ElastiCache cluster and ReplicationGroup, RDS DB Instance, Clusters, Redshitf, Neptune DBCluster
			Delete: default, remove all, DBCluster and DBInstance default is Snapshot and in S3 is necesary that this is empty
		updateReplacePolicy:
			protects resources of being replacing
			that happens if a resource is updated
			updating RDS DB Avialability zon
			Use with any resource
			Delete: (default), delete old an create a new
			Retain: Keeps the resource
			Snapshot: EBS Volume, ElastiCache cluster and ReplicationGroup RDS DB Instance, Clusters, Redshitf, Neptune DBCluster, does not exists in CF scope
		creationPolicy: more details in the CFN init section
		updatePolicy: specific resources
		Metadata: anything
	is possible create dynamic number of resources, using macros and transform
	almost services is soported by CF
	
Mappings:
	are fixed varibles within CF
	this is handy with different envs, regios, AMI types
	all values are hardcoded
	this is used when the values can be taken from variables, for exameple, regios, aws account
	they allow safer control over the template
	
Funtion for mappings:
	Fn::FindInMap = !FindInMap[ MapName, TopLevelKey, SecondLevelKey ]
	
Pseudo parameters in CF
	parameters of type: “AWS::{something}”
	
Output:
	delares optional outputs values that can import from another CF
	is posible see the outputs in console of aws or by cli
	very useful for example if is defined a network with CF, varibales as VPCID or subnet id
	the best collaboration from cross stacks
	is necesary the !ImportValue {name} to get the value of output
	is not possible delete the stack until all references are deleted too

conditions
	are used to control the creation fo resources or outputs
	can be whatever tou want them to be, but commons ones are:
		environment
		aws region
		any parameter value
	each condition can references anorther condition, parameter or mapping
	define:
		CreatesSome: !Equals [ !Ref EnvType, prod ]
		is possible use:
			Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::Or
		
		applied to resources/outpus/...
		iif for example in mount point exists volume attach and the exists condition and is False this is not mounted
		
Rules
	parmeter section gives us abality to validate within a single parameter
	to perform parameter validations based in another parameters (crsoss parameter validation)
	for example all subnets are within the same vpc
	define:
		Rule:
			Rule Condition(optional): determines when rule take effect
			Assertion: describe what value are allowed for a partivular paramter, one or more
	support funtions(all with Fn::): And, Contains, EachMemeberEquals, EachMemberIn, Equals, If, Not, Or, RefAll, VoueOf, ValueOfAll
	
Metadata
	optional, include details about the template or resource
	does not provoque impacts
	exist 4 metadata keys
		AWS::CloudFormation::Designer, resources are laid out in template
		AWS::CloudFormation::Interface, grouping an orderinf og input parameter, is possible change the ordering in AWS interface, and group with a label
		AWS::CloudFormation::Authentication, specified auth credentials for files or sources this on Init
		AWS::CloudFormation::Init, define configuration tasks for cfn-init, it is the more powerful
	is possible overwritte the 4 metadata keys
	
CFN_Init and EC2 User Data
	Many of CF templates will be about provisioning computing resources
	resources can be either:
		EC2 Instances
		Auto Scaling Gtoups
	The instance to be self configured, to execute the work
	fully automate EC2 Fleet state with CF Init
	this is execute the first boot of the instance
	
	the problem with User Data is
		if the configuration is vere large
		what append if exists a change and the previos EC2 without termintaion
		make user-data more readable
		how to know if script finished
	Using helper scripts
	using Init of CF and the flow: cfn-init, cfn-signal, cfn-get-metadata and cfn-up (this is optional)
	
	command are execute in alphebical order
	
Drift
	the cf does not protect of manual configuration changes
	to know if exists changes is used CF drift
	detect drift on an entire stack or on individual
	we can resolver stack/resources drift by using resource import
	Not all resources are supported
	
Nested stack
	stacks as part of another stack
	allow isolate repeat patterns/common components and use from another stack
	this is a best practice
	is necessary to update a stack, update the parent stack
	is possible have multiple nested stack in anohter nested stack
	is important always apply the modification in root stack
	nested vs cross
		cross
			when stacks have different lifecycles
			use outputs exports
			pass values to many stacks (VPC id, etc)
		nested
			when components must be re-used
			only is important to the higher level (it's not shared)
			
			
Stacks Sets
	Create, update or delete stacks across multiple accounts and regions with one operation
	only the administrator create stacksets
	when the stack is updated all stack associeted is updated
	is possible execute in all accounts


ChangeSets
	when update the stack, to know what changes will happend before applying
	won't say if the update will be successful
	in nested stacks, is necesary changes across all stacks
	is possible select if is appliying in nested
	
Stack creation failures
	tried to create som resource
	one resource failed
	CF rolled back the resource
	the status ROLLBACK_COMPLETE
	to resolve is necesary delete and create new stack
	
Custom resources
	enable you write custom provision logic, create, delete and update
	define with:
		AWS::CloudFormation::CustomResource
		Custom::MyCustomResourceTypeName (recommended)
	two types:
		Aws SNS-backed
		AWS lambda-backed
	use cases:
		when AWS resource is not covered yet
		an on premise resource
		Running a lambda to empty an S3 bucket
		fetch an AMI id
		
WaitConditions
	pause tje creation of stack and wait for a signal before continues
	ex. create another resource when is full config a ec2 with one app
	props:
		Count - deafult 1
		Timeout - max 12hr.
		Handle - ref to WaitConditionHandle
	Handle is a presigned URL enable to send signal
	for EC2 and ASG recomended CreationPolicy
	use cfn-signal
	make an HTTP put request
	
Dynamic references
	references external values in SSM or in secrets
	CF retrieves the value, and change set operation
	master password in AWS secrets
	ssm plaint text, ssm-secure
	secretsmanager
	up to 60 dynamics
	
Modules
	package resources and configuration, use across templates
	is used as best practices
	use code written by experts
	
Resource import
	import exisiting resources into existing and new stacks
	is not necesary delete and recreate
	is necesary that describe the entire stacl
	a unique identifier for each target resource
	each resource myst have a DeletePolicy
	used when create a new stack from existing resources, move resource between stacks
	
SAM
	serverless application model
	framework for deploy and develop
	generate complex cloudformation
	
CDK
	Define the infra using a programming
	is possible import migrate a CF template  into to CDK
	
Macreos
	perform custom processing on CF template
	for example AWS::Serverless
	is necesary:
		lambda function
		a resource tupe AWS::CloudFormation::Macro
	process:
		the entire template
		a snipper of a template
		not macros witj macros
		not support stacksets</rich_text>
    </node>
  </node>
</cherrytree>
